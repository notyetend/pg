{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Working with Key/Value Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "supports general computation graphs for data analysis. It also supports a\n",
      "<http://spark.apache.org/>\n",
      "guide, on the [project web page](http://spark.apache.org/documentation.html)\n",
      "[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n",
      "Alternatively, if you prefer Python, you can use the Python shell:\n",
      "examples to a cluster. This can be a mesos:// or spark:// URL,\n",
      "\"yarn\" to run on YARN, and \"local\" to run\n",
      "locally with one thread, or \"local[N]\" to run locally with N threads. You\n",
      "package. For instance:\n",
      "Testing first requires [building Spark](#building-spark). Once Spark is built, tests\n",
      "Please see the guidance on how to\n",
      "storage systems. Because the protocols have changed in different versions of\n",
      "Hadoop, you must build Spark against the same version that your cluster runs.\n",
      "Please refer to the build documentation at\n",
      "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\n",
      "building for particular Hive and Hive Thriftserver distributions.\n",
      "Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)\n"
     ]
    }
   ],
   "source": [
    "val lines = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\")\n",
    "lines.\n",
    "  filter(x => x.split(\" \")(0).length > 5).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,[Ljava.lang.String;@7d91d77)\n",
      "(supports,[Ljava.lang.String;@6cb14793)\n",
      "(<http://spark.apache.org/>,[Ljava.lang.String;@1cc05d4c)\n",
      "(guide,,[Ljava.lang.String;@10e275d9)\n",
      "([\"Building,[Ljava.lang.String;@5f1fe017)\n",
      "(Alternatively,,[Ljava.lang.String;@39f4a9b8)\n",
      "(examples,[Ljava.lang.String;@27c0a9f7)\n",
      "(\"yarn\",[Ljava.lang.String;@248f4c6b)\n",
      "(locally,[Ljava.lang.String;@4b01fb5)\n",
      "(package.,[Ljava.lang.String;@1815dff2)\n"
     ]
    }
   ],
   "source": [
    "val pairs_detail = lines.\n",
    "  filter(x => x.split(\" \")(0).length > 5).\n",
    "  map(x => (x.split(\" \")(0), x.split(\" \")))\n",
    "  \n",
    "pairs_detail.\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-level: high-level, APIs, in, Scala,, Java,, Python,, and, R,, and, an, optimized, engine, that, \n",
      "supports: supports, general, computation, graphs, for, data, analysis., It, also, supports, a, \n",
      "<http://spark.apache.org/>: <http://spark.apache.org/>, \n",
      "guide,: guide,, on, the, [project, web, page](http://spark.apache.org/documentation.html), \n",
      "[\"Building: [\"Building, Spark\"](http://spark.apache.org/docs/latest/building-spark.html)., \n",
      "Alternatively,: Alternatively,, if, you, prefer, Python,, you, can, use, the, Python, shell:, \n",
      "examples: examples, to, a, cluster., This, can, be, a, mesos://, or, spark://, URL,, \n",
      "\"yarn\": \"yarn\", to, run, on, YARN,, and, \"local\", to, run, \n",
      "locally: locally, with, one, thread,, or, \"local[N]\", to, run, locally, with, N, threads., You, \n",
      "package.: package., For, instance:, \n",
      "Testing: Testing, first, requires, [building, Spark](#building-spark)., Once, Spark, is, built,, tests, \n",
      "Please: Please, see, the, guidance, on, how, to, \n",
      "storage: storage, systems., Because, the, protocols, have, changed, in, different, versions, of, \n",
      "Hadoop,: Hadoop,, you, must, build, Spark, against, the, same, version, that, your, cluster, runs., \n",
      "Please: Please, refer, to, the, build, documentation, at, \n",
      "[\"Specifying: [\"Specifying, the, Hadoop, Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version), \n",
      "building: building, for, particular, Hive, and, Hive, Thriftserver, distributions., \n",
      "Please: Please, refer, to, the, [Configuration, Guide](http://spark.apache.org/docs/latest/configuration.html), \n"
     ]
    }
   ],
   "source": [
    "pairs_detail.\n",
    "  collect().\n",
    "  foreach{\n",
    "    case (k, v) => {\n",
    "        print(k + \": \");\n",
    "        v.foreach(vv => print(vv + \", \"))\n",
    "      }\n",
    "    ; println()\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,13)\n",
      "(supports,11)\n",
      "(<http://spark.apache.org/>,1)\n",
      "(guide,,6)\n",
      "([\"Building,2)\n",
      "(Alternatively,,11)\n",
      "(examples,12)\n",
      "(\"yarn\",9)\n",
      "(locally,13)\n",
      "(package.,3)\n",
      "(Testing,10)\n",
      "(Please,7)\n",
      "(storage,11)\n",
      "(Hadoop,,13)\n",
      "(Please,7)\n",
      "([\"Specifying,4)\n",
      "(building,8)\n",
      "(Please,6)\n"
     ]
    }
   ],
   "source": [
    "val pairs_simple = lines.\n",
    "  filter(x => x.split(\" \")(0).length > 5).\n",
    "  map(x => (x.split(\" \")(0), x.split(\" \").length))\n",
    "  \n",
    "pairs_simple.\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations on Pair RDDs\n",
    "> - reduce(), fold(), combine()과 유사한 기능들이 Pair RDD을 위해서 각각 reduceByKey(), foldByKey(), combineByKey()으로 존재한다. 다만 reduce(), fold(), combine()는 **action**인 반면, reduceByKey(), foldByKey(), combineByKey()는 **transform**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### filter() for pair RDDs\n",
    "> - pair RDD에 filter를 적용하는 아래 두 코드는 같다.\n",
    "> - filter는 기본적으로 하나의 parameter를 받아 boolean을 return하는 함수를 argument로 받으므로, pair RDD에 이를 적용할 때에는 argument함수를 적절히 변형해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((high-level,13), (supports,11), (guide,,6), (Alternatively,,11), (examples,12), (\"yarn\",9), (locally,13), (Testing,10), (Please,7), (storage,11), (Hadoop,,13), (Please,7), (building,8), (Please,6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  filter(x => x._2 > 5).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((high-level,13), (supports,11), (guide,,6), (Alternatively,,11), (examples,12), (\"yarn\",9), (locally,13), (Testing,10), (Please,7), (storage,11), (Hadoop,,13), (Please,7), (building,8), (Please,6))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  filter{\n",
    "    case (k, v) => v > 5\n",
    "  }.\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### reduceByKey(func)\n",
    "> - Transformation for pair RDD\n",
    "> - groupByKey().reduce() 와 동일하며, reduceByKey가 성능이 더 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please: see, the, guidance, on, how, to, refer, to, the, build, documentation, at, refer, to, the, [Configuration, Guide](http://spark.apache.org/docs/latest/configuration.html), \n",
      "guide,: on, the, [project, web, page](http://spark.apache.org/documentation.html), \n",
      "high-level: APIs, in, Scala,, Java,, Python,, and, R,, and, an, optimized, engine, that, \n",
      "locally: with, one, thread,, or, \"local[N]\", to, run, locally, with, N, threads., You, \n",
      "Hadoop,: you, must, build, Spark, against, the, same, version, that, your, cluster, runs., \n",
      "examples: to, a, cluster., This, can, be, a, mesos://, or, spark://, URL,, \n",
      "package.: For, instance:, \n",
      "storage: systems., Because, the, protocols, have, changed, in, different, versions, of, \n",
      "[\"Specifying: the, Hadoop, Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version), \n",
      "[\"Building: Spark\"](http://spark.apache.org/docs/latest/building-spark.html)., \n",
      "\"yarn\": to, run, on, YARN,, and, \"local\", to, run, \n",
      "Alternatively,: if, you, prefer, Python,, you, can, use, the, Python, shell:, \n",
      "Testing: first, requires, [building, Spark](#building-spark)., Once, Spark, is, built,, tests, \n",
      "supports: general, computation, graphs, for, data, analysis., It, also, supports, a, \n",
      "<http://spark.apache.org/>: \n",
      "building: for, particular, Hive, and, Hive, Thriftserver, distributions., \n"
     ]
    }
   ],
   "source": [
    "pairs_detail.\n",
    "  reduceByKey((v1, v2) => v1 ++ v2).\n",
    "  collect().\n",
    "  foreach{\n",
    "    case (k, v) => {\n",
    "        print(k + \": \");\n",
    "        v.foreach(vv => print(vv + \", \"))\n",
    "      }\n",
    "    ; println()\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Please,20)\n",
      "(guide,,6)\n",
      "(high-level,13)\n",
      "(locally,13)\n",
      "(Hadoop,,13)\n",
      "(examples,12)\n",
      "(package.,3)\n",
      "(storage,11)\n",
      "([\"Specifying,4)\n",
      "([\"Building,2)\n",
      "(\"yarn\",9)\n",
      "(Alternatively,,11)\n",
      "(Testing,10)\n",
      "(supports,11)\n",
      "(<http://spark.apache.org/>,1)\n",
      "(building,8)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  reduceByKey((v1, v2) => v1 + v2).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### foldByKey()\n",
    "> - Transformation for pair RDD\n",
    "> - reduceByKey()와 동일하나 zero value를 갖는다는 차이\n",
    "> - groupByKey().fold()와 동일, foldByKey가 성능이 더 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Please,30)\n",
      "(guide,,16)\n",
      "(high-level,23)\n",
      "(locally,23)\n",
      "(Hadoop,,23)\n",
      "(examples,22)\n",
      "(package.,13)\n",
      "(storage,21)\n",
      "([\"Specifying,14)\n",
      "([\"Building,12)\n",
      "(\"yarn\",19)\n",
      "(Alternatively,,21)\n",
      "(Testing,20)\n",
      "(supports,21)\n",
      "(<http://spark.apache.org/>,11)\n",
      "(building,18)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  foldByKey(10)((v1, v2) => v1 + v2).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please: [Ljava.lang.String;@7f288d76, [Ljava.lang.String;@c98d34c, [Ljava.lang.String;@158bde0, \n",
      "guide,: [Ljava.lang.String;@287d9159, \n",
      "high-level: [Ljava.lang.String;@1385fa1d, \n",
      "locally: [Ljava.lang.String;@594d4992, \n",
      "Hadoop,: [Ljava.lang.String;@1d3e58e6, \n",
      "examples: [Ljava.lang.String;@598df541, \n",
      "package.: [Ljava.lang.String;@65e4b97f, \n",
      "storage: [Ljava.lang.String;@56566d88, \n",
      "[\"Specifying: [Ljava.lang.String;@3940e6cc, \n",
      "[\"Building: [Ljava.lang.String;@f513b58, \n",
      "\"yarn\": [Ljava.lang.String;@4f3e548c, \n",
      "Alternatively,: [Ljava.lang.String;@2a614ace, \n",
      "Testing: [Ljava.lang.String;@1429bfc8, \n",
      "supports: [Ljava.lang.String;@1224cdee, \n",
      "<http://spark.apache.org/>: [Ljava.lang.String;@2f037112, \n",
      "building: [Ljava.lang.String;@f147906, \n"
     ]
    }
   ],
   "source": [
    "pairs_detail.\n",
    "  groupByKey().\n",
    "  collect().\n",
    "  foreach{\n",
    "    case (k, v) => {\n",
    "        print(k + \": \");\n",
    "        v.foreach(vv => print(vv + \", \"))\n",
    "      }\n",
    "    ; println()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please: Please, see, the, guidance, on, how, to, | Please, refer, to, the, build, documentation, at, | Please, refer, to, the, [Configuration, Guide](http://spark.apache.org/docs/latest/configuration.html), | \n",
      "guide,: guide,, on, the, [project, web, page](http://spark.apache.org/documentation.html), | \n",
      "high-level: high-level, APIs, in, Scala,, Java,, Python,, and, R,, and, an, optimized, engine, that, | \n",
      "locally: locally, with, one, thread,, or, \"local[N]\", to, run, locally, with, N, threads., You, | \n",
      "Hadoop,: Hadoop,, you, must, build, Spark, against, the, same, version, that, your, cluster, runs., | \n",
      "examples: examples, to, a, cluster., This, can, be, a, mesos://, or, spark://, URL,, | \n",
      "package.: package., For, instance:, | \n",
      "storage: storage, systems., Because, the, protocols, have, changed, in, different, versions, of, | \n",
      "[\"Specifying: [\"Specifying, the, Hadoop, Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version), | \n",
      "[\"Building: [\"Building, Spark\"](http://spark.apache.org/docs/latest/building-spark.html)., | \n",
      "\"yarn\": \"yarn\", to, run, on, YARN,, and, \"local\", to, run, | \n",
      "Alternatively,: Alternatively,, if, you, prefer, Python,, you, can, use, the, Python, shell:, | \n",
      "Testing: Testing, first, requires, [building, Spark](#building-spark)., Once, Spark, is, built,, tests, | \n",
      "supports: supports, general, computation, graphs, for, data, analysis., It, also, supports, a, | \n",
      "<http://spark.apache.org/>: <http://spark.apache.org/>, | \n",
      "building: building, for, particular, Hive, and, Hive, Thriftserver, distributions., | \n"
     ]
    }
   ],
   "source": [
    "pairs_detail.\n",
    "  groupByKey().\n",
    "  collect().\n",
    "  foreach{\n",
    "    case (k, v) => {\n",
    "        print(k + \": \");\n",
    "        v.foreach{\n",
    "          vv => vv.foreach(vvv => print(vvv + \", \"))\n",
    "          print(\"| \")\n",
    "        }\n",
    "      }\n",
    "    ; println()\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Please,CompactBuffer(7, 7, 6))\n",
      "(guide,,CompactBuffer(6))\n",
      "(high-level,CompactBuffer(13))\n",
      "(locally,CompactBuffer(13))\n",
      "(Hadoop,,CompactBuffer(13))\n",
      "(examples,CompactBuffer(12))\n",
      "(package.,CompactBuffer(3))\n",
      "(storage,CompactBuffer(11))\n",
      "([\"Specifying,CompactBuffer(4))\n",
      "([\"Building,CompactBuffer(2))\n",
      "(\"yarn\",CompactBuffer(9))\n",
      "(Alternatively,,CompactBuffer(11))\n",
      "(Testing,CompactBuffer(10))\n",
      "(supports,CompactBuffer(11))\n",
      "(<http://spark.apache.org/>,CompactBuffer(1))\n",
      "(building,CompactBuffer(8))\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  groupByKey().\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner)\n",
    "> - 가장 기본이되는 per-key aggregation function으로서, 이것을 이용해 다른 대부분의 per-key combiner 들이 구현되어 있다.\n",
    "> - combine()과 마찬가지로 input data와 다른 type의 값을 return value로 얻을 수 있다.  \n",
    "  \n",
    "  \n",
    "> - combine을 위해 각 partition의 element들을 거쳐가면서 만나게되는 key는 '만나지 못했던 key' 이거나 '이미 만났던 key'로 나뉠 것이다.\n",
    "> - 만약 '만나지 못했던 key'라면 **createCombiner** 함수를 사용하여 그 key에 대한 초기값을 설정하게 된다.(이러한 초기값 설정은 전체 RDD에서 처음 등장하는 key에 대해서가 아니라 각 partition에서 처음 등장하는 key에 대해 일어난다.) \n",
    "> - 또한 만약 '이미 만났던 key'라면 **mergeValue** 함수를 사용하여 그 key에 대한 이전의 누적값(accumulator)과 새로운 value값에 대한 연산한다.\n",
    "> - 앞서 createCombiner과 mergeValue를 이용한 집계는 각 partition단위로 일어났던 것이고, 하나의 key가 여러 partition에 존재할 수 있다.이때 동일한 key에 대한 여러 partition의 값들을 집계하기 위하여 **mergeCombiners** 함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 예제에서는 각 문장의 시작단어에 따른 각 문장의 단어수 평균을 계산해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#,3)\n",
      "(,1)\n",
      "(Spark,14)\n",
      "(high-level,13)\n",
      "(supports,11)\n"
     ]
    }
   ],
   "source": [
    "lines.\n",
    "  map(x => (x.split(\" \")(0), x.split(\" \").length)).\n",
    "  take(5).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(And,(10,1))\n",
      "(Spark,(44,4))\n",
      "(Please,(20,3))\n",
      "(The,(12,1))\n",
      "([run,(8,1))\n"
     ]
    }
   ],
   "source": [
    "lines.\n",
    "  map(x => (x.split(\" \")(0), x.split(\" \").length)).\n",
    "  combineByKey(\n",
    "    newv => (newv, 1),\n",
    "    (acc: (Int, Int), oldv) => (acc._1 + oldv, acc._2 + 1),\n",
    "    (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  ).\n",
    "  take(5).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,13.0)\n",
      "(locally,13.0)\n",
      "(Hadoop,,13.0)\n",
      "((You,13.0)\n",
      "(Many,13.0)\n",
      "(examples,12.0)\n",
      "(for,12.0)\n",
      "(in,12.0)\n",
      "(The,12.0)\n",
      "(rich,12.0)\n"
     ]
    }
   ],
   "source": [
    "lines.\n",
    "  map(x => (x.split(\" \")(0), x.split(\" \").length)).\n",
    "  combineByKey(\n",
    "    newv => (newv, 1),\n",
    "    (acc: (Int, Int), oldv) => (acc._1 + oldv, acc._2 + 1),\n",
    "    (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  ).\n",
    "  map{case (k, v) => (k, v._1 / v._2.toFloat)}.\n",
    "  takeOrdered(10)(Ordering[Float].reverse on(x => x._2)).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### mapValues(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,13)\n",
      "(supports,11)\n",
      "(<http://spark.apache.org/>,1)\n",
      "(guide,,6)\n",
      "([\"Building,2)\n",
      "(Alternatively,,11)\n",
      "(examples,12)\n",
      "(\"yarn\",9)\n",
      "(locally,13)\n",
      "(package.,3)\n",
      "(Testing,10)\n",
      "(Please,7)\n",
      "(storage,11)\n",
      "(Hadoop,,13)\n",
      "(Please,7)\n",
      "([\"Specifying,4)\n",
      "(building,8)\n",
      "(Please,6)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,12)\n",
      "(supports,10)\n",
      "(<http://spark.apache.org/>,0)\n",
      "(guide,,5)\n",
      "([\"Building,1)\n",
      "(Alternatively,,10)\n",
      "(examples,11)\n",
      "(\"yarn\",8)\n",
      "(locally,12)\n",
      "(package.,2)\n",
      "(Testing,9)\n",
      "(Please,6)\n",
      "(storage,10)\n",
      "(Hadoop,,12)\n",
      "(Please,6)\n",
      "([\"Specifying,3)\n",
      "(building,7)\n",
      "(Please,5)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  mapValues(v => v - 1).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 위의 표현은 아래와 같이 map을 이용하여 표현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,12)\n",
      "(supports,10)\n",
      "(<http://spark.apache.org/>,0)\n",
      "(guide,,5)\n",
      "([\"Building,1)\n",
      "(Alternatively,,10)\n",
      "(examples,11)\n",
      "(\"yarn\",8)\n",
      "(locally,12)\n",
      "(package.,2)\n",
      "(Testing,9)\n",
      "(Please,6)\n",
      "(storage,10)\n",
      "(Hadoop,,12)\n",
      "(Please,6)\n",
      "([\"Specifying,3)\n",
      "(building,7)\n",
      "(Please,5)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  map{case (k, v) => (k, v - 1)}.\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### flatMapValues(func)\n",
    "> - 각 (key, value)에 대하여 (value, result1), (value, result2), (value, result...)가 생기고, (key, result1), (key, result2), (key, result3)...이 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<http://spark.apache.org/>,1)\n",
      "(<http://spark.apache.org/>,2)\n",
      "(<http://spark.apache.org/>,3)\n",
      "(<http://spark.apache.org/>,4)\n",
      "(<http://spark.apache.org/>,5)\n",
      "([\"Building,2)\n",
      "([\"Building,3)\n",
      "([\"Building,4)\n",
      "([\"Building,5)\n",
      "(package.,3)\n",
      "(package.,4)\n",
      "(package.,5)\n",
      "([\"Specifying,4)\n",
      "([\"Specifying,5)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  flatMapValues(v => (v to 5)).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-level\n",
      "supports\n",
      "<http://spark.apache.org/>\n",
      "guide,\n",
      "[\"Building\n",
      "Alternatively,\n",
      "examples\n",
      "\"yarn\"\n",
      "locally\n",
      "package.\n",
      "Testing\n",
      "Please\n",
      "storage\n",
      "Hadoop,\n",
      "Please\n",
      "[\"Specifying\n",
      "building\n",
      "Please\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  keys.\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "11\n",
      "1\n",
      "6\n",
      "2\n",
      "11\n",
      "12\n",
      "9\n",
      "13\n",
      "3\n",
      "10\n",
      "7\n",
      "11\n",
      "13\n",
      "7\n",
      "4\n",
      "8\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  values.\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sortByKey()\n",
    "> - 아래와 같이 custom sort를 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"yarn\",9)\n",
      "(<http://spark.apache.org/>,1)\n",
      "(Alternatively,,11)\n",
      "(Hadoop,,13)\n",
      "(Please,7)\n",
      "(Please,7)\n",
      "(Please,6)\n",
      "(Testing,10)\n",
      "([\"Building,2)\n",
      "([\"Specifying,4)\n",
      "(building,8)\n",
      "(examples,12)\n",
      "(guide,,6)\n",
      "(high-level,13)\n",
      "(locally,13)\n",
      "(package.,3)\n",
      "(storage,11)\n",
      "(supports,11)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  sortByKey().\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 아래 예에서는 string의 첫번째 혹은 두번째 문자를 비교하여 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "implicit val sortIntegersByString = new Ordering[String] {\n",
    "  override def compare(a: String, b: String) = a(0).toLower.compare(b(0).toLower)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"yarn\",9)\n",
      "(<http://spark.apache.org/>,1)\n",
      "([\"Building,2)\n",
      "([\"Specifying,4)\n",
      "(Alternatively,,11)\n",
      "(building,8)\n",
      "(examples,12)\n",
      "(guide,,6)\n",
      "(high-level,13)\n",
      "(Hadoop,,13)\n",
      "(locally,13)\n",
      "(package.,3)\n",
      "(Please,7)\n",
      "(Please,7)\n",
      "(Please,6)\n",
      "(supports,11)\n",
      "(storage,11)\n",
      "(Testing,10)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  sortByKey(true).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "implicit val sortIntegersByString = new Ordering[String] {\n",
    "  override def compare(a: String, b: String) = a(1).toLower.compare(b(1).toLower)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([\"Building,2)\n",
      "([\"Specifying,4)\n",
      "(package.,3)\n",
      "(Hadoop,,13)\n",
      "(Testing,10)\n",
      "(<http://spark.apache.org/>,1)\n",
      "(high-level,13)\n",
      "(Alternatively,,11)\n",
      "(Please,7)\n",
      "(Please,7)\n",
      "(Please,6)\n",
      "(locally,13)\n",
      "(storage,11)\n",
      "(supports,11)\n",
      "(guide,,6)\n",
      "(building,8)\n",
      "(examples,12)\n",
      "(\"yarn\",9)\n"
     ]
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  sortByKey(true).\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations on two pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,2)\n",
      "(3,4)\n",
      "(3,6)\n",
      "\n",
      "(3,9)\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq((1,2), (3,4), (3, 6)))\n",
    "rdd.collect().foreach(println); println\n",
    "val other = sc.parallelize(List((3, 9)))\n",
    "other.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### subtractByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,2))"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.\n",
    "  subtractByKey(other).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((3,(4,9)), (3,(6,9)))"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.\n",
    "  join(other).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### rightOuterJoin\n",
    "> - (참고로) Scala에는 Option이라는 datatype이 존재한다. Option type의 value는 그 값이 x로 존재할 경우 Some(x)이고, 그 값이 존재하지 않을 경우 None이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((3,(Some(4),9)), (3,(Some(6),9)))"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.\n",
    "  rightOuterJoin(other).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### leftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9))))"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.\n",
    "  leftOuterJoin(other).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### cogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,(CompactBuffer(2),CompactBuffer())), (3,(CompactBuffer(4, 6),CompactBuffer(9))))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.\n",
    "  cogroup(other).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action on Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((high-level,13), (supports,11), (<http://spark.apache.org/>,1), (guide,,6), ([\"Building,2), (Alternatively,,11), (examples,12), (\"yarn\",9), (locally,13), (package.,3), (Testing,10), (Please,7), (storage,11), (Hadoop,,13), (Please,7), ([\"Specifying,4), (building,8), (Please,6))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Please -> 3, Hadoop, -> 1, examples -> 1, package. -> 1, locally -> 1, <http://spark.apache.org/> -> 1, building -> 1, guide, -> 1, supports -> 1, high-level -> 1, \"yarn\" -> 1, storage -> 1, [\"Building -> 1, Alternatively, -> 1, [\"Specifying -> 1, Testing -> 1)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(high-level -> 13, Hadoop, -> 13, Please -> 6, locally -> 13, [\"Specifying -> 4, package. -> 3, Alternatively, -> 11, building -> 8, Testing -> 10, \"yarn\" -> 9, [\"Building -> 2, guide, -> 6, examples -> 12, <http://spark.apache.org/> -> 1, supports -> 11, storage -> 11)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### lookup(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedArray(7, 7, 6)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_simple.\n",
    "  lookup(\"Please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 데이터를 살펴보면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "\n",
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "supports general computation graphs for data analysis. It also supports a\n"
     ]
    }
   ],
   "source": [
    "lines.\n",
    "  take(5).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - flatMap: 우선 각 단어를 빈칸(\" \")으로 나눈다. 이때 flatMap을 사용하면 각 line 구분을 없앨 수 있다.\n",
    "> - map: 그리고나서 각 단어를 (단어, 1)로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#,1)\n",
      "(Apache,1)\n",
      "(Spark,1)\n",
      "(,1)\n",
      "(Spark,1)\n",
      "(is,1)\n",
      "(a,1)\n",
      "(fast,1)\n",
      "(and,1)\n",
      "(general,1)\n"
     ]
    }
   ],
   "source": [
    "lines.\n",
    "  flatMap(x => x.split(\" \")).\n",
    "  map(x => (x, 1)).\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - reduceByKey: 각 key에 해당하는 value값들을 더해준다.\n",
    "> - takeOrdered: 가장빈번하게 등장하는 단어를 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,67)\n",
      "(the,21)\n",
      "(to,14)\n",
      "(Spark,13)\n",
      "(for,11)\n",
      "(and,10)\n",
      "(a,8)\n",
      "(##,8)\n",
      "(run,7)\n",
      "(is,6)\n"
     ]
    }
   ],
   "source": [
    "lines.\n",
    "  flatMap(x => x.split(\" \")).\n",
    "  map(x => (x, 1)).\n",
    "  reduceByKey((x, y) => x + y).\n",
    "  takeOrdered(10)(Ordering[Int].reverse on(x => x._2)).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism\n",
    "> - size of partition을 명시적으로 지정할 수 있다. 이러한 지정은 RDD 생성시 혹은 Transformation시 지정 할수도 있다.\n",
    "\n",
    "> - repartition(): 말그대로 파티션을 다시 조정하는 기능이다. network를 따라 shuffle이 발생하므로 고비용 함수이다.\n",
    "> - coalesce(): 최적화된 repartition()으로서 파티션을 줄이는 경우에 효과적으로 사용할 수 있다.\n",
    "\n",
    "> - rdd.partitions.size를 통해 파티션 수를 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rdd1 = sc.parallelize(Seq((1,2), (3,4), (3, 6), (4, 9)), 2)\n",
    "val rdd2 = rdd1.reduceByKey((x, y) => x + y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of partition of rdd1: 2\n",
      "# of partition of rdd2: 4\n"
     ]
    }
   ],
   "source": [
    "println(\"# of partition of rdd1: \" + rdd1.partitions.size)\n",
    "println(\"# of partition of rdd2: \" + rdd2.partitions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - pair RDD에 대하여 partitioning을 수행할 수 있다.\n",
    "> - 그렇다면 어느 노드에 어느 데이터를 놓는 것이 연산을 위해 효과적일까? 즉 어떤 규칙으로 partition을 정해야 할까?\n",
    "> - 물론 Spark가 자동적으로 효과적인 partiton을 설정하지만, 사용자가 어느정도 그 원칙을 컨트롤 할수는 있다. 예를들어 hash partitioning이나 range partitioning등의 방법이나 partition의 수를 선택할 수 있다.\n",
    "\n",
    "> - 예를들어 1만건의 접속기록이 있고, 매 5분마나 100건의 접속기록이 유입된다. 매번 100건이 당일 접속했던 유저의 접속인지 혹은 당일 처음 접속하는 유저의 접속기록인지 알고 싶다. 이를 위해 두 데이터 세트를 join하는 일이 필요한데, join을 할 때마다 두 데이터 세트의 key를 hashing하고 hashing결과에 따라 특정 node로 data를 옮기는 shuffling작업이 필요하다.  \n",
    "> 이러한 shuffling을 피하기 위하여 미리 1만건의 데이터를 hash partition해 놓는다면, 5분마다 일어나는 이 join의 속도가 매우 향상될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(high-level,13)\n",
      "(supports,11)\n",
      "(<http://spark.apache.org/>,1)\n",
      "(guide,,6)\n",
      "([\"Building,2)\n",
      "(Alternatively,,11)\n",
      "(examples,12)\n",
      "(\"yarn\",9)\n",
      "(locally,13)\n",
      "(package.,3)\n",
      "(Testing,10)\n",
      "(Please,7)\n",
      "(storage,11)\n",
      "(Hadoop,,13)\n",
      "(Please,7)\n",
      "([\"Specifying,4)\n",
      "(building,8)\n",
      "(Please,6)\n"
     ]
    }
   ],
   "source": [
    "val pairs_simple = lines.\n",
    "  filter(x => x.split(\" \")(0).length > 5).\n",
    "  map(x => (x.split(\" \")(0), x.split(\" \").length))\n",
    "\n",
    "pairs_simple.\n",
    "  collect().\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 아래와 같이 HashPartitioner를 'partition수'와 함께 할당하게 된다. 그런데 partitionBy는 transformation이므로 그 결과는 새로운 RDD이다. 따라서 새로운 변수로 그 결과를 받아야 하고, persist()를 하지 않으면 매번 hash partitioning이 일어나게 된다. 꼭 persist()를 해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val parti_rdd = pairs_simple.\n",
    "  partitionBy(new HashPartitioner(10)).\n",
    "  persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 기본적으로 Spark는 transformation에 따라 적절한 partition rule을 선택한다. 예를들어 sortByKey()는 range-partition을, groupByKey()는 hash-partition을 선택하여 결과를 node에 분배한다. 반면 map()은 parent RDD의 partitioning information을 잊게 만든다.(map으로 key를 수정할 수 있으므로..)\n",
    "\n",
    "> - RDD에 partitioner가 정의되어 있는지 확인하려면, RDD의 **partitioner** property를 확인하면 된다. 이 값은 scala.Option type으로서 값이 정의되어 있거나 그렇지 않을 수 있다. 정의되어 있는지 확인하려면 .isDefined 를 사용하여, 그 값을 얻으려면 .get을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(org.apache.spark.HashPartitioner@a)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parti_rdd.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parti_rdd.partitioner.isDefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.HashPartitioner@a"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parti_rdd.partitioner.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Operations that benefit from partitioning\n",
    "> - cogroup(), groupwidth(), join(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), combineByKey(), lookup()은 partitioning을 할 경우 성능이 향상되는 operation이다.\n",
    "\n",
    "> - 예를들어 reduceByKey()를 partition되어 있는 RDD에 사용할 경우, 같은 key를 같는 데이터는 같은 partition에 있을 확률이 높게 되므로, 각 node에서 reduce연산이 진행되면 연산 그 자체는 끝이나게 된다. (물론 집계를 위해서 각 worker node에서 master로 데이터 이동이 필요하다.)\n",
    "> - 또한 cogroup()이나 join()와 같은 binary operation의 경우 최소한 한쪽 RDD만이라도 partition이 되어 있다면 shuffling이 줄어들게 된다. 만약 두 RDD가 동일한 partitioner를 갖고 있다면 그 연산의 성능은 더 좋아지게 된다.(예를들어 rdd1과 rdd2 = rdd.mapValue(..)이고 rdd1.cogroup(rdd2)인 경우)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Operations that affect partitioning\n",
    "> - Spark는 operation에 따라 그 결과에 자동적으로 partitioner를 할당한다. 예를들어 두 RDD를 join()한 경우 그 결과는 hash-partition되어 있을 것이라 판단한다. 따라서 바로 이후에 진행되는 reduceByKey()의 경우 별도의 shuffling이 일어나지 않아 그 성능이 매우 좋게 된다.\n",
    "\n",
    "> - 반면 pair-RDD에 map()을 사용한 경우, (이론적으로) map()연산이 key를 변경했을 수 있으므로 Spark는 그 결과에 대한 partitioner를 판단할 수 없게 된다. 따라서 pair-RDD의 key를 수정하지 않는 연산을 하는 경우 **mapValues()**나 **flatMapValues()**를 사용하는 것이 매우 효과적이다.\n",
    "\n",
    "> - cogroup(), groupwidth(), join(), leftOuterJoin(), rightOuterJoin(), groupByKey(), reduceByKey(), combineByKey(), partitionBy(), sort()의 경우 기본적으로 결과 RDD에 대한 partitioner가 할당된다. 또한 mapValue(), flatMapValues(), filter()의 경우 parent RDD에 partitioner가 할당되어 있을 경우 그 결과에 대한 partitioner가 할당된다.), 반면 다른 모든 operation에 대해서는 partitioner가 할당되지 않는다.\n",
    "\n",
    "> - 마지막으로 binary operation에 대해서는 parent RDD의 partitioner에 따라 결과의 partitioner가 결정된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### end of chapter 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
