{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. Advanced Spark Progamming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이 장에서는 아래 4가지를 배우게 됨\n",
    "> - Accumulator  \n",
    "  ... ex) 특정 이벤트 수를 카운트?\n",
    "> - Broadcast variable  \n",
    "  ... ex) 각 Task에서 공통으로 사용하는 lookup table을 공유?\n",
    "> - 파티션 단위 Transformation (mapPartitions, mapPartitionsWithIndex, foreachPartition()   \n",
    "  ... ex) DB연결과 같이 각 task단위로 하기에는 부담스러운 일을 Partition단위로 하는 방법?\n",
    "> - pipe()을 이용한 외부 스크립트 활용  \n",
    "  ... ex) R과 같은 외부의 다른 코드를 실행?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 샘플 데이터로는 아래와 같은 ham radio call log를 사용.. 하려 했으나 원본 파일을 구할수 없음..   \n",
    "> 대신 Spark 설치시 만들어지는 README.md 파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import play.api.libs.json._\n",
    "\n",
    "val call = \"\"\"{\"address\":\"address here\", \"band\":\"40m\",\"callsign\":\"KK6JLK\",\"city\":\"SUNNYVALE\",\n",
    "\"contactlat\":\"37.384733\",\"contactlong\":\"-122.032164\",\n",
    "\"county\":\"Santa Clara\",\"dxcc\":\"291\",\"fullname\":\"MATTHEW McPherrin\",\n",
    "\"id\":57779,\"mode\":\"FM\",\"mylat\":\"37.751952821\",\"mylong\":\"-122.4208688735\"}\"\"\"\n",
    "\n",
    "print(call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 어큐뮬레이터(Accumulators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Spark에서 제공하는 <u>shared variable</u>에는 **accumulator**와 **broadcast variable**가 있다.  \n",
    "  \n",
    "</newline>\n",
    "\n",
    "> **accumulator**는 그 이름이 의미하는 것 처럼   \n",
    "Cluster에 분산되어 있는 각 <u>worker node</u>에서 일어나는   \n",
    "어떤 이벤트의 수를 카운트하여 <u>driver program</u>에 집계하는 것과 같은 목적으로 사용한다.\n",
    "\n",
    "> 물론 RDD Transformation으로도 각 <u>worker node</u>에서 <u>driver program</u>으로 동일한 정보 집계를 구현할 수 있으나   \n",
    "원래 목적으로 하는 RDD Transformation에 비해    \n",
    "다소 보조적인 목적(디버깅, 모니터링)을 위해 사용하며   \n",
    "마치 <u>전체 cluster 전역변수</u>를 사용하는 것과 같이 간단한 문법으로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Local image](./_images/figure_2_3.PNG \"Tooltip for local image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example: simple accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **accumulator** 생성은 아래와 같으며,  accumulator에 초기값을 할당하고, 초기값의 Type에 따른 그 동작이 다르다.   \n",
    "(아래 예 에서는 org.apache.spark.Accumulator$<java.lang.Integer>$ type의 object가 생성된다.)\n",
    "```scala\n",
    "val blankLines = sc.accumulator(0)\n",
    "```\n",
    "\n",
    "> Worker node에서 **accumulator**변수에 += 연산을 이용하여 값을 더할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "\n",
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "supports general computation graphs for data analysis. It also supports a\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "MLlib for machine learning, GraphX for graph processing,\n",
      "and Spark Streaming for stream processing.\n",
      "\n",
      "<http://spark.apache.org/>\n",
      "\n",
      "Number of partitions: 1"
     ]
    }
   ],
   "source": [
    "val file = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\")\n",
    "file.\n",
    "  take(10).\n",
    "  foreach(println)\n",
    "  \n",
    "print(\"\\nNumber of partitions: \" + file.partitions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank lines: 35\n"
     ]
    }
   ],
   "source": [
    "val blankLines = sc.accumulator(0, \"counter\")\n",
    "\n",
    "val callSigns = file.flatMap(line => {\n",
    "  if (line == \"\") {\n",
    "    blankLines += 1\n",
    "  }\n",
    "  line.split(\" \")\n",
    "})\n",
    "\n",
    "callSigns.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "\n",
    "println(\"Blank lines: \" + blankLines.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **accumulator**변수는 worker node에게 <u>write-only</u>라서 value attribute에 접근할 수 없다.  \n",
    "> 접근하려 할 경우 에러 발생( java.lang.UnsupportedOperationException: Can't read accumu에게 value in task )\n",
    "\n",
    "> (각 worker node에는 initial value만 전달되므로 worker node에서는 전체 cluster의 집계 값을 알 수 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.UnsupportedOperationException: Can't read accumulator value in task\n",
       "\tat org.apache.spark.Accumulable.value(Accumulators.scala:117)\n",
       "\tat $line18.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:28)\n",
       "\tat $line18.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:25)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1195)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1195)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1195)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1277)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1203)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1183)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1209)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n",
       "$line19.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)\n",
       "$line19.$read$$iwC$$iwC$$iwC.<init>(<console>:33)\n",
       "$line19.$read$$iwC$$iwC.<init>(<console>:35)\n",
       "$line19.$read$$iwC.<init>(<console>:37)\n",
       "$line19.$read.<init>(<console>:39)\n",
       "$line19.$read$.<init>(<console>:43)\n",
       "$line19.$read$.<clinit>(<console>)\n",
       "$line19.$eval$.<init>(<console>:7)\n",
       "$line19.$eval$.<clinit>(<console>)\n",
       "$line19.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val blankLines = sc.accumulator(0)\n",
    "\n",
    "val callSigns = file.flatMap(line => {\n",
    "  if (line == \"\") {\n",
    "    blankLines += 1\n",
    "    print(blankLines.value)\n",
    "  }\n",
    "  line.split(\" \")\n",
    "})\n",
    "\n",
    "callSigns.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "println(\"Blank lines: \" + blankLines.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example: using transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 아래와 같이 RDD Transformation으로 동일한 일을 할수도 있으나,  \n",
    "본래 하려는 작업을 수행하면서 부가적인 디버깅/모니터링을 위해 **accumulator**를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.\n",
    "  filter(line => line == \"\").\n",
    "  count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example: 활용\n",
    "> README.md 파일을 이용해 **책 예제 6-5***를 테스트 \n",
    "  \n",
    "> 파일의 line이 빈줄일 경우 invalidLineCount를 증가시키고, 내용이 있는 줄 일 경우 validLineCount를 증가   \n",
    "  \n",
    "> 실제 하려는 일은 Transformation으로 내용이 있는 줄을 filter하여 저장하는 것인데,   \n",
    "이 과정에서 내용이 있는 줄이 임계값이상일 경우 결과를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "\n",
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "supports general computation graphs for data analysis. It also supports a\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "MLlib for machine learning, GraphX for graph processing,\n",
      "and Spark Streaming for stream processing.\n",
      "\n",
      "<http://spark.apache.org/>\n"
     ]
    }
   ],
   "source": [
    "val file = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\")\n",
    "file.\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "supports general computation graphs for data analysis. It also supports a\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "MLlib for machine learning, GraphX for graph processing,\n",
      "and Spark Streaming for stream processing.\n",
      "<http://spark.apache.org/>\n",
      "## Online Documentation\n",
      "You can find the latest Spark documentation, including a programming\n"
     ]
    }
   ],
   "source": [
    "file.\n",
    "  filter(line => line != \"\").\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위와 같은 일을 filter 과정에서  \n",
    "어떤 값에 따라 처리를 컨트롤 하고 싶은 경우 아래와 같이 할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val validLineCount = sc.accumulator(0)\n",
    "val invalidLineCount = sc.accumulator(0)\n",
    "\n",
    "def validateLine(line: String): Boolean = {\n",
    "  if(line != \"\"){\n",
    "    validLineCount += 1\n",
    "    true\n",
    "  } else{\n",
    "    invalidLineCount += 1\n",
    "    false\n",
    "  }\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid lines: 0, Invalid lines: 0\n",
      "\n",
      "Too many empty lines"
     ]
    }
   ],
   "source": [
    "val validLineRDD = file.filter(validateLine)\n",
    "\n",
    "println(s\"Valid lines: \" + validLineCount.value + \", \" + \"Invalid lines: \" + invalidLineCount.value + \"\\n\")\n",
    "\n",
    "if( validLineCount.value > 1.5 * invalidLineCount.value) {\n",
    "  println(\"Saving result as file...\")\n",
    "  validLineRDD.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "} else{\n",
    "  print(\"Too many empty lines\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **accumulator**는 RDD Transformation과 마찬가지로 <u>lazy evaluated</u>.   \n",
    "즉 action을 실행하는 시점에야 그 값을 집계된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid lines: 60, Invalid lines: 35\n",
      "\n",
      "Saving result as file...\n"
     ]
    }
   ],
   "source": [
    "val validLineRDD = file.filter(validateLine)\n",
    "\n",
    "validLineRDD.count()\n",
    "\n",
    "println(s\"Valid lines: \" + validLineCount.value + \", \" + \"Invalid lines: \" + invalidLineCount.value + \"\\n\")\n",
    "\n",
    "if( validLineCount.value > 1.5 * invalidLineCount.value) {\n",
    "  println(\"Saving result as file...\")\n",
    "  validLineRDD.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "} else{\n",
    "  print(\"Too many empty lines\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count 를 한번 더 하면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid lines: 180, Invalid lines: 105\n",
      "\n",
      "Saving result as file...\n"
     ]
    }
   ],
   "source": [
    "validLineRDD.count()\n",
    "\n",
    "println(s\"Valid lines: \" + validLineCount.value + \", \" + \"Invalid lines: \" + invalidLineCount.value + \"\\n\")\n",
    "\n",
    "if( validLineCount.value > 1.5 * invalidLineCount.value) {\n",
    "  println(\"Saving result as file...\")\n",
    "  validLineRDD.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "} else{\n",
    "  print(\"Too many empty lines\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 어큐뮬레이터와 장애 내구성(Accumulators and Fault Tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Transformation뿐만 아니라 action에서도 **accumulator**연산을 사용할 수 있는데,  \n",
    "Spark는 <u>Action</u>에서 사용하는 **accumulator**연산의 신뢰성을 보장하지만  \n",
    "<u>Transformation</u>에서 사용하는 **accumulator**연산의 신뢰성은 보장하지 않는다.  \n",
    "\n",
    "\n",
    "> [원문] **For accumulator updates performed inside actions only, Spark guarantees** that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. **In transformations, users should be aware** of that each task’s update may be applied more than once if tasks or job stages are re-executed.  \n",
    "http://spark.apache.org/docs/latest/programming-guide.html#accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Transformation은 여러 장애 상황에서 여러번 수행될 수 있다.   \n",
    "예를들어 Node crash가 일어나면 해당 task들을 다른 node에서 다시 실행하게 되고,   \n",
    "느린 task의 경우 복재본이 만들어져 다른 node에서 수행되기도 하고,  \n",
    "Cashed 되었으나 자주 사용되지 않는 RDD의 일부는 메모리에서 내려가게 되는데 재사용시 일부 Task가 재실행되기도 한다.\n",
    "\n",
    "> 이런 여러 가지 fault tolerance 상황에서 transformation이 여러번 실행될 수 있는데,   \n",
    "이때 accumulator값이 중복 집계되는 문제가 발생할 수 있다.   \n",
    "[참고] http://imranrashid.com/posts/Spark-Accumulators/\n",
    "\n",
    "> accumulator값의 신뢰성이 보장되어야 하는 경우 **action**에 accumulator연산을 집어 넣는 것이 좋다.\n",
    "\n",
    "> driver program은 accumulator instance의 복제본을 모든 task에 전달하고,  \n",
    "계산이 끝나면 driver program으로 다시 반환되므로   \n",
    "accumulator instance의 용량이 크고 Task수가 많아질 경우 효율적이지 않다.   \n",
    "따라서 accumulator는 가능한 가볍고 간단한 목적을 위해 사용하는 것이 좋다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid lines: 60, Invalid lines: 35\n",
      "\n",
      "Saving result as file...\n"
     ]
    }
   ],
   "source": [
    "val file = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\")\n",
    "\n",
    "val validLineCount = sc.accumulator(0)\n",
    "val invalidLineCount = sc.accumulator(0)\n",
    "\n",
    "file.foreach(line => {\n",
    "  if(line != \"\") validLineCount += 1\n",
    "  else invalidLineCount += 1\n",
    "})\n",
    "\n",
    "val validLineRDD = file.filter(line => line != \"\")\n",
    "\n",
    "println(s\"Valid lines: \" + validLineCount.value + \", \" + \"Invalid lines: \" + invalidLineCount.value + \"\\n\")\n",
    "\n",
    "if( validLineCount.value > 1.5 * invalidLineCount.value) {\n",
    "  println(\"Saving result as file...\")\n",
    "  validLineRDD.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "} else{\n",
    "  print(\"Too many empty lines\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 지정 어큐뮬레이터(Custom Accumulators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark에서 기본적으로 제공하는 (built-in) accumulator는 Int, Double, Long, Float 이 있다.  \n",
    "앞서 예에서 사용한 accumulator는 $org.apache.spark.Accumulator<java.lang.Integer>$ type이었다.\n",
    "\n",
    "다른 연산이나 데이터 구조를 이용하여 집계하기 위하여 별도의 accumulator를 정의할 수 있다.  \n",
    "이를 위해서는 **AccumulatorParam** class를 상속받은 class/object를 정의해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example: Partition & custom accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashMap 형태의 accumulator를 사용하고자 한다.  \n",
    "예를들어 acc += (\"a\", 1) 라고 할 경우\n",
    "HashMap에 동일 key가 있다면 value += 1이 수행되고   \n",
    "동일 key가 없다면 \"a\"-> 1이 추가되는 accumulator를 필요로 한다고 해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Original Source: https://gist.github.com/fedragon/b22e5d1eee4803c86e53\n",
    "\n",
    "import org.apache.spark.{ AccumulableParam, SparkConf }\n",
    "import org.apache.spark.serializer.JavaSerializer\n",
    "import scala.collection.mutable.{ HashMap => MutableHashMap }\n",
    "\n",
    "/*\n",
    " * Allows a mutable HashMap[String, Int] to be used as an accumulator in Spark.\n",
    " * Whenever we try to put (k, v2) into an accumulator that already contains (k, v1), the result\n",
    " * will be a HashMap containing (k, v1 + v2).\n",
    " *\n",
    " * Would have been nice to extend GrowableAccumulableParam instead of redefining everything, but it's\n",
    " * private to the spark package.\n",
    " */\n",
    "object HashMapParam extends AccumulableParam[MutableHashMap[String, Int], (String, Int)] {\n",
    "\n",
    "  def addAccumulator(acc: MutableHashMap[String, Int], elem: (String, Int)): MutableHashMap[String, Int] = {\n",
    "    val (k1, v1) = elem\n",
    "    acc += acc.find(_._1 == k1).map {\n",
    "      case (k2, v2) => k2 -> (v1 + v2)\n",
    "    }.getOrElse(elem)\n",
    "\n",
    "    acc\n",
    "  }\n",
    "\n",
    "  /*\n",
    "   * This method is allowed to modify and return the first value for efficiency.\n",
    "   *\n",
    "   * @see org.apache.spark.GrowableAccumulableParam.addInPlace(r1: R, r2: R): R\n",
    "   */\n",
    "  def addInPlace(acc1: MutableHashMap[String, Int], acc2: MutableHashMap[String, Int]): MutableHashMap[String, Int] = {\n",
    "    acc2.foreach(elem => addAccumulator(acc1, elem))\n",
    "    acc1\n",
    "  }\n",
    "\n",
    "  /*\n",
    "   * @see org.apache.spark.GrowableAccumulableParam.zero(initialValue: R): R\n",
    "   */\n",
    "  def zero(initialValue: MutableHashMap[String, Int]): MutableHashMap[String, Int] = {\n",
    "    val ser = new JavaSerializer(new SparkConf(false)).newInstance()\n",
    "    val copy = ser.deserialize[MutableHashMap[String, Int]](ser.serialize(initialValue))\n",
    "    copy.clear()\n",
    "    copy\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 README.md 파일에서 빈줄의 수를 count 했었는데,  \n",
    "각 partiton에서 몇건이나 이런 빈줄이 발생하는지 추적하고 싶다면   \n",
    "방금 정의한 HashMapParam를 이용하여 아래와 같이 해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark is built using [Apache Maven](http://maven.apache.org/).\n",
      "The easiest way to start using Spark is through the Scala shell:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\n",
      "# Apache Spark\n",
      "\n",
      "\n",
      "Number of partitions: 11"
     ]
    }
   ],
   "source": [
    "val file_part = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(11)\n",
    "file_part.\n",
    "  take(10).\n",
    "  foreach(println)\n",
    "\n",
    "print(\"\\nNumber of partitions: \" + file_part.partitions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 21, Partition: 0, Host: localhost -> 5\n",
      "Stage: 21, Partition: 9, Host: localhost -> 4\n",
      "Stage: 21, Partition: 5, Host: localhost -> 3\n",
      "Stage: 21, Partition: 1, Host: localhost -> 2\n",
      "Stage: 21, Partition: 6, Host: localhost -> 2\n",
      "Stage: 21, Partition: 10, Host: localhost -> 2\n",
      "Stage: 21, Partition: 2, Host: localhost -> 5\n",
      "Stage: 21, Partition: 7, Host: localhost -> 1\n",
      "Stage: 21, Partition: 3, Host: localhost -> 4\n",
      "Stage: 21, Partition: 8, Host: localhost -> 3\n",
      "Stage: 21, Partition: 4, Host: localhost -> 4\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap\n",
    "import org.apache.spark.TaskContext\n",
    "\n",
    "val mapAcc = sc.accumulable(new HashMap[String, Int])(HashMapParam)\n",
    "\n",
    "val callSigns = file_part.flatMap(line => {\n",
    "  if (line == \"\") {\n",
    "    val ctx = TaskContext.get\n",
    "    val stageId = ctx.stageId\n",
    "    val partId = ctx.partitionId\n",
    "    val hostname = ctx.taskMetrics.hostname\n",
    "\n",
    "    mapAcc += (s\"Stage: $stageId, Partition: $partId, Host: $hostname\", 1)\n",
    "  }\n",
    "\n",
    "  line.split(\" \")\n",
    "})\n",
    "\n",
    "callSigns.saveAsTextFile(\"./_tmp/output_\" + System.currentTimeMillis / 1000)\n",
    "\n",
    "mapAcc.value.foreach{case(key, value) => println(key + \" -> \" + value)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브로드캐스트 변수(Broadcast Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Broadcast variable을 이용하여   \n",
    "Task들이 공통적으로 그리고 반복적으로 사용하는  \n",
    "<u>read-only variable</u>을 각 worker에 cashing할 수 있다.\n",
    "\n",
    "> 예를들어 각 dictionary에 있는 단어의 등장 횟수를 집계할 경우나   \n",
    "머신러닝에서 각 Partition에 흩어져 있는 sample에 대한 예측값을 구하기 위하여 모든 task에서 동일한 parameter vector를 사용해야 하는 경우 등에   \n",
    "효과적으로 사용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SparkContext.broadcast method를 이용하여 생성하며,   \n",
    "value property를 이용하여 값에 접근   \n",
    "\n",
    "\n",
    "> mutable 변수를 broadcast variable로 넘겨줄 수 있고,   \n",
    "각 worker node에서 이 값을 변경할 수도 있지만   \n",
    "변경이 다른 노드에는 영향을 주지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Local image](./_images/sparkcontext-broadcast-executors.png \"Tooltip for local image\")\n",
    "<h6 align=\"right\">(출처: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-broadcast.html)</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example: \n",
    "> 유입되는 로그(file_new_log)에서 특정 단어들(bv_lookup_words)이 포함된 문장만을 처리한다고 해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bv_lookup_words = sc.broadcast(Set(\"spark\", \"you\", \"mllib\", \"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark is built using [Apache Maven](http://maven.apache.org/).\n",
      "The easiest way to start using Spark is through the Scala shell:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\n",
      "# Apache Spark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val file_new_log = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(11)\n",
    "\n",
    "file_new_log.\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set(c)"
     ]
    }
   ],
   "source": [
    "val test1 = Set(\"a\", \"b\", \"c\")\n",
    "val lookup1 = Set(\"c\")\n",
    "print(test1 & lookup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Set(spark),Spark is built using [Apache Maven](http://maven.apache.org/).)\n",
      "(Set(spark),The easiest way to start using Spark is through the Scala shell:)\n",
      "(Set(spark),# Apache Spark)\n",
      "(Set(spark),To build Spark and its example programs, run:)\n",
      "(Set(spark),Spark is a fast and general cluster computing system for Big Data. It provides)\n",
      "(Set(you, spark),You can find the latest Spark documentation, including a programming)\n",
      "(Set(you),(You do not need to do this if you downloaded a pre-built package.))\n",
      "(Set(you),You can set the MASTER environment variable when running examples to submit)\n",
      "(Set(spark),rich set of higher-level tools including Spark SQL for SQL and DataFrames,)\n",
      "(Set(spark),Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported)\n",
      "(Set(mllib),MLlib for machine learning, GraphX for graph processing,)\n",
      "(Set(spark),Testing first requires [building Spark](#building-spark). Once Spark is built, tests)\n",
      "(Set(spark),and Spark Streaming for stream processing.)\n",
      "(Set(python),## Interactive Python Shell)\n",
      "(Set(you),locally with one thread, or \"local[N]\" to run locally with N threads. You)\n",
      "(Set(you, spark),Hadoop, you must build Spark against the same version that your cluster runs.)\n",
      "(Set(spark),## Building Spark)\n",
      "(Set(spark),Spark also comes with several sample programs in the `examples` directory.)\n",
      "(Set(you, python),Alternatively, if you prefer Python, you can use the Python shell:)\n"
     ]
    }
   ],
   "source": [
    "file_new_log.\n",
    "  map{ line => \n",
    "    val words = line.toLowerCase().split(\" \").toSet;\n",
    "    val common = words & bv_lookup_words.value;\n",
    "    if(common.size > 0) (common, line)\n",
    "    else (Set(), line)\n",
    "  }.\n",
    "  filter{case (k, v) => k.size > 0}.\n",
    "  take(100).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위의 작업을 join transformation을 이용하여 수행할 수도 있지만,   \n",
    "분산 환경에서는 상대적으로 큰 테이블(fact table)과 작은 테이블(dimension)을 join하는 경우   \n",
    "map을 이용한 join을 사용하는 것이 효과적(map-side join이라 함)  \n",
    "(참고: http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브로드캐스트 최적화(Optimizing Broadcasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 broadcast variable을 이용한 구현을   \n",
    "일반 변수를 사용하여 아래와 같이 바꿔볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val lookup_words = Set(\"spark\", \"you\", \"mllib\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark is built using [Apache Maven](http://maven.apache.org/).\n",
      "The easiest way to start using Spark is through the Scala shell:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\n",
      "# Apache Spark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val file_new_log = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(11)\n",
    "\n",
    "file_new_log.\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Set(spark),Spark is built using [Apache Maven](http://maven.apache.org/).)\n",
      "(Set(spark),The easiest way to start using Spark is through the Scala shell:)\n",
      "(Set(spark),# Apache Spark)\n",
      "(Set(spark),To build Spark and its example programs, run:)\n",
      "(Set(spark),Spark is a fast and general cluster computing system for Big Data. It provides)\n",
      "(Set(you, spark),You can find the latest Spark documentation, including a programming)\n",
      "(Set(you),(You do not need to do this if you downloaded a pre-built package.))\n",
      "(Set(you),You can set the MASTER environment variable when running examples to submit)\n",
      "(Set(spark),rich set of higher-level tools including Spark SQL for SQL and DataFrames,)\n",
      "(Set(spark),Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported)\n",
      "(Set(mllib),MLlib for machine learning, GraphX for graph processing,)\n",
      "(Set(spark),Testing first requires [building Spark](#building-spark). Once Spark is built, tests)\n",
      "(Set(spark),and Spark Streaming for stream processing.)\n",
      "(Set(python),## Interactive Python Shell)\n",
      "(Set(you),locally with one thread, or \"local[N]\" to run locally with N threads. You)\n",
      "(Set(you, spark),Hadoop, you must build Spark against the same version that your cluster runs.)\n",
      "(Set(spark),## Building Spark)\n",
      "(Set(spark),Spark also comes with several sample programs in the `examples` directory.)\n",
      "(Set(you, python),Alternatively, if you prefer Python, you can use the Python shell:)\n"
     ]
    }
   ],
   "source": [
    "file_new_log.\n",
    "  map{ line => \n",
    "    val words = line.toLowerCase().split(\" \").toSet;\n",
    "    val common = words & lookup_words;\n",
    "    if(common.size > 0) (common, line)\n",
    "    else (Set(), line)\n",
    "  }.\n",
    "  filter{case (k, v) => k.size > 0}.\n",
    "  take(100).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위와 같은 구현에서는 lookup_words의 복사본이 모든 task에 직렬화되어(serialized) 전달되고, 각 task를 실행하기 직전에 역직렬화(deserialized)된다. \n",
    "\n",
    "> 물론 broadcast variable을 사용할 경우에도 비슷한 과정을 거치지만 worker 단위로 한번만 직렬화 개체를 전달하고 역직렬화(deserialized)가 한번만 필요하므로 반복사용되는 데이터의 경우 효과적이다. \n",
    "\n",
    "> 따라서 network overhead를 줄일 수 있는 직렬화(serialization) 포멧을 사용하는 것이 매우 중요하다.\n",
    "\n",
    "> 8장에서 $Kyro$라는 빠른 직렬화 라이브러리에 대해 배우게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파티션별로 작업하기(Working on a Per-Partition Basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RDD의 각 element에 대하여 어떤 작업을 하는 것이 아니라   \n",
    "각 Partition에 대하여 한번식 어떤 작업이 해야하는 경우  \n",
    "예를들어 DB connection이나 난수생성등의 일은 data element단위로 필요한 일이 아니라   \n",
    "partition단위로 필요한 작업니다. \n",
    "\n",
    "> 이런 partition단위의 일을 쉽게 해 주는 것이 **mapPartitions, mapPartitionsWithIndex, foreachPartition**함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common work for partition 0 -> \n",
      "Common work for partition 0 -> You can set the MASTER environment variable when running examples to submit\n",
      "Common work for partition 0 -> building for particular Hive and Hive Thriftserver distributions.\n",
      "Common work for partition 1 -> # Apache Spark\n",
      "Common work for partition 1 -> ## Interactive Scala Shell\n",
      "Common work for partition 1 -> examples to a cluster. This can be a mesos:// or spark:// URL,\n",
      "Common work for partition 1 -> \n",
      "Common work for partition 2 -> \n",
      "Common work for partition 2 -> \n",
      "Common work for partition 2 -> \"yarn\" to run on YARN, and \"local\" to run\n",
      "Common work for partition 2 -> ## Configuration\n",
      "Common work for partition 3 -> Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "Common work for partition 3 -> The easiest way to start using Spark is through the Scala shell:\n",
      "Common work for partition 3 -> locally with one thread, or \"local[N]\" to run locally with N threads. You\n",
      "Common work for partition 3 -> \n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap\n",
    "import org.apache.spark.TaskContext\n",
    "\n",
    "val file_new_log = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(30)\n",
    "\n",
    "file_new_log.\n",
    "  mapPartitions(lineIter => {\n",
    "      \n",
    "      val work2do = \"Common work for partition \" + TaskContext.get.partitionId\n",
    "      \n",
    "      lineIter.toList.map(line => work2do + \" -> \" + line).iterator\n",
    "    }\n",
    "  ).\n",
    "  take(15).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common work for partition 0 -> \n",
      "Common work for partition 0 -> You can set the MASTER environment variable when running examples to submit\n",
      "Common work for partition 0 -> building for particular Hive and Hive Thriftserver distributions.\n",
      "Common work for partition 1 -> # Apache Spark\n",
      "Common work for partition 1 -> ## Interactive Scala Shell\n",
      "Common work for partition 1 -> examples to a cluster. This can be a mesos:// or spark:// URL,\n",
      "Common work for partition 1 -> \n",
      "Common work for partition 2 -> \n",
      "Common work for partition 2 -> \n",
      "Common work for partition 2 -> \"yarn\" to run on YARN, and \"local\" to run\n",
      "Common work for partition 2 -> ## Configuration\n",
      "Common work for partition 3 -> Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "Common work for partition 3 -> The easiest way to start using Spark is through the Scala shell:\n",
      "Common work for partition 3 -> locally with one thread, or \"local[N]\" to run locally with N threads. You\n",
      "Common work for partition 3 -> \n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.HashMap\n",
    "import org.apache.spark.TaskContext\n",
    "\n",
    "val file_new_log = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(30)\n",
    "\n",
    "file_new_log.\n",
    "  mapPartitionsWithIndex((idx, lineIter) => {\n",
    "      \n",
    "      val work2do = \"Common work for partition \" + idx\n",
    "      \n",
    "      lineIter.toList.map(line => work2do + \" -> \" + line).iterator\n",
    "    }\n",
    "  ).\n",
    "  take(15).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 외부 프로그램과 파이프로 연결하기(Piping to External Programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **pipe** 함수를 이용하여   \n",
    "RDD의 데이터를 (Scala, Java, Python이 아니더라도) 스크립트에 전달하고  \n",
    "처리된 결과값을 받아올 수 있다.\n",
    "\n",
    "> SparkContext.addFile(path) 와 같이 context에 file을 추가하면,   \n",
    "driver node에 존재하는 script file이 각 worker node에 복제된다.    \n",
    "\n",
    "\n",
    ">각 node에서 file의 위치는 SparkFiles.getRootDirectory를 이용하여 알수 있다.    \n",
    "\n",
    "\n",
    "> 또한  SparkFiles.get(name)으로 각 node에서 file을 복제해갈 수 있다.    \n",
    "물론 수동 혹은 다른 방법으로 각 worker node의 SparkFiles.getRootDirectory 경로에 위치시켜도 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 예제로 문장을 받아 \" \"으로 나눈 후 단어 수를 반환하는 아래의  R script를 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "f <- file(\"stdin\")\n",
    "open(f)\n",
    "\n",
    "while(length(line <- readLines(f, n=1)) > 0) {\n",
    "  len <- length(strsplit(line, \" \")[[1]])\n",
    "  write(len, stdout())\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is built using [Apache Maven](http://maven.apache.org/).\n",
      "The easiest way to start using Spark is through the Scala shell:\n",
      "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\n",
      "# Apache Spark\n",
      "To build Spark and its example programs, run:\n",
      "    ./bin/pyspark\n",
      "    ./bin/run-example SparkPi\n",
      "    MASTER=spark://host:7077 ./bin/run-example SparkPi\n",
      "Please see the guidance on how to\n",
      "for detailed guidance on building for a particular distribution of Hadoop, including\n"
     ]
    }
   ],
   "source": [
    "val file_new_log = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(11)\n",
    "\n",
    "file_new_log.\n",
    "  filter(line => line != \"\").\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkFiles\n",
    "\n",
    "val rScript = \"/home/sparkuser/work/jupyter/_etc/withSpark.R\"\n",
    "val rScriptName = \"withSpark.R\"\n",
    "sc.addFile(rScript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(6, 12, 4, 3, 8, 5, 6, 7, 7, 12, 3, 5, 8, 8, 14, 8, 10, 6, 13, 13, 10, 8, 6, 2, 11, 6, 13, 6, 12, 3, 12, 3, 10, 8, 12, 13, 6, 8, 8, 2, 3, 9, 10, 11, 12, 6, 4, 13, 4, 13, 3, 4, 11, 14, 1, 11, 11, 3, 5, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_new_log.\n",
    "  filter(line => line != \"\").\n",
    "  pipe(Seq(SparkFiles.get(rScriptName))).\n",
    "  collect().toList\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 RDD 연산들(Numeric RDD Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 수치 데이터를 포함하는 RDD의 기술통계량(descriptive statistics)을 계산하기 위한 함수들을 제공한다.   \n",
    "> count(), mean(), sum(), max(), min(), variance(), sampleVariance(), stdev(), sampleStddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 예제로 문장내 단어의 수가    \n",
    "파일내 문장들의 단어수 분포의  \n",
    "평균을 중심으로 68% 내에 있는 문장들을 필터링 해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Local image](./_images/Empirical_Rule.PNG \"Tooltip for local image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val file_new_log = sc.textFile(\"/home/sparkuser/spark-1.6.2-bin-hadoop2.6/README.md\").repartition(11)\n",
    "\n",
    "file_new_log.\n",
    "  filter(line => line != \"\").\n",
    "  map(line => (line.split(\" \").length, line)).\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val lineLen = file_new_log.\n",
    "  filter(line => line != \"\").\n",
    "  map(line => line.split(\" \").length).\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stddev = lineLen.stdev()\n",
    "val mean = lineLen.mean()\n",
    "println(\"stddev: \" + stddev + \", mean: \" + mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_new_log.\n",
    "  filter(line => line != \"\").\n",
    "  map(line => (line.split(\" \").length, line)).\n",
    "  filter{case (k, v) => math.abs(k - mean) < stddev}.\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_new_log.\n",
    "  filter(line => line != \"\").\n",
    "  map(line => (line.split(\" \").length, line)).\n",
    "  filter{case (k, v) => math.abs(k - mean) > stddev}.\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
