{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter6. Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 없이 샘플링 에피소드로 학습시킬 수 있는 MC의 장점과 에피소드가 끝나기 전에 추정치를 갱신할 수 있는 DP의 장점(bootstrap)을 합한 것이 바로 TD learning이다. (TD methods combine the sampling of Monte Carlo with the bootstrapping of DP.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC는 각 에피소드가 끝날때까지 가다렸다가 구한 $G_t$를 타겟으로 $V(S_t)$를 업데이트 한다.\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\bigg[ G_t - V(S_t) \\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면 TD(0)는 단지 다음 time-step까지만 기다렸다가 바로 획득한 보상 $R_{t+1}$과 다음 상태에 대한 가치 추정치 $V(S_{t+1})$를 할인한 값을 더하여 타겟으로 잡고 $V(S_t)$를 업데이트 한다.\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\bigg[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 TD(0)의 타겟에 이전에 $v_{\\pi}(S_t)$에 대한 추정치 $V(S_t)$를 사용하므로 bootstrapping(추정치를 얻기 위해 다른 추정치를 이용)이라 할 수 있다.\n",
    "\n",
    "또한 이때 $\\bigg[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\bigg]$를 TD error라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Advantages of TD Prediction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP에 비해 TD의 우월한 점은 model-free라는 점이다. 즉 model의 reward function과 state transition probability를 몰라도 된다.\n",
    "\n",
    "또한 MC에 우월한 점은 에피소드가 끝날때까지 기다릴 필요 없이 온라인 학습을 할 수 있다는 것이다. 실제로 에피소드가 매우 긴 어플리케이션의 경우 에피소드가 끝날때까지 기다렸다가 업데이트 하는 것은 매우 느리고 비효율적이다. 또한 에피소드가 끝나지 않는 어플리케이션의 경우에는 MC를 적용할 수가 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD는 과연 정답에 수렴하는가? 정해진 정책 $\\pi$에 대해 점점 작아지는 step-size 를 사용할경우 수렴한다.\n",
    "\n",
    "MC vs TD중 어느것이 먼저 정답에 도달하는가? 증명은 어렵지만 실전에서는 TD가 더 먼저 정답에 도달한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Optimality of TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Example 6.4     \n",
    "$V(B)=3/4$라는 것을 알고 있을 때 $V(A)$는? TD를 적용하면 $V(A)=R_A + V(B)$이므로 $3/4$이다. 반면 MC를 적용하면 $V(A) = R_A + R_B = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Sarsa: On-policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value 대신 action-value를 사용하며, 하나의 state-action 쌍에서 다른 state-action쌍으로의 전이를 이용해 학습시킨다.  \n",
    "- MC Control이 아래와 같이 업데이트해서 $q_{\\pi}(s, a)$를 추정하려 했다면 \n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ G_t - Q(S_t, A_t)\\bigg]$$\n",
    "TD Control은 아래와 같이 업데이트해서 $q_{\\pi}(s, a)$를 추정한다. \n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\bigg]$$\n",
    "\n",
    "또한 MC Control의 업데이트는 각 에피소드 종료마다 진행는데 비해, TD의 업데이트는 매 time-step마다 진행된다.\n",
    "\n",
    "On-policy TD Control은 에피소드의 $\\bigg( S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}\\bigg)$ 마다 업데이트가 진행되므로 SARSA라고도 부른다. 물론 정책에 대한 업데이트도 동시에 greedy 하게 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Q-learning: Off-policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-policy MC Control은 아래와 같이 $Q(S_t, A_t)$를 업데이트 한다.\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{W}{C(S_t, A_t)} \\bigg[ G - Q(S_t, A_t) \\bigg]$$\n",
    "반면 Off-policy TD Control(Q-learning)은 아래와 같이 $Q(S_t, A_t)$를 업데이트 한다.\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma \\max_{\\alpha} Q(S_{t+1}, a) - Q(S_t, A_t) \\bigg]$$\n",
    "\n",
    "Q-learning의 Backup-diagram은 아래 왼편과 같다. 즉 최상단의 action node($Q(S_t, A_t)$)를 업데이트하며, 즉시 보상 $R_{t+1}$과 다음 상태의 $Q(S_{t+1}, A_{t+1})$중 가장 큰 것을 이용한다.\n",
    "![Local image](./images/sutton/figure_6_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning에서는 다음 상태의 $Q(S_{t+1}, A_{t+1})$들 중 가장 큰것을 이용했다. 이를 약간 변형하여  $Q(S_{t+1}, A_{t+1})$들의 기댓값을 이용하는 방법이 Expected Sarsa이다. 이때 $Q(S_{t+1}, a)$에 각 action에 해당하는 정책(확률)을 곱하고 더해서 기댓값을 구한다.\n",
    "\n",
    "$$\\begin{align}\n",
    "Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma \\mathbb{E} \\big[Q(S_{t+1}, A_{t+1}) \\mid S_{t+1} \\big] - Q(S_t, A_t) \\bigg] \\\\\n",
    "&\\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \\bigg]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa에서는 덜 학습된 정책에 따라 greedy하게 선택된 $A_{t+1}$을 따라가기 때문에 variance가 크다. Exptected Sarsa는 기존 Sarsa에 비해 variance가 줄어들고 같으 수의 에피소드가 주어졌을 때 조금 더 나은 성능을 보인다. 또한 Sarsa는 $\\alpha$ 선택에 따라 성능이 크게 좌우되지만 Expected Sarsa는 $\\alpha$ 선택에 대단히 자유롭다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Sarsa를 off-policy로 변형하고, 학습 정책은 greedy, 탬색 정책은 보다 soft한 것을 사용한다면 이는 정확히 Q-learning과 같아진다. 즉 greedy하게 다음 state($S_{t+1}$)에서 취할 action을 선택하기 때문에 $\\max_a Q(S_{t+1}, a)$와 $\\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a)$가 같아진다.($Q(S_{t+1}, a)$가 최대인 정책을 제외하고는 모든 $\\pi(a \\mid S_{t+1})$가 0 이므로.)\n",
    "\n",
    "> Expedted Sarsa with **off-policy**(**greedy**, soft) = Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Maximization Bias and Double Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 논했던 control 알고리즘들은 공통적으로 '추정치의 최대값(maximum over estimated values)'을 '최대값에 대한 추정치(estimate of the maximum value)'로 사용 했다. 이는 실제보다 $Q$를 크게 추정하는 문제('maximization bias')를 발생시킨다. (예를들어 어떤 집단의 키를 100가지로 추정했을 때, 이 추정치의 최대값은 진짜 최대값에 대한 추정치보다 일반적으로 크다.)\n",
    "\n",
    "> - Example 6.7      \n",
    "정답은 '오른쪽으로 가는 것이 좋다.' 인데, 왼쪽의 $B$에서 종료로 가는 수많은 action들의 보상은 $\\mathcal{N}(-0.1, 1)$에서 추출한다. 따라서 왼쪽으로 가는 길의 평균 보상은 $-0.1$이다. 그럼에도 우리가 사용해왔던 control 방법에 따르면 우연히 $\\mathcal{N}(-0.1, 1)$에서 나온 양수값의 영향으로 왼쪽으로 가는 것을 포기하지 않는다는 것이다.(Q-learn의 그래프는 에피소드가 진행되도 왼쪽으로 갈 확률이 5% 이상을 유지한다.)\n",
    "\n",
    "이 문제(maximization bias)의 원인은 $Q$를 최대화 하는 action을 선택할때 사용하는 샘플(에피소드)과 $Q$를 추정할 때 사용하는 샘플(에피소드)가 같기 때문이다. (예를들어 두갈레 길이 있을 때 어느길을 선택할지는 A에게 물어보고, 특정 길이 얼마나 좋을지도 A에게 물어보는 것이다. 이러면 A의 편향이 강하게 반영된다. 반면 어느길을 선택할지는 A에게 물어보고, 그 길이 얼마나 좋은지는 B에게 물어보는 것이 더 나은 방법이다.)\n",
    "\n",
    "이를 해결하기 위해 샘플(에피소드)들을 둘로 나누고, 두개의 독립적인 추정치인 $Q_1(a)$와$Q_2(a)$를 학습하게 하는 것이다. 구체적으로 최적 action은 $Q_1$을 이용해 $A^* = \\text{argmax}_a Q_1(a)$와 같이 구하고, 업데이트는 $Q_2$에 $Q_2(A^*)=Q_2(\\text{argmax}_a Q_1(a))$와 같이 하는 것이다. 그리고 다음 샘플 혹은 랜덤하게 $Q_1$과 $Q_2$의 역할을 바꿔 선택과 업데이트를 한다. 이런 아이디어를 **double learning**이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning에 double learning을 적용하면 각 time-step마다 동전을 던저서 동전 앞면이 나오면 $Q_1$을 이용해 최적 action을 찾고 $Q_2$를 업데이트 한다. 반면 동전 뒷면이 나오면 $Q_1$과 $Q_2$의 역할을 바꾼다.\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q_1(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma Q_2\\big( S_{t+1}, \\text{argmax}_a Q_1(S_{t+1}, a) \\big) - Q(S_t, A_t) \\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고로 Double learning을 적용한 Sarsa나 Expected Sarsa도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Games, Afterstates, and Other Special Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
