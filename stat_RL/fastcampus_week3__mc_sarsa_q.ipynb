{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy가 얼마나 좋은지를 평가하는 것을 Prediction, 여러 .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> transition probablility $P$가 포함되어 있음. 즉 모델에 대한 정보가 주어졌다는 것을 가정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.Reinforcement Learning에서는 model을 모르는 상태에서 어떻게 풀 것인지 고민\n",
    "\n",
    "\n",
    "> model-free prediction & model-free control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model-free란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MC? 에피소드를 끝까지 해보고 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Incremental average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\gamma = 1$, $R_t = -0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\text{1st episode} \\\\\n",
    "G_{(1,1)}^1 &= -0.1 \\times5 + 1 = 0.5 \\\\\n",
    "G_{(2,1)}^1 &= -0.1 \\times4 + 1 = 0.6 \\\\\n",
    "G_{(3,1)}^1 &= -0.1 \\times3 + 1 = 0.7 \\\\\n",
    "G_{(3,2)}^1 &= -0.1 \\times2 + 1 = 0.8 \\\\\n",
    "G_{(3,3)}^1 &= -0.1 \\times1 + 1 = 0.9 \\\\\n",
    "V_{(1,1)}^1 &= 0 + 1/1 (0.5 - 0) = 0.5 \\\\\n",
    "V_{(2,1)}^1 &= 0 + 1/1 (0.6 - 0) = 0.6 \\\\\n",
    "V_{(3,1)}^1 &= 0 + 1/1 (0.7 - 0) = 0.7 \\\\\n",
    "V_{(3,2)}^1 &= 0 + 1/1 (0.8 - 0) = 0.8 \\\\\n",
    "V_{(3,3)}^1 &= 0 + 1/1 (0.9 - 0) = 0.9 \\\\\n",
    "\\\\\n",
    "\\text{2nd episode} \\\\\n",
    "G_{(1,1)}^1 &= -0.1 \\times6 - 1 = -1.6 \\\\\n",
    "G_{(2,1)}^1 &= -0.1 \\times5 - 1 = -1.5 \\\\\n",
    "G_{(3,1)}^1 &= -0.1 \\times4 - 1 = -1.4 \\\\\n",
    "G_{(3,2)}^1 &= -0.1 \\times3 - 1 = -1.3 \\\\\n",
    "G_{(3,3)}^1 &= -0.1 \\times2 - 1 = -1.2 \\\\\n",
    "G_{(2,3)}^1 &= -0.1 \\times1 - 1 = -1.1 \\\\\n",
    "V_{(1,1)}^1 &= 0.5 + 1/2 (-1.6 - 0.5) = -0.55 \\\\\n",
    "V_{(2,1)}^1 &= 0.6 + 1/2 (-1.5 - 0.6) = -0.45\\\\\n",
    "V_{(3,1)}^1 &= 0.7 + 1/2 (-1.4 - 0.7) = -0.35 \\\\\n",
    "V_{(3,2)}^1 &= 0.8 + 1/2 (-1.3 - 0.8) = -0.25 \\\\\n",
    "V_{(3,3)}^1 &= 0.9 + 1/2 (-1.2 - 0.9) = -0.15 \\\\\n",
    "V_{(2,3)}^1 &= 0 + 1/1 (-1.1 - 0) = -1.1 \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모든 샘플 수 $1/N(s_t)$를 사용하는건 모든 샘플이 같은 가치가 있다고 생각하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy가 online으로 변하게 된다면, 최근 샘플에 더 가중치를 줘야한다. 이를 위해 지수 이동 평균을 사용. 현재 갖고 있는 추정치는 $(1-\\alpha)$만큼 사용하고, 샘플로 얻은 정보는 $\\alpha$만큼 사용하겠다.\n",
    "\n",
    "> $$\\begin{align}\n",
    "V_{k} &= \\alpha G_{t-1} + (1-\\alpha)V_{k-1}  \\\\\n",
    "&=\\alpha G_{t-1} + (1-\\alpha) \\bigg[ (1-\\alpha)V_{k-2} + \\alpha G_{t-2} \\bigg]  \\\\\n",
    "&=\\alpha \\bigg[ G_{t-1} + (1-\\alpha) G_{t-2} \\bigg] + (1 - \\alpha)^2 V_{k-1} \\\\\n",
    "&=\\alpha \\bigg[ G_{t-1} + (1-\\alpha) G_{t-2} + (1-\\alpha)^2 G_{t-3} \\bigg] + (1-\\alpha)^3 V_{k-2}\\\\\n",
    "&\\quad\\vdots \\\\\n",
    "&=\\alpha \\bigg[ G_{t-1} + (1-\\alpha) G_{t-2} + (1-\\alpha)^2 G_{t-3} + (1-\\alpha)^3 G_{t-4} + \\cdots \\bigg] \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 에피소드가 끝나야 업데이트 할 수 있는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TD: 조금식만 해봐도 된다. bootstrapping(뭔가를 추정할 때 전에 추정했던 값을 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MC에서는 $G_t$를 업데이트 타겟으로 사용했다면, TD에서는 $R_{t+1} + \\gamma \\hat{V}(S_{t+1})$을 타겟으로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오타) $\\gamma=1$, $\\alpha=0.1$\n",
    "\n",
    "\n",
    "> $R=-0.04$    \n",
    "> 초록색이나 빨간색 셀에 도달하면 에피소드 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy Evaluation에서 $V_k(s')$를 이용해 $V_{k+1}(s)$를 업데이트 했었다.     \n",
    "TD에서도 마찬가지로 이전 시점에 추정한 다음 상태의 가치함수를 사용하여 현재 시점 추정치를 업데이트 한다. (Bootstrapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MC는 에피소드가 끝나야 $V$를 업데이트 할 수 있다.   \n",
    "TD를 쓰면 infinit horizon도 풀수 있게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TD 추정치는 MC 추정치에 비해 variance가 작다. 반면 상대적으로 bias가 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> full backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> green: model-free ( MC)     \n",
    "> blue : DP     \n",
    "> pink : TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Bootstrapping: DP, TD\n",
    "> - Sampling: MC, TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $G_t^{(1)}$은 1-step TD의 타겟, $G_t^{\\infty}$는 MC의 타겟\n",
    "> - MC Part: lower bias, TD Part: lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> n-step TD는 n step만큼 본 후에 업데이트 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $G_t^{(n)}$은 n-step TD return으로서, n-step까지의 미래 정보를 포함한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$G_t^{\\lambda}=(1-\\lambda)\\bigg[ G_t^{(1)} + \\lambda G_t^{(2)} + \\lambda^2 G_t^{(3)} + \\lambda^3 G_t^{(4)} + \\cdots + \\lambda^{T-t-2} G_t^{(T-t-1)} \\bigg] + \\lambda^{T-t-1} G_t^{T-t}$$\n",
    "\n",
    "> 각 n-step return들을 weighted sum. 미래의 샘플은 더 작게 보겠다..\n",
    "\n",
    "\n",
    "> inf 를 $T-t-1$로 바꾸면\n",
    "\n",
    "\n",
    "> $T$이후 tail 부분을 $\\lambda^{T-t-1} G_t^{T-t}$로 퉁치겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\lambda=0$이면 1-step TD\n",
    "- $\\lambda=1$이면 MC와 같아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $G_t^{(n)}$는 n-step을 본 후 $S_t$에서의 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\begin{align}\n",
    "G_0^{(1)} &= -0.1 + 0 \\\\\n",
    "G_0^{(2)} &= -0.1 - 0.1 + 0 \\\\\n",
    "G_0^{(3)} &= -0.1 - 0.1 - 0.1 + 0 \\\\\n",
    "G_0^{(4)} &= -0.1 - 0.1 - 0.1 - 0.1 + 0 \\\\\n",
    "G_0^{(5)} &= -0.1-0.1-0.1-0.1-0.1 = -0.5 \\\\\n",
    "G_0^{(6)} &= -0.1-0.1-0.1-0.1-0.1+1 = 0.5 \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 각 상태마다 그 상태를 얼마나 자주, 최근에 방문했는지를 eligibility trace로 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 시간이 갈수록 $\\gamma \\lambda$만큼 줄어들고 해당 상태에 자주 방문할 수록 커지는 'Eligibility Trace'라는 값을 각 상태마다 보관한다. 즉 얼마나 최근에 그 상태를 방문했는지와 자주 방문했는지를 결합한 값이라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Backward view 방식의 TD($\\lambda$)에서는 각 상태의 가치함수를 TD-error $\\delta_t$와 eligibility trace $E_t$에 비례하여 업데이트 한다.\n",
    "\n",
    "> Forward view 방식의 TD($\\lambda$)에서는 $t$ 시점 상태의 가치함수를 업데이트 하기 위해 $t$시점 이후의 경험들을 사용했다면, Backward view 방식의 TD($\\lambda$)에서는$t$ 시점 상태의 가치함수를 업데이트 하기 위해 $t$시점 이전의 경험들을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $E(S_t) \\leftarrow E(S_t) + 1$은 특성 상태 $S_t$를 방문 했다는 빈도에 대한 가산이고, $E(s) \\leftarrow \\gamma \\lambda E(s)$는 시점이 지나갔으므로 모든 상태의 eligibility trace를 $\\gamma \\lambda$만큼 할인하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-a/week03-a-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MC, TD, TD($\\lambda$)를 이용해 주어진 정책에 대한 $V$나 $Q$를 찾을 수 있었다. 이를 model-free prediction이라 한다. 이렇게 찾은 $V$나 $Q$를 이용해 정책을 개선해야하는데, (DP와는 달리) greedy policy로는 문제가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 환경의 정보가 주어진 경우 DP를 이용해 Q를 계산할 수 있었고, greedy하게 policy를 구할 수 있었다. 하지만 환경의 정보가 없는 상황(model-free)에서는 sampling을 이용한 방법(TD, MC)을 이용해 $v_{\\pi}$를 추정해야 했다. 그런데 샘플링을 하고 있므로 노이즈가 존재하고, 이 경우 greedy하게 policy를 업데이트하면 특정 행동들은 이후 샘플링 되지 않게 된다. 만약 이때 샘플링 되지 않은 것들이 정말 중요한 정보를 갖고 있다면 최적 Q를 찾지 못하게 되고 최적 정책에도 도달하지 못하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy Iteration을 model-free로 바꿔보면!\n",
    "- sampling & Q-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 우선 Policy Improvement를 할 때 $V$를 이용해 $Q$를 구하는데 $P$가 필요 했음. 이 필요성을 없애기 위해 바로 $Q$ 함수를 찾기로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> evaluation에서 $P_{\\pi}$ 필요성을 없애려면, sampling method를 쓴다.(model free policy iter)\n",
    "\n",
    "> 참고로 q learning은 value-iteration을 model-free로 바꾼 것\n",
    "\n",
    "> model-free, on-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> greedy policy improvement는 local optima에 빠질 가능성이 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $m$은 $s$에서 가능한 행동의 수\n",
    "\n",
    "\n",
    "> $\\epsilon$만큼은 당장 최적이 아닌 행동에도 가능성을 주고, $1-\\epsilon$만큼은 현재 최적인 행동을 선택하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> greedy update에서 $v_{\\pi}^2 \\geq v_{\\pi}^1$임을 확인 했었는데, epsilon greedy improv에서도 그런 성질이 성립한다.\n",
    "\n",
    "> $v_{\\pi_{i+1}} \\geq v_{\\pi_{i}}$, 좋아지긴하는데, 이전에 알던것보다 좋아진다는 것\n",
    "\n",
    "> exploitation만큼 improvement가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> off-line & model-free Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 완성~, 매 샘플 에피소드마다 evaluation(1번), improvement를 한다. 에피소드가 끝나야 업데이트 가능한 MC와 forward TD를 쓰고 있기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - SARSA = on-line & model-free(with TD or backward TD($\\lambda$)) Q-Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SARSA\n",
    "\n",
    "> 매 에피소드가 아니라 그냥 매 샘플마다 하자. -> SARSA, 여기서는 온라인 러닝이 가능한 TD와 Backward TD를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\epsilon=0.1$, $\\alpha=0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 왼쪽 그램의 아래 숫자는 각 컬럼의 바람 세기\n",
    "\n",
    "> SARSA를 적용한 결과로서, 에피소드가 증가할 수록 완료하는데 필요한 time-step 수가 줄어들고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Q$ 함수를 업데이트할 때 TD($\\lambda$)를 사용 -> SARSA($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 두번째 그림은 1 step TD\n",
    "\n",
    "> 세번째 그림은 backward TD(lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 아래 두 조건을 만족할 때 SARSA는 수렴.\n",
    "> - Policy가 결국 greedy policy에 수렴\n",
    "> - step size $\\alpha$도 지속적으로 줄여줘야 함(Robbins-Monro)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 이미 그 행동을 잘 하는 인간이나 여타 에이전트가 따르는 연속된 행동-상태가 있다면 그 경로를 따라 샘플을 뽑고 내 정책을 학습시킨다면 좋을 것이다.\n",
    "- 이전 정책을 따라 생성한 에피소드들을 다시 이용할수도 있다. 이전 정책은 단지 특정 상태에서 어떤 행동을 선택할지를 결정하고, 이 선택에 대한 결과(다음 상태, 보상)은 환경이 결정하므로 이 데이터를 다시 이용해 현재 정책을 업데이트 할 수 있다.\n",
    "- 학습하는 정책으로 생성된 에피소드는 확증편향과 같이 특정한 에피소드들말 생성된다. 만약 이 에피소드가 아니라 더 나은 경로가 있었다면 이것들은 무시된다. 이런 문제를 피하기 위해 학습하는 정책과는 다른 정책을 이용해 에피소드를 샘플링하여 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - On-Policy: 샘플을 뽑는 정책($\\pi$)과 학습시키는 정책($\\pi$)이 동일\n",
    "- Off-Policy: 샘플을 뽑는 정책($\\mu$)과 학습시키는 정책($\\pi$)이 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf)\n",
    "> - model-based: 알려져 있는 혹은 학습한 $P(s' \\mid s, a)$를 사용\n",
    "- model-free: $P(s' \\mid s, a)$없이 정책(혹은 가치함수)를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 샘플링하는 정책 $\\mu$과 학습하는 정책 $\\pi$이 다르다면? 샘플링으로 구한 $G_t^{\\mu}$가 우리가 알고자 하는 $G_t^{\\pi}$와 다를 것이다. 이 둘의 차이를 보정하기 위한 방법이 'Importance Sampling'으로서 두 분포의 비율만큼 가중합 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-b/week03-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Importance sampling을 TD에 적용하면 위와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Importance sampling에 문제가 있어.. 분포의 비율이 엄청 커지기도 하거든"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Why don't we use importance sampling for one step Q-learning? \n",
    " - https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning\n",
    " - https://www.quora.com/Why-doesn-t-DQN-use-importance-sampling-Dont-we-always-use-this-method-to-correct-the-sampling-error-produced-by-the-off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q & model-free & off-policy value iteration = Q Learning\n",
    "\n",
    "> value iteration에서는 v_pi를 찾은 후 optimal policy를 찾으려면 V에서 Q를 구해야하고 이때 P가 필요한 문제가 있다. -> 그럼 그냥 처음부터 Q_pi를 찾자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 어떤 behavior policy를 쓰더라도 모든 state-action을 훑을 수 있다면, q-learning은 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\begin{align}\n",
    "\\color{red}{A_{t}} \\sim& \\mu(\\cdot \\mid S_t) \\\\\n",
    "\\color{blue}{A'} \\sim& \\pi(\\cdot \\mid S_t) \\\\\n",
    "Q(S_t, \\color{red}{A_t}) \\leftarrow& Q(S_t, \\color{red}{A_t}) + \\alpha(R_{t+1} + \\gamma Q(S_{t+1}, \\color{blue}{A'}) - Q(S_t, \\color{red}{A_t}))\n",
    "\\end{align}$$\n",
    "\n",
    "> $\\mu$는 단지 어떤 상태-행동에 해당하는 $Q$를 업데이트할지에만 영향을 준다. $Q$를 어떻게 업데이트하는지에는 $\\mu$에서 샘플링하는 것이 없다. 업데이트 방식에 영향을 주는 것은 단지 TD target을 어떻게 선택하는가의 문제이며, 이 선택 방식이 우리가 학습하는 정책이 되는 것이다. $\\mu$가 모든 상태-행동 쌍을 탐색하게만 해 준다면 이 방법은 최적 $Q$함수에 이르게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$로 업데이트, $\\max_{a'}\\hat{Q}(s', a')$를 결정하기 위해 $a'$를 샘플링하지 않아도 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 탐색(exploration, behavior)을 위한 정책이 모든 상태-행동 쌓을 무수히 많이 탐색할 수 있다는 것이 보장되면, 이 정책에 무관하게 Q-learning은 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 탐색 정책을 $\\epsilon$-greedy로 사용할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week03-c/week03-c-8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP(Model based)\n",
    " - $S_t, A_t, R_{t+1}, G_t$\n",
    " - $v(s_t)$, $Q(s_t, a_t)$\n",
    " - Bellman expectation equation, Bellman optimality equation\n",
    " - Policy Iteration, Value Iteration\n",
    "- Sampling(Model free)\n",
    " - MC\n",
    " - TD\n",
    " - TD($\\lambda$)\n",
    " - SARSA(on-policy)\n",
    " - Q-learning(off-policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
