{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RLLAB 박사과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 강의는 크게 3부분으로 나뉨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 인터넷이도 좋은 자료가 많음.\n",
    "- Sutton 교수 책 : http://www.incompleteideas.net/book/the-book-2nd.html\n",
    "- Silver 교수 강의 : http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "- 버클리 강의 : http://rll.berkeley.edu/deeprlcourse/\n",
    "- 게임에 적용한 예 : https://www.slideshare.net/deview/ai-67608549\n",
    "- Coursera 강의 : https://www.coursera.org/learn/practical-rl\n",
    "- Udacity 강의 : https://www.udacity.com/course/reinforcement-learning--ud600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - not IID : 이전 행동이 다음 행동의 결과에 영향을 줌.\n",
    "- non-stationary : 행동에 따라 정책이 바뀌고 에피소드 혹은 time-step 데이터의 분포가 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 3가지 이슈에 대해 알아본다. Reward, Sequential Decision Making, Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 행동에 대한 보상, 그 행동이 얼마나 좋은 것인지를 알려주는 값이라 할 수 있다.\n",
    "> - 결국 받게될 보상의 합을 최대화 하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 보상을 어떻게 설계할 것인가? 좋은 결과에에 이르게 하는 행동에는 양수 보상을, 나쁜 결과에 이르게 하는 행동에는 음수 보상을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 특정 시점에서 이후 받게될 보상의 (할인된)합이 최대화되도록 해야 한다. 그런데 문제는 이 보상을 행동 직후 알 수 없는 경우가 많다. 예를들어 바둑을 둔다면 게임이 끝나봐야 보상을 알 수 있다. 또한 현재 약간 손해를 보는 행동이 나중에는 큰 보상을 가져오는 행동일수도 있다. 이런 여러가지 상황들을 고려한 의사결정 문제를 풀어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트가 행동하는 환경에는 불확실성이 존재한다. 지금 좋은 보상을 가져온 행동이 나중에는 그렇지 않을수도 있다. 즉 행동에 따른 결과가 확률적이고 확률 분포 또한 변한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 세상을 에이전트와 환경으로 설계한다. 에이전트는 보상과 관측을 통해 다음에 할 행동을 만들어낸다. 환경은 에이전트와 상호작용하는 주변의 모든 것을 의미한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경은 에이전틔 행동에 보상과 상태 변화를 내놓는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트는 자신의 행동에 따른 환경의 반응(보상과 상태변화)을 토대로 누적된 보상을 최대화하는 행동을 선택하려 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 이전 상태/행동들에서 관측되었던 정보들으..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경의 상태, 환경이 다음에 어떤 관측과 보상을 줄지와 같이 세상이 어떻게 반응할지에 대한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트가 얻을 수 있는 상태 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 다음 상태를 결정할 때 필요한 모든 정보량을 하나로 압축해서 information state이라 부름, information state이 주어져 있으면 다음에 무슨일이 일어날지는 이 information state에 의해서만 결정된다.\n",
    "\n",
    "> - 0초부터 t-1초까지 일어난 일들에 대한 정보를 갖고 있다면 다음에 일어날 일은 t-1번째 information state만 주어져 있으면 된다. 이를 markov process라 한다. 즉 다음 상태의 상황은 현재 상태의 정보에만 의존한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 용을 피해서 보석에 도달해야하는 예시, 각 문직임미다 -0.08의 부정적 보상을 받기 때문에 가능한 최소의 움직임으로 목표에 도달해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 바둑과 같이 실전 문제에서는 상태가 매우 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 위 3가지를 살펴보면..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 정책은 상태가 들어오면 행동을 알려주는 함수이다. 결정적으로 행동을 알려줄수도 있고 행동에 대한 보상의 확률 분포를 알려주는 것도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 미래에 내가 받을 보상의 기댓값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 내 상태가 다음에 어떻게 변할지를 예측해주는 것, 즉 다음 상태 변화를 확률 분포로 알려주고 보상을 알려주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 3가지 개념을 어떻게 사용하느냐에 따라 \n",
    "\n",
    "> - value based : 가치함수를 사용\n",
    "> - policy based : 가치함수를 통하지 않고 바로 정책을 학습\n",
    "> - 두가지를 섞는 것이 AC\n",
    "> - 모델을 만들 샘플링해서 정책을 학습하는 것이 model based, 모델을 만들지 않고 바로 정책이나 가치함수를 학습하는 것이 model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - RL : 환경과의 상호작용으로 정보(상태/보상)를 얻고 이를 이용해 정책을 개선하는 것이 RL\n",
    "> - Planning : 환경에 대한 모델이 주어져 있거나 학습된 모델이 있을 때, 에이전트가 이 모델과 상호작용을 통해 정보를 얻고 이를 이용해 정책을 개선하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경이 어떻게 생겼는지 모르는 상태에서 계속되는 보상 관측을 통해 학습\n",
    "- 환경이 어떻게 변하는지 아는 상황에서 \n",
    "\n",
    "\n",
    "planning vs RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 모험을 해보느냐 아니면 현재 알려진 가장 좋은 행동을 하느냐, 탐험을 해보지 않으면 숨겨진 좋은 길을 갈수 없어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Prediction : 정책이 주어져 있을 때 그 정책에 따라 행동했을 때 미래 보상을 예측\n",
    "- Control : 정책을 개선하는 것\n",
    "\n",
    "> 주어진 정책으로 Prediction한 후 이 결과를 바탕으로 Control 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Event : 시행, 주사위를 한번 던진 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 이번트의 집합을 Sample Space라 함, 주사뤼를 한번 던지는 이벤트의 sample sapces는 $\\{1, 2, 3, 4, 5, 6\\}$\n",
    "- Sample space의 부분집합의 집합을 Sigma-algebra라 함. 예를들어 $\\{ ... \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 집합연산들... 교/하/여/차 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigma-algebra는 몇가지 특성이 있어야 함.\n",
    "\n",
    "Sample space가 원소이어야 함, 어떤 A가 원소이면 이걸 뺀것도 원소이어야 하고, 어떤 것들이 원소라면 이것들의 합집합도 원소이어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률이란 이벤트들의 집합을 하나 넣어주면 그 집합에 해당하는 어떤 실수를 주는 녀석, 즉 집합에 정의된 함수이다. 즉 Sigma-algebra의 원소에 대응하는 실수를 주는 녀석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률은 당연히 0과 1 사이의 값이어야 하고, 일어날 수 있는 모든 이벤트 확률의 합은 1이고, 공집합의 확률은 0이고, 겹치지 않는 이벤트 집합에 대한 합집합의 확률은 각 원소 확률의 합과 같이야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\{H, T\\}$는 H나 T가 나올 확률\n",
    "\n",
    "이 성질만 만족하면 probability measure라 할 수 있다. 즉 P(H)=1/3, P(T)=2/3이라 해도 이 성질을 만족시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률변수, 이벤트를 어떤 실수로 메핑하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들어 코인의 앞면이나 뒷면이 나오는 것이 이벤트라면, 앞면이 나오는 이벤트는 1이라하고, 뒷면이 나오는 것을 0이라 할 수 있다. 즉 각 이벤트를 실수로 메핑하는 것을 확률변수라 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(X=0)이라는 것은 P({T})와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y에 대한 사전정보가 주어져 있을 때, X를 관측하고, 이 근거가 주어졌을 때 Y의 분포 P(Y|X)를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-41.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-42.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-44.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-45.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 에이전트가 환경과 상호작용을 하면서 순차적으로 행동을 결정하는 상황이 있을 때, 어떤 목표를 이루도록 최적의 행동정책을 알아내는 것이 우리의 목표다. 이런 상황을 모델링하는 일반적인 방법이 MDP인데, 에이전트는 현재 상태 $S_t$에 놓여 있을 때 행동 $A_t$를 하게 되고, 환경은 $(S_t, A_t)$를 입력으로 받아 새로운 상태 $S_{t+1}$와 보상 $R_{t+1}$을 반환한다. 이제 에이전트는 새로운 상태 $S_{t+1}$로 옮겨가게 되고, 환경이 알려준 보상 $R_{t+1})$을 토대로 누적된 보상 $G_t \\doteq R_{t+1} + R_{t+2} + \\cdots + R_T$를 최대화 하는 방향으로 다음 행동 $A_{t+1}$을 선택한다. 다시 환경은 입력 $(S_{t+1}, A_{t+1})$에 대한 출력 $(S_{t+2}, R_{t+2})$를 내놓고 이런 상호작용이 반복된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마코프 성질을 만족시키는 연이은 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 상태 집합(State Space) $\\mathcal{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $t$ 시점의 상태는 $\\mathcal{S}$중 하나로 결정되는 확률변수 $S_t$라 할 수 있다.     \n",
    "실제 사건이 일어나 관측된 상태는 $s$로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 마코프 성질: 현재 상태는 이전 상태에만 영향 받는다. - 모델링 문제\n",
    "\n",
    "> - 이전 상태가 주어졌을 때 지금 상태로 이동할 확률을 생각해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마코프 성질을 이용해 에피소드(연이은 상태)의 결합확률분포를 여러개의 확률 분포로 쪼갤 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 에피소드 $s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow s_4$가 일어날 확률은 시작점 $s_1$에서 시작할 확률에 $s_1$에서 $s_2$로 이동할 확률, $s_2$에서 $s_3$로 이동할 확률, $s_3$에서 $s_4$로 이동할 확률을 모두 곱한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 상태간 전이 확률은 상태쌍에 대한 확률이므로 행렬로 표현할 수 있고 이를 'Transition Probability Matrix'라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 전이 불가능한 상태쌍도 존재할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 에피소드는 시작에서 끝까지 연이은 상태가 실제로 일어난 사건 혹은 샘플이라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Markov Process는 연이은 상태를 표현한 것이었다. 그런데 강화학습에서는 각 상태에 도달했을 때의 보상을 추가로 생각해볼 수 있다. 이 경우 모형은 상태 사이에 보상 $R_t$가 추가된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 보상 또한 확률적으로 결정될 수 있으므로 확률 변수라 할 수 있고, 특정 상태에서의 기대보상을 보상함수(reward function, $r(s)$)라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 우리의 목표는 에이전트의 단기적 보상 $R_t$를 최대화 하는 것이 아니라, 환경과의 상호작용을 시작해서 끝날때까지 받는 보상들의 합을 최대화 하는 것이다.\n",
    "\n",
    "> - 어떤 상태 $S_t$에 있을 때 앞으로 받게될 누적 보상은 (확률적으로) 변한 수 있으며 확률변수 $G_t$(반환값, return)로 표현한다. 가장 간단한 형태의 $G_t$는 아래와 같이 정의할 수 있다.($R_t$와 마찬가지로 $G_t$ 또한 확률 변수이다.)\n",
    "$$G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_T$$\n",
    "\n",
    "> - 하지만 종료를 딱히 정의할 수 없는 문제에서는 위와 같은 보상은 무한대가 될수도 있다. 지속되는 문제(continuing tasks)까지도 표현하기 위해 할인(discounting, $0 \\leq \\gamma \\leq 1$)을 도입하며, 미래의 보상은 더 먼 미래일수록 $\\gamma$를 여러번 곱해서, 미래 보상의 현재 가치를 계산한다.\n",
    "$$G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma^{T-1} R_T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 보상을 할인하는 이유는? 당장 받는게 더 좋아서, 혹은 끝나지 않거나 상당히 오래 지속되는 이벤트를 표현하기 좋게 하기 위해서.\n",
    "\n",
    "> 할인의 정도는 $0 \\leq \\gamma \\leq 1$로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 보상 할인을 표현하기 위해 $t$ 시점 이후에 일어나는 보상은 $\\gamma^t$를 곱해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 상태 $S_t$에서 앞으로 받게될 누적된 보상을 확률변수 $G_t$로 표현할 수 있었다. 이에 대한 기댓값을 가치함수(value function, $v(s)$)라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $G_t$와 $G_{t+1}$의 재귀적 관계($G_t = R_{t+1} + \\gamma G_{t+1}$)를 이용해 가치함수를 아래와 같이 표현할 수 있다. \n",
    "$$\\begin{align}\n",
    "v(s) &= \\mathbb{E}[G_t \\mid S_t = s] \\\\\n",
    "&= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\mathbb{E}[Y] = \\mathbb{E}\\big[\\mathbb{E}[Y\\mid X]\\big]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-41.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-42.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-44.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-45.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-46.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-47.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-48.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 최적 가치함수(optimal value function)을 찾게 되면, 벨만 변환을 하더라도 $V$가 변하지 않는다. 또한 벨만 방정식이 위와 같이 단순히 선형 방정식이므로 닫힌해를 구할수도 있다. 물론 $P$에 따라 이 해를 못 구하는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-49.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-50.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MP: $S_0, S_1, S_2, \\cdots $ \n",
    "- MRP: $S_0, R_1, S_1, R_2, S_2, \\cdots $ \n",
    "- MDP: $S_0, A_0, R_1, S_1, A_1, R_2, S_2, \\cdots $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-51.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트가 현재 상태($S_t=s$)에서 행동($A_t=a$)를 하고, 환경은 이에 대한 응답으로 다음 상태($S_{t+1}=s'$)가 무엇인지와 다음 상태에 도달했을 때 받는 보상($R_{t+1}=r$)이 무엇인지를 알려준다.\n",
    "- 환경의 이런 반응을 토대로 에이전트는 다음에 할 행동을 결정하고, \n",
    "- 에이전트와 환경의 상호작용이 반복된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 어떤 상태($S_t = s$)에서 어떤 행동($A_t = a$)을 했을 때 도달하게될 다음 상태($S_{t+1}=s'$)에 도달할지와 이때 받게될 보상($R_{t+1}=r$)은 상황에 따라 달리질 수 있고, 이에 대한 확률 분포를 생각해볼 수 있다. 이걸 알게 되면 에이전트의 행동에 대한 환경의 (확률적)반응을 모두 알 수 있다.\n",
    "$$p(s', r \\mid s, a) \\doteq \\Pr\\big\\{ S_{t+1}=s', R_{t+1} = r \\mid S_{t}=s, A_t = a  \\big\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위 확률분포를 이용해 상태-전이-확률(state-transition probability)를 아래와 같이 구할 수 있다.\n",
    "$$p(s' \\mid s, a) \\doteq \\Pr\\big\\{ S_{t+1}=s'\\mid S_{t}=s, A_t = a  \\big\\} = \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-52.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 보상을 보다 일반적으로는 현재상태-행동-다음상태에 대한 함수로 아래와 같이 정의한다.\n",
    "$$r(s, a, s') \\doteq \\mathbb{E}[ R_{t+1} \\mid S_t = s, A_t = a, S_t = s'] = \\sum_{r\\in \\mathcal{R}} r \\frac{p(s', r \\mid s, a)}{p(s' \\mid s, a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-53.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-54.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 정책: 주어진 상태에서 어떤 행동을 할 확률, $\\pi(a \\mid s)$\n",
    "\n",
    "> - $p(s', r \\mid s, a)$가 환경의 반응을 설명한다면 $\\pi(a \\mid s)$는 에이전트의 행동을 설명\n",
    "\n",
    "> - $(s, a)$ 쌍에 대한 확률은 분포가 아니라 스칼라값이다.(deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-58.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$p(s', r \\mid s, a) p(a \\mid s) = \\frac{p(s', r, s, a)}{p(s, a)} \\frac{p(a, s)}{p(s)}=p(s', r, a \\mid s)$$\n",
    "$$\\sum_a \\sum_r p(s', r, a \\mid s) = p(s' \\mid s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $s$에서 $a$를 고르는 것은 $\\pi(s \\mid a)$에 의해 결정되고      \n",
    "이후 $s'$이 선택되는 것은 $p(s' \\mid s, a)$에 의해 결정된다.     \n",
    "(이 설명에서는 $(s, a)$에 따른 $r$은 결정되어 있다고 가정)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-59.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\pi$가 주어졌을 때 MDP가 MRP와 같아지는 것에 대한 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-60.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-61.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-62.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-63.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-64.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-65.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-66.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 정책에 따라 확률변수 $G_t$의 분포가 달라질 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-67.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 특정 정책 $\\pi$를 따랐을 때 상태 $s$의 기대반환값을 $v_{\\pi}(s)$로 표기\n",
    "\n",
    "> - 특정 정책 $\\pi$를 따랐을 때 상태 $s$에서 행동 $a$를 했을 때 기대반환값을 $q_{\\pi}(s, a)$로 표기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-68.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-69.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-70.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\gamma$ 어디갔나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-71.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-72.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
