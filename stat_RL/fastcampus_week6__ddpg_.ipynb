{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MC를 개선하기 위한 TD, TD(lambda)의 접근\n",
    "- Bootstrapping 하려면 value functio 또한 근사 해야 한다.\n",
    "- -> DDPG(Deepmind), GAE(OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - DPG(Deterministic Policy Gradient Algorithms), 2014\n",
    " - https://deepmind.com/research/publications/deterministic-policy-gradient-algorithms/\n",
    "\n",
    "\n",
    "> - DDPG(Deep Deterministic Policy Gradients), 2016\n",
    " - Continuous control with deep reinforcement learning\n",
    " - https://arxiv.org/abs/1509.02971\n",
    " \n",
    " \n",
    "> - GAE(Generalized Advantage Estimation), 2016\n",
    " - High-Dimensional Continuous Control Using Generalized Advantage Estimation\n",
    " - https://arxiv.org/abs/1506.02438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variance Reduction methods\n",
    "> 1. Q function을 appriximate하는 접근 -> DDPG\n",
    "> - base line을 쓰는 접근 $G_t - b$\n",
    "> - GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variance를 줄이는 방법중 하나로 $G_t$에서 baseline($b$)를 빼주는 방법이 있다. \n",
    "- $b$를 빼면 policy gradient가 틀어진다거나 하지는 않을까? - policy gradient에서 $b$만 포함된 항을 계산해보면 0이 된다. 상수항의 expectation이 0이므로 gradient의 방향을 바꾸지 않는 것이다. 하지만 variance를 줄이는 효과는 있다.\n",
    "- 그렇다면 variance를 가장 많이 줄일 수 있는 $b$값은 무엇일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy gradient의 variance를 최소화하는 b를 찾기 위해 $Var[\\text{policy gradient}]$를 $\\mathbb{E}[\\cdot^2] - \\mathbb{E}[\\cdot]^2$ 형태로 정리하고, $b$에 대해 미분한 값을 0으로 놓고 정리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy gradient의 variance를 최소화하는 $b$를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 22:25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q에서 'function of state'을 빼도 미분의 방향이 바뀌지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GAE는 TD(lambda)의 아이디어를 적용.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 두번째 줄은 $\\delta_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>On Policy Actor Critic</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - [1]. 47:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Actor-Critic with REINFORCE & GAE\n",
    " - training policy network and value network\n",
    " - using $G_t$ to train value network\n",
    " - using GAE($A_t^{(\\lambda)}$) as advantage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Policy Network: $\\pi_{\\theta}(\\cdot \\mid S_t)$\n",
    " - with optimize rule of $\\min_\\theta \\frac{1}{N} \\sum_i \\log \\pi_\\theta(a_i \\mid s_i) A_i^{(\\lambda)}$\n",
    " \n",
    "> - Value Network: $V_{\\phi}(S_t)$\n",
    " - with optimize rule of $\\min_\\phi \\frac{1}{N} \\sum_i 1/2 \\big(G_i - V_{\\phi}(s_i)\\big)^2$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Sampling 과정은 아래와 같다.(맞나???)\n",
    " 1. 에피소드의 시작 상태 $S_0$를 결정\n",
    " 2. Policy Network $\\pi_{\\theta}(\\cdot \\mid S_t)$에서 $A_0$을 샘플링\n",
    " 3. 환경에 $S_0, A_0$을 주고 $S_1, R_1$을 관찰\n",
    " 4. 에피소드가 끝날때까지 2, 3 과정을 반복\n",
    " 5. (몇개의 에피소드를 뽑나??)\n",
    " 6. $(S_0, G_0), (S_1, G_1), \\cdots, (S_T, G_T)$를 계산하고 이를 이용해 Value Network $V_{\\phi}(S_t)$의 $\\phi$를 업데이트\n",
    " 7. $\\delta_0, \\delta_1, \\cdots, \\delta_T$와 $A_0^{(\\lambda)}, A_1^{(\\lambda)}, \\cdots, A_{T+1}^{(\\lambda)}$을 차례로 계산하고 이를 이용해 $\\pi_{\\theta}(\\cdot \\mid S_t)$의 $\\theta$를 업데이트\n",
    " 6. 2번으로 돌아가 과정을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Actor-Critic with TRPO & GAE\n",
    " - training policy network and value network.(Actor-Critic)\n",
    " - using $G_t$ to train value network.\n",
    " - using GAE($A_t^{(\\lambda)}$) as advantage. (GAE)\n",
    " - using importance ratio and constraint.(TRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Actor-Critic with TRPO & GAE\n",
    " - training policy network and value network.(Actor-Critic)\n",
    " - using $G_t$ to train value network.\n",
    " - using GAE($A_t^{(\\lambda)}$) as advantage. (GAE)\n",
    " - using cripped importance ratio and constraint.(PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-a/week06-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> REINFORCE, TRPO, PPO에서는 현재 정책망을 이용한 샘플링으로 advantage를 계산해 사용했었다. 이를 발전시켜 value network를 별도로 학습시키고, 여기서 나온 value 값을 이용해 $A_t^{(\\lambda)}$를 계산해 advantage로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (오타) min -> max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>Off Policy Actor Critic</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - time2~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>On Policy DPG</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\frac{f(x+h) - f(x-h)}{h}$$, action 축으로의 미분임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Policy gradient in the DPG\n",
    "- update rule for policy parameter $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mu_theta(s, a) 오타... mu_theta(s)가 되어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>Off Policy DPG</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> objective function in the off-policy DPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Critic is just a Q-learning network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Actor is updating policy network with Off-Policy DPG, $Q$ value is extracted from critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>DDPG</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RL에서는 batch normalization이 별로 효과 없더라.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week06-b/week06-b-27.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
