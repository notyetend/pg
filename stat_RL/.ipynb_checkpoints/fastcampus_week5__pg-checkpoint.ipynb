{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지난 시간에는 value function을 더 이상 tabular 형태로 표현하지 않고, 다양한 함수추정 방법을 이용해 parameterized function으로 나타내는 방법을 다뤘다. 또한 Q-learning의 function appriximator로 deep neural network를 시용한 방법인 DQN에 대해 알아봤고, 이를 개선하는 몇가지 아이디어에 대해서도 다뤘다. 이런 아이디어로 DDQN, PER, Dueling Architecture를 소개했었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - RL 알고리즘들은 크게 Value based, Actor Critic, Policy based로 나눌 수 있다. 우선 Value based 알고리즘은 별도의 Policy function을 두지 않고, Q-function을 학습시킨 후 각 상태에서 Q 값을 가장 크게 하는 행동을 선택하여 policy를 결정한다. 반면 Policy based 알고리즘은 별도의 Value function(혹은 Q-function)을 두지 않고, 바로 Policy function을 학습시킨다. 마지막으로 Value function과 Policy function을 모두 사용하는 방법은 Actor Critic으로 분류된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 각 카테고리에 속하는 알고리즘들을 살펴보면, 우선 Value Iteration은 MDP 프레임으로 Q-function을 학습시키며, 이를 model-free로 변형한 것이 Q-learning이고, Q-learning의 function approximator로 Deep neural network를 사용한 것이 DQN이고, 이를 개선한 것이 DDQN, Dueling, PER이다. 이들 모두 Q-function을 학습시키므로 Value based 알고리즘이라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 다음으로 Policy Iteration은 그 이름으로 인해 Policy based일것 같지만 실제로는 학습과정에 Value function과 Policy function이 모두 사용되므로 Actor Critic에 속한다. 그리고 Policy Iteration의 model-free 버전이 SARSA이고, Value function과 Policy function의 function approximator로 Deep neural network를 사용한 것이 DDPG와 GAE이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 마지막으로 Policy based 알고리즘에 해당하는 MDP, RL 방법은 명확히 존재하지 않으며 Policy based 알고리즘의 시작이라 할 수 있는 Policy gradient 방법이 기본적으로 function approximation과 model-free를 사용하고 있다. Policy gradient의 연장 선상에 있는 방법이 REINFORCE, TRPO, PPO이며 오늘은 이것들에 대해 자세히 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Value-based는 Policy function을 바로 학습시키는 것이 아니라 Q-function을 먼저 구한 후 각 상태에서 Q-function값을 최대로 하는 action을 선택하여 정책으로 사용한다. 반면 Policy based는 Policy function을 바로 학습시킨다. \n",
    "- Value based 알고리즘에서는 Q-function을 먼저 학습시키고, 각 행동에서 Q 값을 최대로 하는 행동을 정책으로 사용한다. 그런데 만약 각 상태에서 할 수 있는 행동의 수가 매우 많다면 최대값을 구하기 위한 연산량이 커지게 되고, action space가 continuous하다면 action distribution에서 최대 값을 찾기 위한 최적화 과정이 또 필요하게 된다. 반면 Policy based 알고리즘은 Q 함수를 거치지 않고 바로 정책을 알 수 있으므로 Value based 알고리즘에서 정책을 찾을 때 발생하는 문제가 사라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy based 알고리즘의 또 다른 장점은 stochastic policy도 학습시킬 수 있다는 것인데, 이 내용은 다다음주(7주차)에 다룬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 가위바위보에서 최적의 정책은 무엇일까? 정답은 무작위로 내는 것이다. 만약 Value based 방법으로 이 문제를 풀면 가위-바위-보 각 선택에 대한 Q 값을 계산하고, 이값을 가장 크게 하는 행동을 고르게 될 것이다. 또한 샘플링 과정에서 한 방향의 행동이 더 큰 Q 값을 갖는다면 샘플링 또한 이 방향에 집중되기 때문에 이 행동을 더 지지하는 결과를 낳게 된다.\n",
    "\n",
    "> 위와 같은 Value based 방법 대신 행동에 대한 분포를 학습하여 확률적으로 행동을 결정하는 Policy based 방법을 사용하는 것이 더 나은 정책을 찾게 해주기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 로봇 컨트롤 같은 문제에서는 각 관절을 컨트롤하는 행동이 continuous한 action space를 갖는다. 이런 문제에는 policy based 알고리즘을 적용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MuJoCo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - action space에 따른 Policy Optimization의 일반적인 구조\n",
    "  - 좌: discrete action space, 우 : continuous action space\n",
    "\n",
    "\n",
    "\n",
    "> - action space에 따라 출력 레이어의 형태가 달라진다. 우선 discrete action space인 경우 좌측과 같이 상태가 입력으로 들어가 그 상태에서 가능한 행동에 preference를 찾고 마지막 레이어에 softmax를 붙여 확률 분포를 출력하는 형태가 되고, continuous action space인 경우 행동에 대한 확률 분포를 어떤 연속함수로 모델링하는데 주로 정규분포를 사용하며, 마지막 레이어에 정규분포의 평균과 분산을 출력으로 내놓도록 설계한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy Optimization 방법은 expected sum of discounted reward, $\\eta(\\pi_{\\theta}) = \\mathbb{E}\\big[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid \\pi_{\\theta}\\big]$를 목적함수로 하여 gradient ascent($\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} \\eta(\\pi_{\\theta})$)로 이 목적함수를 최대화 하는 $\\theta$를 찾는 것을 기본으로 한다. 이 최적화 문제를 어떻게 푸는지에 따라 여러 방법들로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\eta(\\pi_{\\theta})$의 $\\theta$에 대한 gradient를 구하는 것은 'Policy Gradient Theorem' 증명되어 있으며 그 결과는 위와 같다.\n",
    "\n",
    "> - $\\rho_{\\pi_{\\theta}}$: (아래 설명)\n",
    "- $Q^{\\pi_{\\theta}}(s, a)$: 지금 학습하고 있는 정책에 대한 Q function이므로 on-policy learning이라 할 수 있다. (물론 off-policy로 해도 수렴한다는 것이 증명되어 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\rho_{\\pi_{\\theta}}$는 시간에 대한 정보를 모두 포함하게 하는 수학적 trick으로서 특정 상태 $s$에 있을 확률을 의미한다.\n",
    "> - $P_{\\pi_{\\theta}}(S_t = s)$: 특정 time-step $t$에 특정 상태 $s$에 있을 확률, $\\mathbb{E}\\big[ \\mathbb{I}_{\\{S_t = s\\}} \\big]$와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\rho(s)$는 stationary distribution이라고 하고 에이젼트가 폴리시 $\\pi$를 따라서 움직일 때 훑고 지나가는 정도를 확률로 나타낸 것입니다. $P_{\\pi}(S_{t}=s)$는 시간 $t$일 때 에이젼트의 상태가 $s$일 확률을 의미합니다. 따라서 $\\rho$와 $P_{\\pi}$의 차이는 시간에 있습니다. $\\rho$는 모든 시간에 대해서 $s$를 얼마나 방문했는지를 확률로 나타내는 것이고 $P_{\\pi}$는 특정시간 $t$에서 $s$에 있을 확률을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $1-\\gamma$는 위 기댓값을 확률분포로 만들기 위한 가중치.\n",
    "- $1-\\gamma$을 사용하지 않으면 State visitation이라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy Gradient를 증명해보자.\n",
    "\n",
    "> 우리의 목적함수는 $\\eta(\\pi_{\\theta}) = \\mathbb{E}\\big[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid \\pi_{\\theta}\\big]$이고, 이를 최대화 하는 $\\theta$를 찾기 위해서는 $\\nabla \\eta(\\pi_{\\theta})$를 구해야 한다.\n",
    "1. $\\mathbb{E}\\big[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid \\pi_{\\theta}\\big]$는 에피소드의 시작상태에서의 return(sum of discounted reward)들의 기댓값이므로, '각 상태 가치'를 '각 상태에서 시작할 확률'로 가중치를 주고 합한 것이라 할 수 있다.\n",
    "- $\\nabla_{\\theta} V_{\\pi_{\\theta}(s_1)}$은 value function의 정의에 따라 Q function을 정책으로 평균낸 것의 미분이라 할 수 있다.\n",
    "- 미분 곱의 법칙에 따라 마지막 식과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> fc week5 time1 - 32:00 ~ 45:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Q function의 정의에 따라 $Q_{\\pi_{\\theta}}(s_1, a)$를 reward와 value-function으로 나타낼 수 있다.\n",
    "2. $r(s_1, a)$는 $\\theta$에 무관하므로 미분하여 사라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s')$에는 $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s'')$이 사용되고\n",
    "- $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s'')$에는 $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s''')$이 사용되는 식으로 $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s^?)$를 재귀적으로 표현할 수 있다.(Unrolling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s')$, $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s'')$와 같은 재귀적 항들을 $\\nabla_{\\theta} V_{\\pi_{\\theta}}(s_1)$에 넣어줄 수 있고..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\sum_{a'} \\nabla_{\\theta} \\pi_{\\theta} (a' \\mid s) Q_{\\pi_{\\theta}}(s', a')$$\n",
    "- 위 식에서 항들의 위치를 바꾸고, $\\sum_a \\gamma P(s' \\mid s_1, a) \\pi_{\\theta}(a \\mid s_1) = \\gamma P_{\\pi_{\\theta}}(S_1=s' \\mid S_0 \\ s_1)$이므로 마지막 식과 같이 정리된다.(슬라이드 마지막 식에 $\\gamma$가 빠짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마찬가지 과정을 다음 $\\sum_{a'}$에 대해서도 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{\\pi_{\\theta}}(S_t=s' \\mid S_0 = s) = \\begin{cases} 0 ~~ \\text{if } s'=s\\\\\n",
    "1 ~~ \\text{if } s' \\neq s \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $P_{\\pi_{\\theta}}(\\cot)$가 3번 등장하는데, 위에서부터 차례로 $S_0$에서 $S_0$으로 갈 확률, $S_0$에서 $S_1$로 갈 확률, $S_0$에서 $S_2$으로 갈 확률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 위 정의는 time-step이 무한히 진행되며, 할인율($\\gamma$)이 사용되는 경우에 해당한다.(Infinite horizon, discounted rewards)\n",
    "- $\\frac{1}{(1-\\gamma)}$ 항은 infinite horizon 문제에서만 사용된다.(사실 이 항은 확률분포로 만들어 주기 위한 normalization constant이다.)\n",
    "- 위 식에서는 performance measure로 sum of reward($\\sum \\gamma^t R_t$)를 사용하고 있으나, average reward를 이용하는 경우도 있다.($\\lim_{T\\rightarrow \\infty}??$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> gradient값을 sampling으로 계산하기 위해 log ratio trick을 사용할 수 있다.\n",
    "> - $\\nabla_{\\theta} \\mathbb{E}[f(x)]$: gradient of expectation over $x$, probability of x is parameterized with $\\theta$.\n",
    "> - log ratio trick은 $\\nabla_{\\theta} \\mathbb{E}[f(x)] = \\mathbb{E}[f(x) \\nabla^x_{\\theta} \\log_{p_{\\theta}(x)}]$이라는 것인데, $\\nabla$가 $\\mathbb{E}[]$안으로 들어가기 때문에 $f(x) \\nabla^x_{\\theta} \\log_{p_{\\theta}(x)}$를 sampling할 수 있다면 empirical하게 $\\nabla_{\\theta} \\mathbb{E}[f(x)]$를 구할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 결국 $\\nabla_{\\theta}\\eta(\\pi_{\\theta})$를 $\\nabla_{\\theta} \\log_{\\pi_{\\theta}}(a_t \\mid s_t) Q_{\\pi_{\\theta}}(s_t, a_t)$를 sampling하여 구할수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Softmax Policy** - discrete action space\n",
    "- 어떤 상태 $s$에서 가능한 행동들($b$)중 특정 행동 $a$에 대한 action preference를 함수 $f(s, a; \\boldsymbol{\\theta})$로 표현할 때, 상태 $s$에서 행동 $a$를 선택할 'Softmax Policy'를 아래와 같이 표현할 수 있다.\n",
    "$$\\pi_{\\boldsymbol{\\theta}}(a \\mid s, \\boldsymbol{\\theta}) = \\frac{\\exp(f(s, a;\\boldsymbol{\\theta}))}{ \\sum_b \\exp(f(s, b;\\boldsymbol{\\theta})}$$\n",
    "\n",
    "> - 만약 action perference function $f(s, a; \\boldsymbol{\\theta})$가 아래와 같이 state-action feature $\\mathbf{x}(s, a)$에 대해 linear한 function이라면\n",
    "$$f(s, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^T \\mathbf{x}(s, a)$$\n",
    "\n",
    "> - 이때의 softmax policy는 아래와 같다.\n",
    "$$\\pi_{\\boldsymbol{\\theta}}(a \\mid s, \\boldsymbol{\\theta}) = \\frac{\\exp(\\boldsymbol{\\theta}^T \\mathbf{x}(s, a))}{\\sum_b \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}(s, b))}$$\n",
    "\n",
    "> $$\\frac{\\exp(\\boldsymbol{\\theta}^T \\mathbf{x}(s, a))}{\\pi_{\\boldsymbol{\\theta}}(a \\mid s, \\boldsymbol{\\theta})}$$\n",
    "\n",
    "> - 또한 score function(gradient of log)은 아래와 같다.\n",
    "$$\\begin{align}\n",
    "\\nabla_{\\theta} \\log \\pi_{\\boldsymbol{\\theta}}(s, a) &= \\nabla_{\\theta} \\log \\bigg( \\frac{\\exp(\\boldsymbol{\\theta}^T \\mathbf{x}(s, a))}{\\sum_b \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}(s, b))} \\bigg) \\\\\n",
    "&= \\nabla_{\\theta} \\bigg( \\log \\big( \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}(s, a)) \\big) - \\log( \\sum_b \\exp \\big(\\boldsymbol{\\theta}^T \\mathbf{x}(s, b)) \\big) \\bigg) \\\\\n",
    "&= \\nabla_{\\theta} \\bigg( \\boldsymbol{\\theta}^T \\mathbf{x}(s, a) \\bigg) -  \\nabla_{\\theta} \\bigg( \\log( \\sum_b \\exp \\big(\\boldsymbol{\\theta}^T \\mathbf{x}(s, b)) \\big) \\bigg) \\\\\n",
    "&= \\mathbf{x}(s, a) - \\frac{\\nabla_{\\theta} \\exp( \\boldsymbol{\\theta}^T \\mathbf{x}(s, b)) }{\\sum_b \\exp( \\boldsymbol{\\theta}^T \\mathbf{x}(s, b))} \\\\\n",
    "&= \\mathbf{x}(s, a) - \\frac{\\sum_b \\mathbf{x}(s, b) \\exp( \\boldsymbol{\\theta}^T \\mathbf{x}(s, b)) }{\\sum_b \\exp( \\boldsymbol{\\theta}^T \\mathbf{x}(s, b))} \\\\\n",
    "&= \\mathbf{x}(s, a) - \\sum_b \\bigg( \\frac{\\exp( \\boldsymbol{\\theta}^T \\mathbf{x}(s, b)) }{\\sum_b \\exp( \\boldsymbol{\\theta}^T)}\\mathbf{x}(s, b) \\bigg) \\\\\n",
    "&= \\mathbf{x}(s, a) - \\sum_b \\bigg( \\pi_{\\boldsymbol{\\theta}(s, b)}\\mathbf{x}(s, b) \\bigg) \\\\\n",
    "&= \\mathbf{x}(s, a) - \\mathbb{E}_{\\pi_{\\boldsymbol{\\theta}}} \\bigg[ \\mathbf{x}(s, b) \\bigg] \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> - 즉 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Gaussian Policy** - continuous action space\n",
    "- 로봇 관절을 컨트롤하는 것과 같이 action space가 continuous하다면 policy function으로 Gaussian function으로 모델링할수 있다.\n",
    "- Gaussian function의 $\\mu$와 $\\sigma$를 각각 $\\boldsymbol{\\theta}$에 대한 함수로 둘수도 있고, $\\sigma$는 상수로 둘수도 있다. 후자의 경우 정책은 아래와 같다.\n",
    "$$a \\sim \\mathcal{N}\\big(\\mu(s), \\sigma^2\\big)$$ \n",
    "- 간단히 $\\mu$를 아래와 같은 state feature에 대한 linear function으로 두기도 한다.\n",
    "$$\\begin{align}\n",
    "\\mu &= \\boldsymbol{\\theta_{\\mu}}^T \\mathbf{x}(s)\n",
    "\\end{align}$$\n",
    "- 이때의 score function은 아래와 같다.\n",
    "$$\\nabla_{\\theta} \\log \\pi_{\\boldsymbol{\\theta}}(s, a) = \\frac{(a - \\mu(s)) \\mathbf{x}(s)}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 정리해보면 \n",
    "1. 목적함수 $\\eta(\\pi_{\\theta}) = \\mathbb{E}\\big[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid \\pi_{\\theta}\\big]$를 최대화 하는 $\\boldsymbol{\\theta}$를 찾는 것이 목표이다.\n",
    "2. Gradient Ascent를 사용하기 위해 gradient $\\nabla_{\\theta} \\eta(\\pi_{\\theta})$를 구해야한다.\n",
    "3. Policy Gradient Theorem에 의해 $\\nabla_{\\theta} \\eta(\\pi_{\\theta})$를 아래와 정리할수 있다.\n",
    "$$\\nabla_{\\theta} \\eta(\\pi_{\\theta}) = \\frac{1}{(1-\\gamma)} \\sum_s' \\sum_a \\nabla_{\\theta} \\pi_{\\theta}(a \\mid s') Q_{\\pi_{\\theta}}(s', a) \\rho_{\\pi_{\\theta}}(s')$$\n",
    "4. log ratio trick을 이용해 $\\nabla_{\\theta} \\eta(\\pi_{\\theta})$를 아래와 같이 근사할 수 있다.\n",
    "$$\\nabla_{\\theta} \\eta(\\pi_{\\theta}) \\approx \\nabla_{\\theta} \\log_{\\pi_{\\theta}}(a_t \\mid s_t) Q_{\\pi_{\\theta}}(s_t, a_t)$$\n",
    "5. Policy의 형태(Softmax, Gaussian 등)에 따라 $\\nabla_{\\theta} \\log_{\\pi_{\\theta}}(a_t \\mid s_t)$를 feature function $\\mathbf{x}$과 policy $\\pi_{\\boldsymbol{\\theta}}$의 함수로 표현할 수 있다.\n",
    "\n",
    "\n",
    "> 이제 필요한 것은 $Q_{\\pi_{\\theta}}(s_t, a_t)$를 어떻게 구하느냐 인데, Monte Carlo 방법을 이용해 sample return $G_t$로 대체할수 있고 이 방법이 바로 **REINFORCE** 알고리즘이다. 또는 직접 $Q_{\\pi_{\\theta}}(s_t, a_t)$를 함수 근사로 구할 수 있는데 이런 방법들을 **Actor Critic**이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Q$는 monte carlo sample return $G_t$로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-a/week05-a-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - TRPO, PPO - OpenAI's - in this lecture.\n",
    "> - DDPG - Deepmind's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minirization maximnization아니고.. conjugate descent, ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\rho_{\\pi'}(s) = E[\\sum \\gamma^t I]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마지막에 rho_pi'를 rho_pi로 바꿔버리는데, 그러면 이게 함수 M의 성질을 만족함을 다음에 보임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\theta_{i+1}$은 $\\pi'$의 parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 식에서 머리가 max_{delta_theta} 로 바껴야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 식에서 모든 max 대신 $\\rho$로 바껴야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TRPO의 직관 : 바꾼 정책과 현 정책이 너무 다르지 않으면좋겠어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week05-b/week05-b-23.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kyungjae.lee@cpslab.snu.ac.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
