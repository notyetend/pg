{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이전의 소개했던 방법들에서는 action-value function을 학습한 후 각 state에서 이 값($Q$)를 최대화하는 최적 정책으로 선택했었다. 반면 policy gradient 방법에서는 action-value function(혹은 value function)이 아니라 바로 policy function을 학습한다. 구체적으로는 시점 $t$에 어떤 상태 $s$에서 선택할 정책에 대한 확률을 $\\pi(a \\mid s, \\theta) = \\text{Pr}\\{ A_t=a \\mid S_t=s, \\theta_t = \\theta \\}$로 모델링한다. 그리고 performace measure $J(\\theta)$를 설정하고, 이를 maximize하도록 $\\theta$를 최적화 한다. 물론 value function을 함께 학습하는 방법도 있으며 이런 방법을 **actor-critic method**라 한다. 여기서 actor는 학습된 정책, critic은 학습된 가치함수를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.1 Policy Approximation and its Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selecting actions according to the softmax in action preference\n",
    "\n",
    "\n",
    "- $\\epsilon$-greedy action selection\n",
    "\n",
    "\n",
    "- softmax over action value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.2 The Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $J(\\theta) \\doteq v_{\\pi_{\\theta}(s_0)}$\n",
    " - This is the performance measure that is the value of the start state of the episode, where $v_{\\pi_{\\theta}(s_0)}$ is the true value function for $\\pi_{\\theta}$, the policy determined by $\\theta$.\n",
    " - We want to seed parameter $\\theta$ maximizing performance measure using gradient ascent likes $\\theta_{t+1} = \\theta_t + \\alpha \\widehat{\\nabla J(\\theta_t)}$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How can we find gradient of the performance measure $J(\\theta)$?\n",
    "- Policy Gradient Theorem: $$\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_{\\pi} (s, a) \\nabla_{\\theta} \\pi(a \\mid s, \\theta)$$\n",
    " - $J(\\theta)$: column vectors of partial derivatives with respect to the components of $\\theta$.\n",
    " - $\\pi$: policy corresponding to parameter vector $\\theta$\n",
    " - $\\mu$: on-policy distribution under $\\pi$(p161), (state distribution or fraction of time spend in $s$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.3 REINFORCE: Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.4 REINFORCE: with Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.5 Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.6 Policy Gradient for Continuing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.7 Policy Parameterization for Continuous Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
