{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이전의 소개했던 방법들에서는 action-value function을 학습한 후 각 state에서 이 값($Q$)를 최대화하는 최적 정책으로 선택했었다. 반면 policy gradient 방법에서는 action-value function(혹은 value function)이 아니라 바로 policy function을 학습한다. 구체적으로는 시점 $t$에 어떤 상태 $s$에서 선택할 정책에 대한 확률을 $\\pi(a \\mid s, \\boldsymbol{\\theta}) = \\text{Pr}\\{ A_t=a \\mid S_t=s, \\boldsymbol{\\theta}_t = \\boldsymbol{\\theta} \\}$로 모델링한다. 그리고 performace measure $J(\\boldsymbol{\\theta})$를 설정하고, 이를 maximize하도록 $\\boldsymbol{\\theta}$를 최적화 한다. 물론 value function을 함께 학습하는 방법도 있으며 이런 방법을 **actor-critic method**라 한다. 여기서 actor는 학습된 정책, critic은 학습된 가치함수를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.1 Policy Approximation and its Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 몇가지 조건을 만족하면 policy를 $\\pi(a \\mid s, \\boldsymbol\\theta)$로 parameterize할 수 있다. action space가 discrete인지 아니면 continuous한지에 따라 parameterization은 달라지는데, 이 섹션에서는 discrete action space에 대해 다룰 것이며, continuous action space에 대해서는 13.7에서 다룰 것이다.\n",
    "\n",
    "\n",
    "> - discrete action space에서 각 state-action 쌍에 대한 preference를 $h(s, a, \\boldsymbol\\theta)$로 나타낼 수 있으며, 각 상태 $s$에서 가능한 여러 행동들 중 $h(s, a, \\boldsymbol\\theta)$값을 가장 크게 하는 행동이 가장 높은 선택 가능성을 갖게 된다. 각 상태에서 가능한 행동을 선택할 확률 분포를 표현하는 방법중 하나가 아래와 같은 softmax distribution이다. (분모항은 사실 sum to one을 위한 normalization term이다.)\n",
    "$$\\pi(a \\mid s, \\boldsymbol\\theta) = \\frac{\\exp(h(s, a, \\boldsymbol\\theta)}{\\sum_b \\exp(h(s, b, \\boldsymbol\\theta)}$$\n",
    "\n",
    "\n",
    "> - perference function $h(s, a, \\boldsymbol\\theta)$를 모델링하는 하는 방법은 매우 다양하다. 예를들어 Deep neural network를 사용할수도 있고, 여타 다른 형태의 함수 근사 방법을 사용할 수 있다. 가장 간단히는 아래와 같이 feature function $\\mathbf x(s, a) \\in \\mathbb{R}^{d'}$에 대한 선형함수를 사용할 수 있다.(feature function에 대해서는 Chapter9에서 다뤘음)\n",
    "$$h(s, a, \\boldsymbol \\theta) = \\boldsymbol \\theta^{\\intercal} \\mathbf x(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.2 The Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\boldsymbol \\theta$로 parameterize한 policy $\\pi_{\\boldsymbol \\theta}$를 따랐을 때, 받게되는 reward들을 종합하여 **performance measure** $J(\\boldsymbol \\theta)$를 정의하는데, discrete action space와 continuous action space에서 그 정의 방식이 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - discrete action space에서의 **performance measure**를 아래와 같이 start state의 value로 정의한다.(모든 에피소드가 하나의 특정 상태에서 시작한다고 가정하며, $v_{\\pi_{\\boldsymbol{\\theta}}(s_0)}$는 정책 $\\pi_{\\boldsymbol{\\theta}}$에 대한 true value function이다.)\n",
    "$$J(\\boldsymbol \\theta) \\doteq v_{\\pi_{\\boldsymbol\\theta}}(s_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - performance measure $J(\\boldsymbol \\theta)$를 최대화하는 $\\boldsymbol\\theta$를 찾기 위해 아래와 같은 gradient ascent를 사용할 것이다.($\\widehat{\\nabla J(\\boldsymbol{\\theta}_t)}$는 그것의 평균이 $\\nabla J(\\boldsymbol{\\theta}_t)$에 근사하는  확률적 추정량이다.)\n",
    "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha \\widehat{\\nabla J(\\boldsymbol{\\theta}_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\nabla J(\\boldsymbol{\\theta}_t)$는 **Policy Gradient Theorem**을 이용해 아래와 같이 정리된다. (증명 과정은 Ch13.2을 참고)\n",
    "$$\\begin{align}\n",
    "\\nabla J(\\boldsymbol{\\theta}) &\\propto \\sum_s \\mu(s) \\sum_a q_{\\pi} (s, a) \\nabla_{\\boldsymbol{\\theta}} \\pi(a \\mid s, \\boldsymbol{\\theta}) \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\big[ \\sum_a q_{\\pi} (s, a) \\nabla_{\\boldsymbol{\\theta}} \\pi(a \\mid s, \\boldsymbol{\\theta}) \\big]\n",
    "\\end{align}$$\n",
    " 1. $J(\\boldsymbol{\\theta})$: column vectors of partial derivatives with respect to the components of $\\boldsymbol{\\theta}$.\n",
    " - $\\pi$: policy corresponding to parameter vector $\\boldsymbol{\\theta}$\n",
    " - $\\mu$: on-policy distribution under $\\pi$(p161), (state distribution or fraction of time spend in $s$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.3 REINFORCE: Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.4 REINFORCE: with Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.5 Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.6 Policy Gradient for Continuing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.7 Policy Parameterization for Continuous Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
