{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이전의 소개했던 방법들에서는 action-value function을 학습한 후 각 state에서 이 값($Q$)를 최대화하는 최적 정책으로 선택했었다. 반면 policy gradient 방법에서는 action-value function(혹은 value function)이 아니라 바로 policy function을 학습한다. 구체적으로는 시점 $t$에 어떤 상태 $s$에서 선택할 정책에 대한 확률을 $\\pi(a \\mid s, \\theta) = \\text{Pr}\\{ A_t=a \\mid S_t=s, \\theta_t = \\theta \\}$로 모델링한다. 그리고 performace measure $J(\\theta)$를 설정하고, 이를 maximize하도록 $\\theta$를 최적화 한다. 물론 value function을 함께 학습하는 방법도 있으며 이런 방법을 **actor-critic method**라 한다. 여기서 actor는 학습된 정책, critic은 학습된 가치함수를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.1 Policy Approximation and its Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selecting actions according to the softmax in action preference\n",
    "\n",
    "\n",
    "- $\\epsilon$-greedy action selection\n",
    "\n",
    "\n",
    "- softmax over action value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.2 The Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.3 REINFORCE: Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.4 REINFORCE: with Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.5 Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.6 Policy Gradient for Continuing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13.7 Policy Parameterization for Continuous Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
