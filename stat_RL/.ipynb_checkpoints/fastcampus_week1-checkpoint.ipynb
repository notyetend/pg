{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RLLAB 박사과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 강의는 크게 3부분으로 나뉨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 인터넷이도 좋은 자료가 많음.\n",
    "- Sutton 교수 책 : http://www.incompleteideas.net/book/the-book-2nd.html\n",
    "- Silver 교수 강의 : http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "- 버클리 강의 : http://rll.berkeley.edu/deeprlcourse/\n",
    "- 게임에 적용한 예 : https://www.slideshare.net/deview/ai-67608549\n",
    "- Coursera 강의 : https://www.coursera.org/learn/practical-rl\n",
    "- Udacity 강의 : https://www.udacity.com/course/reinforcement-learning--ud600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-a/week01-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - not IID : 이전 행동이 다음 행동의 결과에 영향을 줌.\n",
    "- non-stationary : 행동에 따라 정책이 바뀌고 에피소드 혹은 time-step 데이터의 분포가 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 3가지 이슈에 대해 알아본다. Reward, Sequential Decision Making, Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 행동에 대한 보상, 그 행동이 얼마나 좋은 것인지를 알려주는 값이라 할 수 있다.\n",
    "> - 결국 받게될 보상의 합을 최대화 하는 것을 목표로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 보상을 어떻게 설계할 것인가? 좋은 결과에에 이르게 하는 행동에는 양수 보상을, 나쁜 결과에 이르게 하는 행동에는 음수 보상을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 특정 시점에서 이후 받게될 보상의 (할인된)합이 최대화되도록 해야 한다. 그런데 문제는 이 보상을 행동 직후 알 수 없는 경우가 많다. 예를들어 바둑을 둔다면 게임이 끝나봐야 보상을 알 수 있다. 또한 현재 약간 손해를 보는 행동이 나중에는 큰 보상을 가져오는 행동일수도 있다. 이런 여러가지 상황들을 고려한 의사결정 문제를 풀어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트가 행동하는 환경에는 불확실성이 존재한다. 지금 좋은 보상을 가져온 행동이 나중에는 그렇지 않을수도 있다. 즉 행동에 따른 결과가 확률적이고 확률 분포 또한 변한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 세상을 에이전트와 환경으로 설계한다. 에이전트는 보상과 관측을 통해 다음에 할 행동을 만들어낸다. 환경은 에이전트와 상호작용하는 주변의 모든 것을 의미한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경은 에이전틔 행동에 보상과 상태 변화를 내놓는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트는 자신의 행동에 따른 환경의 반응(보상과 상태변화)을 토대로 누적된 보상을 최대화하는 행동을 선택하려 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 이전 상태/행동들에서 관측되었던 정보들으..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경의 상태, 환경이 다음에 어떤 관측과 보상을 줄지와 같이 세상이 어떻게 반응할지에 대한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 에이전트가 얻을 수 있는 상태 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 다음 상태를 결정할 때 필요한 모든 정보량을 하나로 압축해서 information state이라 부름, information state이 주어져 있으면 다음에 무슨일이 일어날지는 이 information state에 의해서만 결정된다.\n",
    "\n",
    "> - 0초부터 t-1초까지 일어난 일들에 대한 정보를 갖고 있다면 다음에 일어날 일은 t-1번째 information state만 주어져 있으면 된다. 이를 markov process라 한다. 즉 다음 상태의 상황은 현재 상태의 정보에만 의존한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 용을 피해서 보석에 도달해야하는 예시, 각 문직임미다 -0.08의 부정적 보상을 받기 때문에 가능한 최소의 움직임으로 목표에 도달해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 바둑과 같이 실전 문제에서는 상태가 매우 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 위 3가지를 살펴보면..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 정책은 상태가 들어오면 행동을 알려주는 함수이다. 결정적으로 행동을 알려줄수도 있고 행동에 대한 보상의 확률 분포를 알려주는 것도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 미래에 내가 받을 보상의 기댓값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 내 상태가 다음에 어떻게 변할지를 예측해주는 것, 즉 다음 상태 변화를 확률 분포로 알려주고 보상을 알려주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 3가지 개념을 어떻게 사용하느냐에 따라 \n",
    "\n",
    "> - value based : 가치함수를 사용\n",
    "> - policy based : 가치함수를 통하지 않고 바로 정책을 학습\n",
    "> - 두가지를 섞는 것이 AC\n",
    "> - 모델을 만들 샘플링해서 정책을 학습하는 것이 model based, 모델을 만들지 않고 바로 정책이나 가치함수를 학습하는 것이 model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - RL : 환경과의 상호작용으로 정보(상태/보상)를 얻고 이를 이용해 정책을 개선하는 것이 RL\n",
    "> - Planning : 환경에 대한 모델이 주어져 있거나 학습된 모델이 있을 때, 에이전트가 이 모델과 상호작용을 통해 정보를 얻고 이를 이용해 정책을 개선하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경이 어떻게 생겼는지 모르는 상태에서 계속되는 보상 관측을 통해 학습\n",
    "- 환경이 어떻게 변하는지 아는 상황에서 \n",
    "\n",
    "\n",
    "planning vs RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 모험을 해보느냐 아니면 현재 알려진 가장 좋은 행동을 하느냐, 탐험을 해보지 않으면 숨겨진 좋은 길을 갈수 없어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Prediction : 정책이 주어져 있을 때 그 정책에 따라 행동했을 때 미래 보상을 예측\n",
    "- Control : 정책을 개선하는 것\n",
    "\n",
    "> 주어진 정책으로 Prediction한 후 이 결과를 바탕으로 Control 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-b/week01-b-38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Event : 시행, 주사위를 한번 던진 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 이번트의 집합을 Sample Space라 함, 주사뤼를 한번 던지는 이벤트의 sample sapces는 $\\{1, 2, 3, 4, 5, 6\\}$\n",
    "- Sample space의 부분집합의 집합을 Sigma-algebra라 함. 예를들어 $\\{ ... \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 집합연산들... 교/하/여/차 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigma-algebra는 몇가지 특성이 있어야 함.\n",
    "\n",
    "Sample space가 원소이어야 함, 어떤 A가 원소이면 이걸 뺀것도 원소이어야 하고, 어떤 것들이 원소라면 이것들의 합집합도 원소이어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률이란 이벤트들의 집합을 하나 넣어주면 그 집합에 해당하는 어떤 실수를 주는 녀석, 즉 집합에 정의된 함수이다. 즉 Sigma-algebra의 원소에 대응하는 실수를 주는 녀석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률은 당연히 0과 1 사이의 값이어야 하고, 일어날 수 있는 모든 이벤트 확률의 합은 1이고, 공집합의 확률은 0이고, 겹치지 않는 이벤트 집합에 대한 합집합의 확률은 각 원소 확률의 합과 같이야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\{H, T\\}$는 H나 T가 나올 확률\n",
    "\n",
    "이 성질만 만족하면 probability measure라 할 수 있다. 즉 P(H)=1/3, P(T)=2/3이라 해도 이 성질을 만족시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률변수, 이벤트를 어떤 실수로 메핑하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들어 코인의 앞면이나 뒷면이 나오는 것이 이벤트라면, 앞면이 나오는 이벤트는 1이라하고, 뒷면이 나오는 것을 0이라 할 수 있다. 즉 각 이벤트를 실수로 메핑하는 것을 확률변수라 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(X=0)이라는 것은 P({T})와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y에 대한 사전정보가 주어져 있을 때, X를 관측하고, 이 근거가 주어졌을 때 Y의 분포 P(Y|X)를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-41.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-42.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-44.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-c/week01-c-45.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태 $S$가 변하는데, 다음 상태는 현재의 상태에만 의존하는 상태 전이 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-41.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-42.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-44.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-45.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-46.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-47.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-48.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-49.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-50.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-51.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-52.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-53.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-54.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy가 정상성을 갖는다고??? stationary, 앞에서 뭐가 non-stationary라고 했던 것 같은데 차이가 뭔가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-55.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-56.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-57.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an MDP and a Policy, MDP becomes MRP???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-58.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-59.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-60.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-61.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-62.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-63.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-64.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-65.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-66.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-67.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State value function과 state action value function이 같다는 말이야?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-68.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-69.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-70.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ 어디갔나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-71.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week01-d/week01-d-72.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
