{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Markov Process는 '상태' $\\mathcal{S}$의 연속으로서 각 상태 전이는 '상태 전이 확률'로 결정된다. 결국 Markov Process는  $\\left< \\mathcal{S}, \\mathcal{P}\\right>$ 튜플로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 에피소드는 실현된 연속된 상태로서 Markov Chain의 시작 상태와 끝 상태를 포함한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Markov Reward Process는 보상 $\\mathcal{R}$과 감가율 $\\gamma$을 포함하는 Markov chain이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi}(s)$ as $q_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{\\pi}(s)$ as $v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 행동 'Pub'에 대한 Optimal action-value function, $q_*(s, a)$은 아래와 같이 구할 수 있다. 강의 슬라이드에는 8.4로 나와 있으나 이는 오타이다.($\\gamma=1$)\n",
    "\n",
    "$$\\begin{align}\n",
    "q_*(s, a) \n",
    "&= \\mathcal{R}_s^a + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}^a_{ss^{\\prime}} v_*(s^{\\prime}) \\\\\n",
    "&= \\mathcal{R}_s^a + \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}^a_{ss^{\\prime}} v_*(s^{\\prime}) \\\\\n",
    "&= 1 + \\left( 0.2*6 + 0.4*8 + 0.4*10\\right) \\\\\n",
    "&= 9.4\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 행동 'Facebook'에 대한 Optimal action-value function, $q_*(s, a)$은 아래와 같이 구할 수 있다. 이 경우 상태 전이에 확률적 요소가 없이 어떤 행동을 하면 반드시 하나의 상태로 바뀐다. ($\\gamma=1$, $\\mathcal{P}_{ss^{\\prime}}^{a}=\\mathbb{1}$)\n",
    "\n",
    "$$\\begin{align}\n",
    "q_*(s, a) \n",
    "&= \\mathcal{R}_s^a + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}^a_{ss^{\\prime}} v_*(s^{\\prime}) \\\\\n",
    "&= \\mathcal{R}_s^a + \\sum_{s^{\\prime} \\in \\mathcal{S}} v_*(s^{\\prime}) \\\\\n",
    "&= -1 + \\left( 6 \\right) \\\\\n",
    "&= 5\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $v_*(s) = \\max_a q_* (s, a)$이므로 $v_*(s)$는 각 상태에서 출발하는 화살표에 적힌 $q_*$중 가장 큰 값과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 $V^{\\pi}$에 대한 Bellman Expectation Equation은 $v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) q_{\\pi}(s, a)$와 같이 아직 최적 정책을 알기 전이므로 action-value function을 정책으로 평균 내는 것으로 정의했었다.\n",
    "\n",
    "> 반면 $v_{*}(s)$에 대한 Bellman Optimality Equation은 $v_*(s) = \\max_a q_* (s, a)$와 같이 현재 상태 $s$에서 가능한 여러 행동($a \\in \\mathcal{A}$)들 중 최적 정책 $\\max_a$를 선택했을 때의 action-value function을 그대로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 최적 정책이 나오면 행동과 그 행동에 따른 현재 상태에 대한 보상($\\mathcal{R}^a_s$)은 결정되어 있다. 하지만 행동에 따른 다음 상태는 '상태 전이 확률'에 의해 확률적으로 바뀌는 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $v_*(s)$와 $v_*(s^{\\prime})$의 관계\n",
    "\n",
    "$$\\begin{align}\n",
    "v_*(s) \n",
    "&= \\max_a q_* (s, a) \\\\\n",
    "&= \\max_a \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{ss^{\\prime}}^{a}v_*(s^{\\prime}) \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $q_*(s, a)$와 $q_*(s^{\\prime}, a^{\\prime})$의 관계\n",
    "\n",
    "$$\\begin{align}\n",
    "q_*(s, a) \n",
    "&= \\mathcal{R}_s^a + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}^a_{ss^{\\prime}} v_*(s^{\\prime}) \\\\\n",
    "&= \\mathcal{R}_s^a + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}^a_{ss^{\\prime}} \\left( \\max_{a^{\\prime}} q_*(s^{\\prime}, a^{\\prime}) \\right) \\\\ \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 상태 'Class_1'에 대한 Optimal state value function, $v_*(s)$은 아래와 같이 구할 수 있다.($\\gamma=1$, $\\mathcal{P}_{ss^{\\prime}}^{a}=\\mathbb{1}$)\n",
    "\n",
    "$$\\begin{align}\n",
    "v_*(s) \n",
    "&= \\max_a \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{ss^{\\prime}}^{a}v_*(s^{\\prime}) \\right) \\\\\n",
    "&= \\max_a \\left( \\mathcal{R}_s^a + \\sum_{s^{\\prime} \\in \\mathcal{S}} v_*(s^{\\prime}) \\right) \\\\\n",
    "&= \\max \\{ -1+6, -2+8\\} \\\\\n",
    "&= 6\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-48.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/02_MDP-57.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
