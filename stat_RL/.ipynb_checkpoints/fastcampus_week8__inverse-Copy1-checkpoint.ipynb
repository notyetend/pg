{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Reward를 어떻게 디자인 할 것인가? 앞서 RL에서는 reward를 사람이 정하는 것이라 했었다. 그런데 사람이 정하는 것이 과연 최선인가? 사실 reward 디자인에 따라 전혀 다른 정책이 학습된다.          \n",
    "- IRL(Inverse Reinforcement Learning)에서는 reward를 데이터로부터 찾아내고 싶다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. MDP: optimal policy/value를 찾기 위해 transition model $P(s' \\mid s, a)$를 알아야 했다.\n",
    "2. RL: 현실에서는 transition model를 모르기 때문에 이걸 sampling으로 대체 했었다. 그런데 예를들어 자동자 운전을 학습시킨다면 위험한 주행도 트라이 해 보고 여러가지 해보고 학습시키기는 부적절하지 않은가?\n",
    "3. IRL: transition model도 모르고, reward를 그냥 세팅하는 것이 아니라 데이터로부터 최적의 reward setting을 찾아낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 보석은 1점, 불구덩이는 -1점, 나머지 상태에서의 R은 -0.01인 상황\n",
    "- expectation of sum of reward를 최대화 하고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Goal state에 도달하면 +1의 reward, hazard에 빠지면 -1의 reward(penalty)를 받게 된다. 또한 다른 상태(otherwise)에 도달하면 -0.01의 reward(penalty)를 받게 된다.\n",
    "- 이런 reward setting은 goal state에 도달해야 하고 ,hazard에는 가지 말아야 하고, 다른 상태에서 많이 배회 하지는 말라는 것이다. 그런데 이런 reward setting 밖에 없을까? reward setting에 따라서 학습되는 정책이 달라지지는 않을까?\n",
    "\n",
    "\n",
    "> - 또한 이런 세팅에서는 reward(function)을 사람이 정하고, optimal policy를 찾고 있다. 이때 optimal policy를 찾는 방법은 transition prob이 주어져 있다면 MDP를 풀 것이고, 그렇지 않다면 RL(sampling을 이용한..)을 이용해 optimal value/policy를 찾을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - otherwise의 reward를 바꿔가면서 학습시켜봤더니, optimal policy가 바뀐다. 위&좌의 경로는 모두 같으나 나머지 경로에서의 정책은 reward setting에 따라 달라진다. otherwise의 penalty가 커질수록 world 탐색을 최대할 줄이는 정책이 학습된다.극단적으로 R(otherwise)=-2.0이면 -1로도 그냥 가려하는 정책이 학습된다.\n",
    "\n",
    "> - 즉 reward setting 또한 학습되는 정책에 많은 영향을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 예를들어 주행을 학습시킨다면, 주행을 위한 다양한 feature가 존재할텐데, 차선, 속도, 앞차와의 거리, 사각지대접근등을 생각해볼 수 있다. 이때 어떤 feature를 더 중요하게 볼 것인지를 weight으로 컨트롤 하여 가중합으로 전체 reward를 설계하게 된다. 만약 $w_1$이 매우 크다면 차선만을 아주 잘 지키는 정책이 학습될 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - transition prob이 주어져 있다면 reward를 우선 세팅하고 MDP를 풀어 정책을 확인하고, reward setting을 수정하고, 다시 MDP를 풀고 정책을 확인하여 reward setting을 바꾸는 과정을 반복할 수 있을 것이다. \n",
    "- 하지만 현실은 그렇게 단순하지 않다. 우선 transition prob이 주어져 있지 않기 때문에 RL로 문제를 푼다면, 예를들어 자율 주행을 학습시킨다면 reward를 임의로 정하고, 학습을 위해 (위험할 수도 있는) 여러 형태의 주행을 테스트 하고 정책을 학습시킨 후 이 정책을 평가 하기까지 대단히 많은 비용이 들어간다. 따라서 이런 과정으로 reward setting을 변경하는 것은 거의 불가능하다. \n",
    "\n",
    "> - 따라서 보다 똑또갛게 reward를 세팅하는 방법이 필요하다. $\\rightarrow$ IRL로 접근해본다.(expert의 선택을 이용해 reward func을 구한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert는 optimal policy를 아는 사람을 의미한다. (그 대상을 정말 잘하는 사람)\n",
    "- expert가 각 상태에서 어떤 행동을 선택했는지가 expert's episode이고, 이런 episode를 모아 놓은 것이 expert's demonstrations(set of episode)이다.\n",
    "\n",
    "> - expert's demo를 주고 expert의 policy를 학습하는 것 즉 expert와 유사한 행동을 generate하는걸 Imitation Learning 혹은 Learning from demonstration이라 하고, 이 범주 안에 reward와 policy를 같이 학습하는 걸 Inverse Reinforcement Learning(IRL)이라 한다. (IRL이 Imitation Learning의 범주에 들어가는 이유는 IRL이 demonstration을 사용하기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 어떤 상태에서 어떤 행동을 할 것인가, 상태를 입력으로 하고 행동을 출력으로 하는 식의 지도학습 문제를 풀 수 있다. action이 discrete하다면 classification, conutious하다면 regression 문제가 된다. 이런 방식을 크게 **BC(Behavior Cloning)**이라 한다. robotics에서는 많이 쓴다. \n",
    "\n",
    "\n",
    "> - IRL은 demo로부터 reward func을 구하고, 이를 이용해 MDP나 RL을 풀어서 optimal policy를 구한다.\n",
    " - IRL의 기본 전제는 expert의 policy가 optimal policy가 되도록하는 reward를 찾는 것이다. 이게 무슨말이냐고? 다음에 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - BC: Find policy dist maximizing likelihood\n",
    "- IRL: Find reward function? > yes\n",
    " - Q-function이 아니라 reward function을 구한단 말이야? > yes\n",
    " - reward function을 알면 Q-function을 바로 알수있나? > 아니 reward func를 이용해 MDP나 RL을 푼다.\n",
    " - 우선 reward function을 어떻게 구하지? > 이후에 나옴\n",
    " - BC와 독립적인 것인가? > 전혀 다르다.\n",
    " - transition prob는 필요 없나? > reward를 구한후 transition prob이 있다면 MDP를, 없다면 RL을 풀 것이다.\n",
    " - reward function으로 policy dist를 어떻게 구하지? > MDP나 RL을 푼다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 표기법: $\\mathbb{E}_{\\pi_E}[r(s, a)] = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t r_t(s, a) \\mid \\pi]$\n",
    "\n",
    "\n",
    "> - (expert의 policy를 이용하는)expert's performance와 (나의 여러 policy들 중 가장 좋은 것을 이용하는)optimal policy's performance가 같아지도록 reward function을 세팅하고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 그런데 이 문제는 Ill posed problem이다. 즉 솔루션이 유일하지 않다. 예를들어 $r(s, a)=0$이면 equation이 항상 성립한다. 따라서 어떤 다른 조건이 추가로 필요하다. \n",
    "- 또한 scale problem도 있다. equation 양변에 상수를 곱해줘도 똑같이 성립현다. 이 문제를 해결하기 위해 regularization을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - scale?\n",
    "- regularization? >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL 분야는 크게 두가지 방향으로 발전해왔다. $\\mathbb{E}_{\\pi_E}[r(s, a)] = \\max_\\pi \\mathbb{E}_\\pi [r(s, a)]$를 푸는 건 같은데 다른 두개의 접근 방법이 있었다.\n",
    " - Large margin based: 처음 Kalman이 Inverse Optimal Control 혹은 Inverse Optimality Design이란 이름으로 이 아이디어를 제시 했다. 그 후 2000년에 Andrew Ng이 Kalman의 아이디어를 기반으로 expert의 demo를 이용해 reward를 찾는 접근 방법을 만들어 IRL이란 이름으로 소개한다. 그 후 Pieter Abbeel등 연구가 계속 된다. \n",
    " history... blabla.... ...\n",
    " - Maximum entropy: Brian Ziebard 혼자 다 함. \n",
    " \n",
    " \n",
    "> - 그런데 2년전 앞서 두 방향이 결국 많은 공통점을 갖고 있으며 이 두 접근을 더한 GAIL 방법을 제시한다. 여기에는 GAN의 아이디어가 많이 사용된다.(네가 알고 있는 그 GAN이다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 앞서 PG에서는 $1/1-\\gamma$가 $\\mathbb{E}$앞에 붙어 있었는데 이걸 뗀걸 visitation이라 부른다. $\\mathbb{I}_{S_t=s}$을 보면 알 수 있듯이 time-step $t$에 상태 $s$를 방문했으면 (discounted)1을 더해준다. 반면 방문하지 않았다면 더하는 값이 없다. 즉 특정 상태 $s$의 (discounted) 방문 빈도라 할 수 있다. (이때 $S_t$가 확률변수이므로 첫줄과 같이 전개된다.)\n",
    "- discount가 없으면 무한대가 되어 버린다. 물론 특성 time-step이후에는 그 상태를 다시 방문하지 않는다면 discount factor가 없어도 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - PG에서는 특정 상태를 방문할 빈도(state visitation) $\\rho_{\\pi_\\theta}(s)$만 생각했었는데, 이제 특성 상태에서 특정 행동을 하는 빈도(state action visitation) $\\rho_{\\pi_\\theta}(s, a)$에 대해 생각해볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $P_{\\pi_\\theta}(S_t=s, A_t=a)$: 시간 $t$에 상태 $s$에 있고 행동 $a$를 할 확률\n",
    "> - $P_{\\pi_\\theta}(S_t=s, A_t=a) = P_{\\pi_\\theta}(A_t=a \\mid S_t = s) P_{\\pi_\\theta}(S_t=s)$(bayes rule)\n",
    "> - $P_{\\pi_\\theta}(A_t=a \\mid S_t = s)$는 policy distribution $\\pi_\\theta(a \\mid s)$이다.\n",
    "- policy distribution은 시간에 따라 변하지 않으므로(업데이트 주기 내에서 겠지?..) $\\sum$ 밖으로 나가고 아래와 같이 정리된다.\n",
    "$$\\begin{align}\n",
    "\\rho_{\\pi_\\theta}(s, a) &= \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s, A_t=a) \\\\\n",
    "&= \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(A_t=a \\mid S_t = s) P_{\\pi_\\theta}(S_t=s) \\\\\n",
    "&= \\sum_t^\\infty \\gamma^t \\pi_\\theta(a \\mid s) P_{\\pi_\\theta}(S_t=s) \\\\\n",
    "&= \\pi_\\theta(a \\mid s) \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s) \\\\\n",
    "&= \\pi_\\theta(a \\mid s) \\rho_{\\pi_\\theta}(s)\n",
    "\\end{align}$$\n",
    "- 즉 특정 (state, action)에 대한 visitation은 '그 state에 대한 visitation'과 '그 state에서 특정 action을 확률(정책)'을 곱한 것과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\sum_t^{\\infty}\\gamma^t P_{\\pi_\\theta}(S_t = s) = \\rho_{\\pi_\\theta}(s)$ : discounted accumulated state  prorability at time $t$.\n",
    "- $\\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s, A_t=a) = \\rho_{\\pi_\\theta}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - policy를 정하면 state action visitation이 unique하게 정해진지고, 반대로 state action visitation을 정해놓으면 그걸 만들어내는 policy는 unique하다. (one to one 관계), (모든 state action space의 모든 (s, a) 쌍에 대한 visitation을 주어지는 것을 말한다.)\n",
    "\n",
    "\n",
    "> - 이 관계를 이용하면 '정책'을 찾는 문제를 state action visitation을 찾는 문제로 바꿀 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 아래 **RL의 objective function**의 전개중 $\\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t  P(s, a) r(s, a, s') P(s' \\mid s, a)$에서 첫 $\\sum$에서 $s, a, s'$를 고정했고, 두번째 $\\sum$은 모든 time-step에 대해 더한다. 따라서 안쪽 $\\sum$의 항들에서 $r(s, a, s')P(s' \\mid s, a)$를 묶어낼 수 있다.\n",
    "\n",
    "> $$\\begin{align}\n",
    "\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t R_t \\mid \\pi] &= \\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t r(s, a, s') P(s, a, s') \\\\\n",
    "&= \\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t r(s, a, s') P(s' \\mid s, a) P(s, a) \\\\\n",
    "&= \\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t  P(s, a) r(s, a, s') P(s' \\mid s, a) \\\\\n",
    "&= \\sum_{s, a, s'} \\rho_{\\pi_\\theta}(s, a) r(s, a, s') P(s' \\mid s, a) \\\\\n",
    "&= \\sum_{s, a, s'} r(s, a, s') P(s' \\mid s, a) \\rho_{\\pi_\\theta}(s, a)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\sum_t^{\\infty}\\gamma^t P_{\\pi_\\theta}(S_t = s) = \\rho_{\\pi_\\theta}(s)$ : discounted accumulated state  prorability at time $t$.\n",
    "- $\\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s, A_t=a) = \\rho_{\\pi_\\theta}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 결국 objective function이 (시간에 무관하게) 'state action visitation' $\\rho_{\\pi_\\theta}(s, a)$에 선형적이다.\n",
    " - 반면 정책이 바뀌는건 시간에 대해 선형적이지 않을 수 있다.\n",
    "> - IRL의 objective function은 두 벡터의 내적으로 표현됨.\n",
    "> - $\\pi$가 달라지면 $\\rho_\\pi$가 달라짐.\n",
    "-  $\\sum_{s, a, s'}r(s, a) \\rho_\\pi(s, a)$를 벡터 $r$과 벡터 $\\rho_\\pi$의 내적으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL objective function 식에서 $\\sum_{s, a, s'}$은 $\\sum_{s, a}$가 되어야 할 듯. 이건 sutton 책 38쪽을 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL에서는 왜 $P(s' \\mid s, a)$가 없지? s'에 대해 더하면 1이 된다?\n",
    "- $\\sum_{s, a, s'}r(s, a) \\rho_\\pi(s, a)$를 벡터 $r$과 벡터 $\\rho_\\pi$의 내적으로 표현했네?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 결국 objective function을 maximize하는건 $\\langle r, \\rho_\\pi \\rangle$를 최대화 하는 $\\pi$를 찾는 것과 같다. 두 벡터의 내적을 최대화 하는 policy 혹은 reward를 찾아야 함. \n",
    " - 즉 $r$이 정해져 있을 때, 여러 정책들 각각에 대응되는 $\\rho_\\pi$들 중 $r$에 투영선의 길이가 가장 긴 것이 가장 좋은 정책이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert의 policy를 $\\pi_E$라 할 때 이에 대응되는 state action visitation은 $\\rho_{\\pi_E}$이다.\n",
    " - $\\langle r, \\rho_{\\pi_E} \\rangle = \\max_\\pi \\langle r, \\rho_\\pi \\rangle$을 만족하는 $r$을 찾으려면 우선 랜덤하게 $r$을 정하고 $\\rho_{\\pi_E}$와 여러 $\\rho_{\\pi}$들을 $r$에 투영한다. 이때 $\\rho_{\\pi}$들의 투영중 가장 긴것보다 $\\rho_{\\pi_E}$의 투영이 더 길어야 한다는 조건을 만족할 때까지 $r$을 바꿔보는 것이다.\n",
    " - $\\rho_{\\pi}$ set을 위 그림의 녹색 원으로 표시한다면 $r$을 바꿔가면서 여기에 포함된 모든 $\\rho_{\\pi_E}$을 $r$에 투영해서 위 조건을 만족하는 $r$을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - BC vs IRL: IRL의 장점은 reward를 알아낼 수 있다는 것이다. reward를 다른 agent에 transfer시키면 다른 agent들도 이를 기반으로 학습을 할 수 있다. 또한 예를들어 내 차선에 전방 장애물이 있을 때 좌나 우로 피하야하는데, BC를 하면 좌/우를 smoothing하게 되서 가운데를(직진) 학습하게 된다. 반면 IRL에서는 직진은 reward가 작다고 판단하여 좌나 우에 높은 reward를 부여하여 좌/우로 피한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1; 54:00 @@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Pieter Abbeel's apprenticeship learning paper (https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)\n",
    "\n",
    "> - reward 함수를 linear model로 modeling, 나중에는 NN으로 발전.\n",
    "\n",
    "> - regularization을 사용하는데, weight abs sum이 1보다 작아져야한다는 조건을 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $S \\rightarrow [0, 1]^k$는 $S$가 0과 1 사이의 값을 갖는 길이가 $k$인 벡터라는 거지? >> 그냥 feature의 차원을 의미함\n",
    "- star $*$는 무슨 의미이지? transpose? >> ground truth\n",
    "- 왜 $R^*(s)$의 차원이 $k$이지? 그냥 scalar 값 아닌가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> w 뒷부분은 expected dis accum feature vec이라 하고 sampling으로 추정치를 feature expection이라 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(S_t)] \\sum_s I[S_t=s])$ S_t=s일때만 값이 있으므로 \\phi(s_t)가 \\phi(s)가 됨. 이걸 E 밖으로 뺄수 있다. \\E는 두번째 sum앞으로 간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $D$: initial distribution of state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 결국 performance가 parameter에 대해 linear하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $s~D$는 뭘 의미하지? $D$가 분포라면 어떤 분포인거지? state dist?\n",
    "- $\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\mid \\pi] = \\sum_s \\rho_{\\pi}(s) \\phi(s)$ 이 부분 잘 모르겠다.\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\mid \\pi] &= \\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) P_\\pi(S_t=s)\\\\\n",
    "&=\\sum_s \\rho_{\\pi}(s) \\phi(s) \\\\\n",
    "&\\because \\sum_t^{\\infty}\\gamma^t P_{\\pi_\\theta}(S_t = s) = \\rho_{\\pi_\\theta}(s)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - sampling으로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오타) transpose아니고 내적임.\n",
    "\n",
    "> - expert's performance는 키우고, 내 정책중 최고 정책(my optimal policy)의 performance와의 차이를 가장 크게 하고 싶다. 즉 앞에건 키우고 뒤에건 작게 할 것이다. 즉 w(reward vector direction)를 잘 조절해서 expert's perfoance를 가장 크게 표현하는 reward function을 찾겠다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 문제를 약간 다른 관점에서 보면 maximum margin 문제로 바꿀 수 있다.\n",
    "\n",
    "> - $$t = w^t \\mu(\\pi_E) - \\max_j w^t \\mu(\\pi_j) \\\\\n",
    "\\max_j w^t \\mu(\\pi_j) + t = w^t \\mu(\\pi_E)$$  SVM으로 이런 문제를 풀 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy를 하나식 try해 보고, 각 margin을 최대화 하는 w를 찾음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - x축은 demo의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ill pose problem 즉 솔루션이 하나가 아닌 상황.. 이런 ambiguity가 있는데.. 'Principle of maximum entropy'를 이용.. \n",
    "- expert와 비슷하게 행동하되 나머지 자유도를 이용해서는 entropy가 최대가 되도록 행동하자. 가위바위보 예를 보면 랜덤하게만 내면 33%의 승률일 기대할 수 있는데, data가 있는 곳에서는 expert를 따라가고 나머지 부분에서는 entropy를 최대화 하겠다?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$h(p_1) = \\sum -p_i \\log p_i ~~ \\text{: entropy}\\\\\n",
    "H(\\pi) = \\mathbb{E}[\\sum_{t=0}^\\infty r^t h(\\pi(\\cdot \\mid s_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - time-step마다 RV가 하나식 있는 것이므로 $\\sum$을 하게 됨.\n",
    "- $\\pi$에 대한 entropy를 시간에 대해 더한 것의 기댓값을 $H(\\pi)$라 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> entropy의 특성은 분포가 한쪽으로 몰리면 entropy가 낮아지고, uniform해지면 entropy가 높아진다. max entropy한다는 것은 분포를 uniform하게 한다는 것임. 그냥 uniform하게 하는 건 아니고 constraint를 추가하는데, expert's feature exteaction과 내가 학습시키는 정책의 expectation이 같아지도록 하는 조건"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert의 feataure 중 앞차와의 거리가 얼마라는 게 있다면, 내 정책에서도 이 조건을 맞추고 나머지 선택에 대해서는 entropy를 높이는 정책을 선택한다. 즉 거리가 동일하면 여러 방향으로 갈 수 있도록 비교적 uniform한 정책(multi modal)을 선택하게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - \\sum - \\sum = 0으로 만들고 이걸 objective function식에 넣어준 것, 넣어줄 때 lagrangian multiplier를 곱해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 그런데 보니까 $\\theta^T \\mu_\\pi$가 앞서 다뤘던 weight *?와 같은 형태가 된다. 즉 이게 reward을 의미하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 빨간 박스를 잘 보면... policy에 대해 최대화 한다. 이 부분만 보면 $\\theta$가 weight vector이고, 이고 ?sms feature expectation이고, feature exp는 \\phi * \\rho이고, 이건 ?? 기댓값이다. 즉 MDP를 푸는 것과 같아진다.(expected sum of reward)\n",
    "- 이런 문제는 soft MDP라 함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 우리가 처음 배웠던 MDP는 entropy H 가 없었다.\n",
    "- entropy가 추가된 MDP를 soft mdp이고, 여기에 대응되는 bellman eq를 soft bellman eq라 한다.\n",
    "- 그리고 이때의 policy는 q 값의 softmax가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - soft MDP를 풀면 \\max_\\pi가 해결되고, theta에 대한 부분은 theta로 미분해서 gradient descent로 푼다. 전체 항이 theta에 대해 linear하다. \n",
    "- gradient를 구하고, 이걸로 뒤 term을 구하고, policy가 바꼈으므로 \\max_\\pi를 다시 풀고\n",
    "gradient를 다시 구하고 이걸로 뒤 term을 update, 다시 max_pi를 풀고 반복.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> >>>>>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 게일? 가일?\n",
    "- 앞서 두 방법이 결국 같은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - margin based IRL에서 두번째 식으로 넘어올때 부호가 바뀌면서 max가 min으로 바뀜. 첫 식 안쪽 max는 policy에 대한 것이고, 밖으로 나올 수 있음.\n",
    "\n",
    "> - 두 개념이 서로 대응됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MB, ME의 제약조건이 서로 다름.\n",
    "- Unified framework에서는 두 조건을 모두 포함시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy에 대한 max를 먼저 품. 그리고 theta에 대한 gradient를 구하고 이 과정을 반복\n",
    "- 그런데 매번 MDP를 풀어야 하므로 매번 RL을 푸는게 부담됨. (tracjectory를 한번 sampling 해야하고, policy를 구하고 theta를 업데이트 해야한다. )\n",
    "- traj를 뽑아서 theta도 policy도 같이 update 하고 싶다. 즉 policy를 아주 많이 update해야하고 sampl이 너무 많이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - discriminator는 classification문제를 풀고 있고, generator는 random vector로부터 진짜 같은 vector를 만드는 문제를 풀고 있음\n",
    "- generator의 목표는 discriminant score가 1이 되도록 G를 만듬. \n",
    "- discimi는 진짜면 1 가짜면 0을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - p_data는 진짜 데이터\n",
    "- 진짜 데이터에는 1을 주도록 학습\n",
    "- p_z는 generator, 1을 주도록 하겠다. 뒤 항을 최소화 하려면 1이 되어야 함.\n",
    "- D는 discriminator\n",
    "\n",
    "\n",
    "\n",
    "> - 진짜 데이터는 검은 선, 초록색은 genetator, 파란점선은 discrimi이다. real과 같은 영역에서는 높은 discr score가 나오고, 아닌 영역에서는 낮은 score를 주고 있다.\n",
    "- 파라미터를 업데잇 할 때마다 점차 generator의 출력이 real data와 같아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GAN과 IRL의 공통점. 구조.\n",
    "- Reward : Discriminator\n",
    "- Policy : Generator\n",
    "\n",
    "- matching안되는건 inside, outside가 다름. 이걸 바꾸고 싶은데 말이야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - geneator min, discr max\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - max, min 문제는 안장에서 saddle point가 하나만 있으면 min, max의 순서가 상관 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 좌측은 policy net\n",
    "- 우측은 discri\n",
    "\n",
    "> - score를 할당하고, 경로에 대해 점수를 할당해준다. 경로에 대한 점수가 있으므로 이걸로 policy를 학습시킨다. 이때는 trpo나 ppo를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 요즘에는 TRPO대신 PPO를 많이 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - BC와 비교함.\n",
    "- x축은 demo sample 의 갯수인데, BC는 sampl이 많아지면 성능이 좋아진다. 그런데 데이터가 적으면 잘 안된다. \n",
    "- 반면 게일은 데이터가 적어도 잘 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 실습 - maximum entropy irl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - soft MDP를 푸는 부분 : Softmax and Log-Sum-Exp\n",
    "- numerical stability를 위해 max를 빼줌(def logsumexp, def softmax)\n",
    "- def soft_value_iteration: soft MDP를 풀어줌. 왜 soft이냐면.. bellman backup operator가 조금 다름.(Soft Bellman backup operator)\n",
    " - max 대신 log-sum을 사용함.\n",
    " - MDP 풀어서 optinal policy를 구함(expert demo 역할을 할 녀석임)\n",
    " - MDP를 풀어 Q를 구하고 policy를 구할 때는 max가 아니라 softmax가 사용됨(soft MDP의 특성)\n",
    "- State visitation의 E[]를 실제 구할일은 없고, 현실에서는 sampling으로 구함.\n",
    " - 어떤 상태에 오는 방법은 그 상태에서 시작하거나 다른 상태에서 전이되서 오는 경우가 있다.\n",
    " $$d(s)+\\gamma\\sum_{s',a'}P(s|s',a')\\rho_{\\pi}(s',a')$$\n",
    "- maximum entropy 방법에서는 state visitation을 구해야 함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Maximum Entropy Inverse Reinforcement Learning\n",
    " - step2: \\max_pi \\theta^T \\mu(\\pi)를 계산한 것임.\n",
    " \n",
    "> - transition prob가 필요하다는 문제\n",
    "- continuous action을 풀고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 실습 - generative_adversarial_imitation_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- demo 데이터의 비율이 너무 높으면 accuracy가 너무 높게 나온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy, val_func은 PPO\n",
    "- rew_func은 discrimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three networks\n",
    "policy = GaussPolicy(obs_dim, act_dim, epochs=10, hdim=64, lr=3e-4, clip_range=0.2, seed=seed)\n",
    "val_func = Value(obs_dim, epochs=20, hdim=32, lr=1e-3, seed=seed)\n",
    "rew_func = Reward(obs_dim, act_dim, epochs=10, hdim=32, lr=1e-4, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
