{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{E}[Y|X]$는 X에 대한 RV\n",
    "\n",
    "3번째 식은 다시 X에 대한 기댓값을 취하고 상수가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인버스 RL은 전문가의 정책이 주어지고, 최적 정책을 찾는 것\n",
    "\n",
    "결국 리턴 기댓값을 최대로 하는 정책을 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transition prob에 의해 위로 가는 행동을 선택했을 때 정말 위로가는 확률은 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3가지 리워드는 뭐가 다르지?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy distribution : 특정 상태에서 할 수 있는 행동들에 대한 확률 분포\n",
    "- $\\pi(a|s)$만 알면 에이전트가 할 행동을 모두 알 수 있다.\n",
    "- MDP에서의 정책은 현재 상태에만 의존한다. 즉 이전의 상태와는 무관하다.\n",
    "- Q : time-step마다 정책 update을 하면 non-stationary이지 않나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 현재 시점의 반환값($G_t$)은 현재 시점에 받은 보상($R_{t+1}$)와 다음 시점의 반환값($G_{t+1}$)의 합으로 표현된다.\n",
    "$$G_t = R_{t+1} + \\gamma G_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 큰 보상은 빨리 받는 것이 좋다. 나중에 받으면 discount factor에 의해 많이 작아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### +++ Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$G_t = R_{t+1} + \\gamma G_{t+1}$$와 같이 현재 시점의 반환값($G_t$)과 다음 시점의 반환값 사이의 관계를 재귀적으로 표현했었다. 이와 마찬가지로 현재 시점의 상태-가치함수($V_{\\pi}(s)$)와 다음 시점의 상태-가치 함수($V_{\\pi}(s')$) 사이의 관계를 재귀적으로 표현할 수 있고 이를 벨만 방정식이라 한다.\n",
    "$$V_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s', r} \\big[ r + \\gamma v_{\\pi}(s') \\big] p(s', r \\mid s, a), ~~\\text{for all } s \\in \\mathcal{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$P(s', s, a) = \\frac{P(s', s, a)}{P(s, a)} \\frac{P(a, s)}{P(s)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 현재 시점의 상태-가치함수($V_{\\pi}(s)$)와 다음 시점의 상태-가치 함수($V_{\\pi}(s')$) 사이의 관계를 재귀적으로 표현한 것과 마찬가지로, 현재 시점의 행동-가치함수($Q_{\\pi}(s, a)$)와 다음 시점의 행동-가치 함수($Q_{\\pi}(s', a')$) 사이의 관계를 재귀적으로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $V_{\\pi}(s)$와 $Q_{\\pi}(s, a)$사이의 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 어떤 정책이 주어져 있을 때 반환값($G_t = \\sum_{t=0}^{\\inf} \\gamma^t R_t \\mid \\pi$)의 기댓값은, 어떤 확률로 시작 상태($s$)를 선택하고 그 상태에서의 가치를 모두 더하는 것과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $d(s)$는 처음 시작 상태에 대한 확률 분포, 즉 특정 상태에 떨어질 확률과 그 상태에서의 보상을 곱하고, 이를 모든 상태에 대해 더하면 우리가 현재 정책으로 받을 보상의 기댓값이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $V_{\\pi}(s)$에서 star 빠짐(오타)\n",
    "\n",
    "> argmax Q* 아래 a 빠짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-a/week02-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위 두 스텝을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 두 벡터가 있을 때 각 벡터에 이 연산을 수행한 전 후 거리를 비교하면, 연산을 수행한 후 거리가 최소한 $\\gamma$만큼 줄어들게 된다.\n",
    "\n",
    "> $T_{\\pi}$ is a max-norm $\\gamma$-contraction, i.e.,\n",
    "$$\\lVert T_{\\pi}U - T_{\\pi}V\\rVert_{\\infty} \\leq \\gamma \\lVert U - V \\rVert_{\\infty}$$\n",
    "\n",
    "> - $V_{k+1} = T_{\\pi}V_{k}$ : $V_{\\pi}$를 찾기 위한 업데이트\n",
    "\n",
    "> $V_{\\pi} = T_{\\pi}V_{\\pi}$ : 주어진 정책 $\\pi$에 대한 상태-가치 함수가 구해지면 $T_{\\pi}$를 통과시켜도 변화가 없다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $T_{\\pi}$ is a max-norm $\\gamma$-contraction, i.e.,\n",
    "$$\\lVert T_{\\pi}U - T_{\\pi}V\\rVert_{\\infty} \\leq \\gamma \\lVert U - V \\rVert_{\\infty}$$\n",
    "\n",
    "> Bellman Backup Operator의 이런 특성 때문에 Policy Evaluation은 항상 한 점으로 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\begin{align}\n",
    "& \\max_s \\bigg| \\sum_a \\sum_{s'} \\gamma \\max_{s'} \\big| U(s') - V(s') \\big| ~ P(s' \\mid s, a) \\pi(a \\mid s) \\bigg| \\\\\n",
    "= & \\gamma \\max_{s'} \\big| U(s') - V(s') \\big| \\sum_a \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a)  \\\\\n",
    "= & \\gamma \\max_{s'} \\big| U(s') - V(s') \\big| \\\\\n",
    "= & \\gamma ~ \\lVert U - V \\rVert_{\\infty}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "> - 참고\n",
    " - max norm - https://en.wikipedia.org/wiki/Lp_space\n",
    " - theorem 4 - https://inst.eecs.berkeley.edu/~cs294-40/fa08/scribes/lecture2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy Evaluation을 통해 현재의 정책 $\\pi_i$를 따랐을 때 가치함수 $V_{\\pi_i}(s)$를 구할 수 있었다. 이제 $V_{\\pi_i}(s)$를 이용해 현재의 정책 $\\pi_i$를 더 나은 정책 $\\pi_{i+1}$로 업데이트 한다. 이 업데이트는 각 상태마다 $Q_{\\pi_i}(s, a)$를 최대로 하는 행동을 선택하도록 하는 것인데, $V_{\\pi_i}(s) = \\arg\\max_{a'}Q_{\\pi_i}(s, a)$라면 정책 변화가 없을 것이고 $V_{\\pi_i}(s) < \\arg\\max_{a'}Q_{\\pi_i}(s, a)$라면 정책이 바뀌게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그런데 각 상태마다 가장 좋은 정책을 고르는 것이 과연 전체 정책을 개선하는 효과를 가져올것인가? 현재 상태에 더 나은 정책을 선택하는 것이 다른 상태에는 나쁜 영향을 주지는 않을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $a = \\arg\\max_{a'}Q_{\\pi_i}(s, a)$와 같은 방식으로 상태 $s$에서의 정책을 $\\pi_{i}(a \\mid s)$에서 $\\pi_{i+1}(a \\mid s)$로 업데이트하기 때문에 $\\sum_a Q_{\\pi_i}(s, a) \\pi_{i+1}(a \\mid s) = \\max_{a'}Q_{\\pi_i}(s, a')$가 성립한다. (다른 정책들을 바뀌지 않고 상태 $s$에서의 정책만 바뀜).  \n",
    "\n",
    "> 또한 $V_{\\pi_i}(s)$는 $Q_{\\pi_i}(s, a')$의 기댓값이므로, $V_{\\pi_i}(s)$는 $Q_{\\pi_i}(s, a')$의 최대값 보다는 작거나 같다. 이 때문에 위 부등식이 성립한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\because ~  R_{t+1}^{\\pi_i} \\leq R_{t+1}^{\\pi_{i+1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\because ~ R_{t+2}^{\\pi_i} \\leq R_{t+2}^{\\pi_{i+1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\because ~ R_{t+3}^{\\pi_i} \\leq R_{t+3}^{\\pi_{i+1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $V_{\\pi_i}(s) \\leq V_{\\pi_{i+1}}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 결국 Policy Evaluation으로 현재의 정책 $\\pi_i$를 평가하고, Policy Improvement로 $\\pi_i$를 $\\pi_{i+1}$로 개선하는데 이 과정을 반복하여 최적 가치함수와 최적 정책에 도달하는 것을 Policy Iteration이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Policy evaluation을 할 때 주어진 정책 $\\pi_i$에 대하여 최초의 가치함수 $V_0$가 $V_{\\pi}$에 수렴할 때까지 업데이트를 반복했고, $V_{\\pi}$를 이용해 정책을 업데이트 했다. 그런데 정책을 업데이트 하기 위해 꼭 $V_0$를 $V_{\\pi}$까지 업데이트 할 필요가 있을까? 정책을 업데이트할 때 어느 행동이 $Q_{\\pi_i}(s, a)$를 최대로 하는지만을 보기 때문에 꼭 $V_{\\pi}$에 수렴하는 가치함수일 필요는 없는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-b/week02-b-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1 step of Policy Evaluation + greedy policy update + 1 step of Policy Evaluation = Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Principle of Optimality\n",
    " - Any optimal policy can be subdivided into two components, 1) An optimal first action $\\mathcal{A}^*$, 2) followed by an optimal policy successor state $s'$\n",
    " - A policy $\\pi(a \\mid s)$ achieves the optimal value from state $s$, if and only if, for any state $s'$ rechable from $s$, $\\pi$ achieves the optimal value from state $s'$, $v_{\\pi} = v^{*}(s')$ \n",
    " - If we know the solution to subproblems $v^*(s')$, then solution $v^*(s)$ can be found by one-step lookahead\n",
    " $$\\begin{align}\n",
    " v^*(s) &\\doteq \\max_a q_(s, a) \\\\\n",
    " &=\\max_a p(s', r \\mid s, a) \\big[ r + \\gamma v_{\\pi}(s')\\big]\n",
    " \\end{align}$$\n",
    " - An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n",
    " - 상태 $s$에서도 도달 가능한 상태 $s'$가 있고 $s'$에서 최적인 정책 $\\pi'$가 있다면, $s$에서 시작하는 최적 정책은 $\\pi'$를 포함한다. \n",
    " - 예를들어 $s$에서 $s'$를 거쳐 $s''$로 가는 최적 정책 $\\pi^*$을 결정해야하고, 만약 $s'$에서 $s''$으로 가는 최적 정책 $\\pi'$을 알고 있다면, $\\pi^*$는 반드시 $\\pi'$를 포함한다.\n",
    " \n",
    "\n",
    "> 참고\n",
    " - http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl3.pdf\n",
    " - http://www.inf.ed.ac.uk/teaching/courses/rl/slides16/rl07.pdf\n",
    " - https://www.cs.cmu.edu/~katef/DeepRLControlCourse/lectures/lecture3_mdp_planning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $s'$ 이후의 최적 솔루션 $V^*(s')$를 알고 있다면, $s$에서의 최적 솔루션은 $V^*(s')$와 $r(s, a, s')$을 이용해 구할 수 있다.(one-step lookahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 회색 셀이 종료 상태 $s_T$일때, Value Iteration을 반복하는 과정을 살펴보면,\n",
    "> - $v_1$ : 항상 $s_T$에서 에피소드가 끝나므로 $v(s_T)=0$는 최적이다.\n",
    "- $v_2$ : $s_T$에 바로 인접한 셀의 가치는 $v(s') = \\max\\big\\{-1 + v(s_T), -1 -1, -1 -1 \\big\\} = -1 + v(s_T)$ 이므로 이 값은 최적이다.    \n",
    "\n",
    "\n",
    "> 이런 식으로 종료 상태로부터 최적 가치함수가 전파되어, 이를 반복하면 가치함수가 최적 가치함수 $v^*(s)$에 수렴하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Value Iteration은 최적 솔루션을 전파하는 방식으로 업데이트가 진행된다. 최적 솔루션이 모든 솔루션에 전파되면 결국 최적 가치함수 $v^*(s)$가 구해진다.\n",
    "\n",
    "> - Policy Iteration과 달리 별도로 정책을 저장하고 업데이트 할 필요가 없고, $v^*(s)$가 구해지면 그때 정책을 결정하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 Policy Evaluation을 아래와 같이 $V_k$에서 $V_{k+1}$로의 변환(Bellman Backup Operator)이라 생각할 수 있었다.\n",
    "$$T_{\\pi}X(s) = \\sum_a \\pi(a \\mid s) \\sum_{s'} \\big[ r(s, a, s') + \\gamma X(s') \\big] P(s' \\mid s, a)$$\n",
    "또한 이를 이용해 Policy Evaluation은 $V_{k+1} = T_{\\pi}V_k$로 Bellman Equation은 $V_{\\pi} = T_{\\pi}V_{\\pi}$로 표현할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마찬가지로 Value Iteration을 아래와 같이 $V_k$에서 $V_{k+1}$로의 변환(Bellman Optimality Backup Operator)이라 생각할 수 있다.\n",
    "$$TX(s) = \\max_a \\sum_{s'} \\big[ r(s, a, s') + \\gamma X(s') \\big] P(s' \\mid s, a)$$\n",
    "또한 이를 이용해 Value Iteration을 $V_{k+1} = TV_k$로 Bellman Optimality Equation은 $V^* = TV^*$로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $T$ is a max-norm $\\gamma$-contraction, i.e.,\n",
    "$$\\lVert TU - TV\\rVert_{\\infty} \\leq \\gamma \\lVert U - V \\rVert_{\\infty}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week02-c/week02-c-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Policy Iteration\n",
    " - (최적이 아닐 수 있는) 어떤 정책 $\\pi_1$과 주어지고 이 정책을 따랐을 때 실제 얻게될 가치함수 $v_{\\pi_1}(s)$를 찾기 위해 초기 가치함수 $v^1$로 시작하여 아래와 같은 가치함수 업데이트(Policy Evaluation)을 진행한다.\n",
    " $$v^1 \\rightarrow v^2 \\rightarrow v^3 \\rightarrow \\cdots \\rightarrow v_{\\pi_1}$$\n",
    " - 위 과정을 통해 $v_{\\pi_1}(s)$을 찾게 되면 이를 기준으로 greedy하게 정책을 업데이트 한다.\n",
    " $$\\pi_1 \\rightarrow \\pi_2$$\n",
    " - 다시 $v_{\\pi_2}(s)$를 찾기 위해 아래와 같은 가치함수 업데이트를 진행한다.\n",
    " $$v_{\\pi_1} \\rightarrow v^{10} \\rightarrow v^{20} \\rightarrow \\cdots \\rightarrow v_{\\pi_2}$$\n",
    " - 이런 과정(Policy Evaluation + Policy Improvement)을 반복하여 $v^*(s)$와 $\\pi^*$을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Value Iteration\n",
    " - 임의의 가치함수 $v_1(s)$가 주어졌을 때, 최적 가치함수 $v^*(s)$를 찾기 위한 업데이트를 진행한다.\n",
    " $$v_1 \\rightarrow v_2 \\rightarrow \\cdots \\rightarrow v^*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   Algorithm : 1 step of Policy Iteration with 1 step Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "&\\color{green}{\\text{1 step Policy Evaluation}}\\\\\n",
    "&\\Delta \\leftarrow 0\\\\\n",
    "&\\text{Loop for each }s \\in \\mathcal{S}: \\\\\n",
    "&\\qquad v \\leftarrow V(s) \\\\\n",
    "&\\qquad V(s) \\leftarrow \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma V(s')\\big] \\\\\n",
    "\\\\\n",
    "&\\color{green}{\\text{Policy Improvement}}\\\\\n",
    "&\\text{Loop for each }s \\in \\mathcal{S}: \\\\\n",
    "&\\qquad \\pi(s) \\leftarrow \\arg\\max_a \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma V(s') \\big]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Algorithm : 1 step of Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "&\\Delta \\leftarrow 0\\\\\n",
    "&\\text{Loop for each }s \\in \\mathcal{S}: \\\\\n",
    "&\\qquad v \\leftarrow V(s) \\\\\n",
    "&\\qquad V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\big[ r + \\gamma V(s')\\big] \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
