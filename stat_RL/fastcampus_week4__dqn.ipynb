{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위에 것들은 P가 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model-free로 만들기 위해 sampling을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - SARSA = on-line & model-free(with TD or backward TD($\\lambda$)) Q-Policy Iteration\n",
    "- Q-Learning = model-free & off-policy Q-Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지금까지 다뤘던 타뷸러 형태의 문제는 상태와 액션이 몇가지로 제한되어 있다. 하지만 실제 필드에서 다뤄야 하는 문제는 그렇게 단순하지 않다. 실제 문제에 강화학습을 적용하기 위해서는 지금까지 다뤘던 방법들을 scale-up할 수 있는 접근이 필요하다.\n",
    "\n",
    "> 예를들어 바둑에서는 $10^{170}$개 정도로 많은 상태를 다뤄야 한다. 또한 헬리콘터 컨트롤 문제에서는 심지어 연속적인 상태 공간을 다뤄야 한다. 앞서 다뤘던 방법에서는 각 상태를 테이블 형태로 나타내고 관측한 에피소드에 따라 각 에피소드의 가치 값을 바꿨었는데, 상태가 셀수 없이 많은 문제는 더 이상 이런 접근으로 다룰 수 없다.         \n",
    "다수의 상태를 쉽게 다루기 위해 각 상태(+행동)에 대응하는 가치 값을 출력하는 함수가 있다고 가정하고, 에피소드 정보를 이용해 이 함수를 근사하는 방법을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지금까지의 문제는 각 상태에 대한 가치($v(s)$)나 상태-행동에 대한 가치 $Q(s, a)$를 테이블 형태로 표현했었다. \n",
    "\n",
    "> 하지만 상태가 매우 많은 문제를 테이블 형태로 표현하려면 필요로 하는 메모리 양이 너무 많아지고, 각 상태들을 모두 학습시켜야 하므로 그 시간 또한 오래 걸리는 문제점이 있다.\n",
    "\n",
    "> 결국 알고 싶은 것은 실제 가치에 대응하는 가치값을 표현하는 어떤 함수 $v_{\\pi}(s)$인데, 이를 어떤 가중치 벡터 $\\mathbf{w}$를 모수로 하는 모수 함수(parametric function) $\\hat{v}(s, \\mathbf{w})$로 근사하는 것이다. 이 함수는 회귀 모형이나 신경망 혹은 의사결정 나무 등 다양한 지도학습 모형이 될 수 있다. 예를들어 이 함수가 신경망이라면 $\\mathbf{w}$는 그 신경망으로 학습된 가중치 벡터가 될 것이다. 이 가중치의 수는 보통 가능한 상태의 수 보다 훨씬 작은 수준이 될 것이다.\n",
    "\n",
    "> 이런 함수 근사를 상태-행동에 대응하는 방식으로 설계할수도 있다. 즉 $q_{\\pi}(s, a)$를 근사하는 함수 $\\hat{q}_{\\pi}(s, a, \\mathbf{w})$를 학습시키는 것이다.\n",
    "\n",
    "> 이렇게 함수 근사로 가치함수를 표현하게 되면 관측하지 않았던 상태(혹은 상태-행동)에 대응하는 가치 또한 근사적으로 알 수 있게 된다.\n",
    "\n",
    "> $\\hat{v}(s, \\mathbf{w})$가 $v_{\\pi}(s)$에 가까워지도록 $\\mathbf{w}$를 업데이트(학습)해야하는데, 지금까지 배웠던 MC나 TD와 같은 방법을 여기에 사용하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 가치함수를 $\\hat{v}(S, \\mathbf{w})$로 근사한다면, $S$자리에 들어갈 정보가 필요하다. 이 정보는 각 상태의 특성을 표현하는 값들이며 피쳐 벡터이다. 예를들어 로봇의 움직임을 컨트롤 한다면 로봇과 장애물과의 상대 위치를 표현하는 각도와 거리가 각각 $x_1(S)$과 $x_2(S)$라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 가치 함수를 근사하는 방식(구조)을 3가지로 나눠볼 수 있다.      \n",
    "첫번째 방식은 상태 $s$를 입력하면 그 상태에 해당하는 가치를 반환하는 함수 $\\hat{v}(s, \\mathbf{w})$로 근사하는 것이고(TRPO, PPO, CPO),       \n",
    "두번째 방식은 상태-행동 쌍 $(s, a)$에 대한 가치를 반환하는 함수 $\\hat{q}(s, \\mathbf{w})$로 근사하는 것이고(DDPG),     \n",
    "세번째 방식은 상태 $s$를 입력하면 그 상태에서 가능한 모든 행동에 해당하는 가치를 반환하는 함수 $\\hat{q}(s, a_1, \\mathbf{w}), \\cdots, \\hat{q}(s, a_m, \\mathbf{w})$로 근사하는 것이다.(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3번째, $m$개의 행동에 대한 $m$개의 output이 나오는 함수, 이 경우 행동이 discrete해야한다는 제약사항이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다양한 함수 근사 방법들중 이 강의에서는 선형 가중합 형태의 모형(회귀 모형 형태)이나 신경망과 같이 미분 가능한 함수 근사 방법들을 사용할 것이다.(이런 방법들은 gradient를 이용해 바로 모수를 업데이트 할 수 있는 장점이 있다.) \n",
    "\n",
    "> 사실 사용할 수 있는 지도학습 방법에는 몇가지 제약이 더 있다. 비정상(non-stationary)이고 각 샘플간에 iid 가정이 성립하지 않는(non-iid) 데이터를 이용해 학습이 가능한 방법들이어야 한다. 정상성을 이란 시간에 따라 확률분포가 변하지 않는 특성을 의미하는데, 강화학습에서는 일정한 time-step마다 정책을 업데이트 하고 그에 따라 다음번에 추출할 상태-행동의 분포가 달라지기 때문에 정상성을 갖지 않는다. 또한 지금의 상태-행동이 다음 시점의 상태-행동에 영향을 주기 때문에 iid(independent identically distribution)이지도 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 가치함수를 근사하는 가장 단순한 방법은 가치함수를 $\\mathbf{x}(S)^T \\mathbf{w}$와 같이 피쳐들의 가중합으로 표현하는 것이다. 이 경우 (MSE를 이용한) 목적 함수는 아래와 같은 형태가 될 것이다.\n",
    "$$J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\Big[ \\big( v_{\\pi}(S) - \\mathbf{x}(S)^T \\mathbf{w} \\big)^2 \\Big]$$\n",
    "위 목적함수는 $w$에 대한 2차식으로서 $J(\\mathbf{w})$의 극소점이 반드시 하나 존재하기 때문에 최적화가 매우 쉽다.\n",
    "\n",
    "> 만약 이를 SGD형태로 구현하는 경우 gradient가 $x(S)$이므로 그 계산 또한 매우 단순하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그런데 초반부터 다뤘던 테이블 룩업 방법은 함수 근사의 특별한 경우라 할 수 있다. 예를들어 A, B, C 3개의 상태가 존재하는 환경에서 10개의 에피소드를 관측했을 때까지 각 상태의 가치가 $v_{\\pi}(A)=3$, $v_{\\pi}(B)=1$, $v_{\\pi}(C)=5$라고 해보자. 이런 단순한 테이블 룩업을 선형 함수 근사로 표현할 수 있다.\n",
    "\n",
    "> 즉 상태에 대한 피쳐 벡터를 아래와 같이 나타낼 수 있다.\n",
    "$$\\begin{align}\n",
    "\\mathbf{x}(A)=(1, 0, 0)^T \\\\\n",
    "\\mathbf{x}(B)=(0, 1, 0)^T \\\\\n",
    "\\mathbf{x}(C)=(0, 0, 1)^T \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> 또한 이에 대응하는 가중치 벡터 $\\mathbf{w}$를 아래와 같이 나타낼 수 있다.\n",
    "$$\\mathbf{w} = (3, 1, 5)$$\n",
    "\n",
    "> 이때 각 상태의 가치 값을 아래와 같이 표현할 수 있다.\n",
    "\n",
    "> $$\\begin{align}\n",
    "\\hat{v}(A, \\mathbf{w}) &= \\mathbf{x}(A)^T \\mathbf{w} = (1, 0, 0) \\cdot (3, 1, 5)^T = 3 \\\\\n",
    "\\hat{v}(B, \\mathbf{w}) &= \\mathbf{x}(B)^T \\mathbf{w} = (0, 1, 0) \\cdot (3, 1, 5)^T = 1 \\\\\n",
    "\\hat{v}(C, \\mathbf{w}) &= \\mathbf{x}(C)^T \\mathbf{w} = (0, 0, 1) \\cdot (3, 1, 5)^T = 5\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서의 논의에서는 실제 가치함수 $v_{\\pi}(s)$가 주어지고, 이것을 근사의 타겟으로 하여 근사 함수 $\\hat{v}(S, \\mathbf{w})$와 이것과의 차이를 목적함수에 이용했었다. 그런데 실전에서는 실제 가치함수 $v_{\\pi}(s)$가 주어지지 않는다. 대신 경험(에피소드)을 통해 알게된 조금 더 실제 가치함수에 근접한 어떤 값 혹은 함수를 근사의 타겟으로 이용한다.\n",
    "\n",
    "> 예를들어 MC에서는 각 에피소드를 통해 계산된 반환값 $G_t$를 타겟으로 한다. 예를들어 에피소드를 끝까지 관찰한 후 알게되는 반환값 $G_t$과 $t$번째 time-step에서 근사한 가치값 $\\hat{v}(S_t, \\mathbf{w})$과의 차이($G_t - \\hat{v}(S_t, \\mathbf{w})$)를 이용한다.       \n",
    "마찬가지로 TD(0)에서는 1-step TD return ($R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})$)을 타겟으로 하며      \n",
    "TD($\\lambda$)에서는 ($\\lambda$+1)-step TD return ($G_t^{\\lambda}$)을 타겟으로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-a/week04-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Value Network: $\\hat{V}(s_t; w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Clipping -> standardize, max Q(s, a..)만 찾으면 되므로 각 행동에 따른 상대값만 알고 있으면 된다.\n",
    "\n",
    "\n",
    "> reward scaling이 매우 매우 중요!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> breaking correlation to make it iid with experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every C steps reset ... : copy weights to target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W: importance weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> priority를 기준으로 샘플을 정렬... & ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - value: 내 state에서 받을 수 있는 평균 reward\n",
    "- advantage: 평균기대값보다 좋은지 나쁜지를 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/m 다음에 sum이 빠짐.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week04-b/week04-b-36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 환경에서 현재 상태($s$)를 확인한다.\n",
    " - 카트의 수평선상 위치\n",
    " - 속도\n",
    " - 폴의 수직선으로부터 기운 각도\n",
    " - 각속도\n",
    "2. 모델을 이용해 현재 상태에서 할 행동($a$)을 결정하고 이행한다.\n",
    "3. 환경에서 보상($r$)과 다음 상태($s'$)를 확인한다.\n",
    "3. $s, a, r, s'$를 이용해 모델을 학습시킨다.\n",
    " - 학습은 mini-batch([$\\mathbf{X}$, $\\mathbf{Y}$] = [$s$, $r + \\gamma \\max_{a'}Q(s', a', \\theta^-)$])를 이용한다.\n",
    "4. (에피소드가 끝났으면) 타겟 모델 파라미터를 업데이트한다.\n",
    "5. 위 과정을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
