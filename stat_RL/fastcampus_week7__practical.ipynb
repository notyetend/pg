{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 과감한 도전을 해볼 것인가? 아니면 당장 보기에 좋은 선택을 할 것인가? trade-off가 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 예시: explor vs exploi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지금까지 배웠떤 알고리즘들은 exploration을 어떻게 하고 있는가? \n",
    "- off-policy 접근: 탐색을 위한 별도의 정책을 마련\n",
    "- on-policy 접근: 현재 정책을 따라 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오늘 다룰 exploration 방법은 위와 같은 것들이 있다.\n",
    "- NE: greedy + epsilon random, stochastic policy \n",
    "- OI: Q 초기화를 좀 달리한다??\n",
    "- OFU: .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> optimistic initialization?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지금까지 다뤘던 것들은 S, A, R이 연이어 등장하는 문제였다면, S는 없는(하나뿐인) 문제를 생각해본다.\n",
    "- arm 하나가 하나의 action, 여러 arm이 있다면 어떤 arm을 땡길 것인가?\n",
    "- $\\mathcal{R}^A(R)$: action이 주어졌을 때 reward의 확률분포, 이걸 모른다.\n",
    "- transition prob는 없음.\n",
    "- 누적 보상을 최대로 하는 정책을 찾아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MAB 문제에서는 새로운 형태의 $Q$함수를 정의. $Q(a) = \\mathbb{E}[r \\mid a]$\n",
    "- regret을 줄이는 정책을 찾으면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\begin{align}\n",
    "L_t &= \\mathbb{E}\\bigg[ \\sum_\\tau^t v^* - Q(a_\\tau) \\bigg] \\\\\n",
    "&= \\mathbb{E} \\bigg[ N_t(a_1)\\Delta_{a_1} + N_t(a_2)\\Delta_{a_2} + \\cdots \\bigg] \\\\\n",
    "&= \\sum_i \\mathbb{E}[N_t(a_1)] \\Delta_{a_1}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\Delta_a$는 constant이므로 위와 같이 정리된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 첫식 이후... $E[\\sum_i^t v* - Q(a_i) \\sum_a I(a_i = a)]$\n",
    "- $\\sum_{i=0}^t I(a_i=a)$ : 그 액션을 선택한 빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> total regret\n",
    "- action마다의 gap으로 표현(두번째 식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. random action\n",
    "2. non exploration: \n",
    "3. 잘못된 행동 선택이 점차 줄어든다면 sublinear하게 regret이 증가할 것이다. 이런 정책을 찾는 것이 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 잘못된 action을 선택하고 있다면 time-step이 지날때마다 이게 선형적으로 늘어날것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sublinear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> greedy하게만 행동을 선택하면 잘못된 행동만을 고르게될수도 있다. -> suboptimal\n",
    "- suboptimal을 탈출하려면 당장 optimal이 아닌것 같은 행동도 탐색해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> why linear total regret?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $|\\mathcal{A}|$: size of action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $l_t$: minimum regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> epsilon만큼 random action을 한다는건 항상 suboptimal을 선택할 가능성을 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오타? epsilon-greedy가 되어야?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> why linear total regret?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $d$: 0보다 큰 최소 regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $d$(regret)가 클때, 즉 Q-function을 제대로 학습하지 못했을 때에는 최소 $1=100\\%$ 탐색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> greedy in the limit\n",
    "- epsilon을 점차 줄이는 방법\n",
    "- time-step이 계속될수록 점차 random action을 선택할 확률이 줄어든다. 이 경우 sublinear하다는 것이 알려져 있다. 그런데 epsilon을 그냥 줄이는 것이 아니라, 원칙이 있다..\n",
    "- 이렇게 하면 log t를 따라서 regret이 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그런데 reward 분포를 모르므로 $\\Delta_a$에 대한 정보가 없다.\n",
    "- 일반적으로 사용하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> total regret의 lower bound가 알려져 있다. \n",
    "- $(\\log t) * \\sum$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 뭔말이냐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Q function의 초기값을 잘 줘서, reward가 갖을 수 있는 값 보다 더 큰 값으로 초기화\n",
    "- MC로 업데이트 하면... 너무 큰 것들은 값이 줄어들게 된다.\n",
    "- 점차 나은 행동을 선택하게 된다.\n",
    "- discrete action space인 경우 구현하기 쉽다. 하지만 neural net에서는 어려움?\n",
    "- argmax Q로 action을 선택하므로 모두 동일하게 키워놓으면, 탐색안해본 큰 것들을 선택하게 되어 탐색이 쉬워진다. \n",
    "\n",
    "둘다 linear total regret이야?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사실 이런 것들을 Q-learning에서 탐색 정책으로 사용할수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그냥 Q를 찾는 것이 아니라 Q의 분포를 추정한다.\n",
    "- 특정 action 분포의 variance가 크다면 이 action을 선택해서 variance를 줄여야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> UCB: Q의 variance를 표현함. 어떤 행동을 많이 해보지 않았다면 불확실성이 크다. 반면 많이 해 봤다면 불확실성(UCB)이 작아진다. 즉 여러 행동중 UCB가 가장 큰 행동을 골라 탐색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그렇다면 어떤식으로 Confidence bound를 정의해야하나? 진짜 expectation이 sample mean보다 $u$만큼 더 큰 쪽에 있을 확률은 $e^{-2tu^2}$보다 작다.\n",
    "- t는 특정 action을 몇번 골랐느냐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $p=e^{-2N_t(a) \\hat{U}_t(a)^2}$가 작을수록 $\\hat{U}_t(a)$는 커진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 각 action을 많이 관측할수록 UCB가 작아지도록 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$a*_t = \\arg\\max_a \\big( \\hat{Q}_t(a) + \\sqrt{\\frac{2\\log{t}}{N_t(a)}} \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 알고리즘들은 total regret이 $N_t(a)$에 선형적이었다. 반면 UCB 알고리즘은 $\\log{t}$에 비례한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> x축이 log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mu_a$와 $\\sigma_a^2$에 대한 사전정보(분포)를 사용\n",
    "- posterior , = prior, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 두번째 식. reward에 대한 likelihood\n",
    "- mu 분포의 variance 항을 UCB 형태로 사용(마지막식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (오타) argmax아니라 max임.\n",
    "- 사후분포에서 reward를 샘플링?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i번째 action을 취했을 때 reward의 분포\n",
    "\n",
    "- reward는 1혹은 0\n",
    "- 1이 나올 확률은 $\\mathcal{R}^i$\n",
    "- $\\mathcal{R}^i$의 prior는 beta 분포를 사용.\n",
    "- posterior는 다시 beta가 된다. 이를 다시 prior로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 환경은 reward, transition prob을 의미한다. 이걸 모르므로 sampling으로 학습이 진행되었다. model-based RL은 sampling을 이용해 model을 학습시키겠다는거다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(오타) 처음, 마지막 experience 아니고 environment, 가운데건 model?\n",
    "\n",
    "- model-free: policy or value function을 이용해 action을 뽑고 environment에 던저보고 반응(r, s)을 보고 다시 policy or value function을 update한다.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> what kinds of supervised learning?\n",
    " - next slide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모델학습이란 transition prob, reward func을 찾는 것.\n",
    "- 일반적으로 reward는 조건에 따라 사람이 정하는 것이므로 학습시키지 않음.\n",
    " - rewards learning: $s$, $a$에 따른 $r$ 추정 - regression\n",
    " - $s$, $a$가 주어졌을 때 $s'$로 전이될 확률 추정 - density estimation (transition prob은 density estimation을 하는건 드물고, (s, a) -> s'의 regression을 푸는 식으로 진행)\n",
    " \n",
    "> - density estimation? most basic form is a rescaled histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> transition prob은 (s, a)를 입력으로 하고 s'를 출력으로 하는 다양한 형태의 함수추정 방법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. table lookup : emphirical 하게 prob를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - (s, a)를 입력으로 하고 s'를 출력으로하는 함수 f(s, a)를 추정, mse를 최소로 하도록 파라미터 수정\n",
    "- f(s, a)를 이용해 ephisode를 뽑고 policy를 update\n",
    "- policy...??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 환경에서 $(s, a, s')$를 샘플\n",
    "2. 샘플을 이용해 모델(transition, $f(s, a)$)을 학습\n",
    "3. $f(s, a)$를 이용해 optimal policy를 학습(planning, sampling and policy/value update)\n",
    "4. $(s, a)$에 따른 결과 $s'$를 환경에서 관찰\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model predictive control?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 학습된 f(s, a)를 이용하는데, 랜덤한 행동을 선택해서 f(s, a)의 출력을 본다. 이런 과정을 반복하면 tractory가 쌓인다. 이런 tracjectory들의 reward를 각각 계산하고, 가장 reward가 큰 궤적을 선택, \n",
    "- 그런데 실제 이 궤적으로 환경에서 실험하면 잘 안되므로, 이 궤적의 초기 행동만 취해서 환경의 결과를 보고 이것으로 model update에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MPC???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> backprob을 이용한 방식??\n",
    "- tractory\n",
    "- 첫 상태에서 핵션을 하나 고르고, 리워드를 계산하고\n",
    "- 다음 상태를 f(s, a)로 구하고, \n",
    "- 정책을 통과시켜서 행동을 고르고, 리워드 계산하고\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> demonstration이란 (s, a, r) 쌍을 의미함\n",
    "- 이것들을 이용해 q network를 잘 학습시켜보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minimize해야하는거 아녀? 아닌가봐..?\n",
    "- expert가 선택한 action에 해당하는 Q 값 ($Q(s, a_E$) 보다 \n",
    "현재 Q 함수를 가장 크게 하는 a가 다르다면 gap은 음수일 것이다.\n",
    "- 예를들어 expert가 선택한 best action이 $a_2$이고 현재 Q-function을 최대로 하는 action이 $a_1$이라면, 현재 Q-function을 기준으로 $Q(s, a_2) - \\max_a Q(s, a) = Q(s, a_2) - Q(s, a_1) \\leq 0$이다. 만약 expert의 선택과 현재 Q-functino을 최대로 하는 action이 같다면 $Q(s, a_2) - \\max_a Q(s, a) = 0$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - deepmind's paper\n",
    "> - expert's loss function: $l(a, a_E) = I[a \\neq a_E]$ --> expert의 행동이 내 Q에 기반한 행동과 다르면 1을 증가시킨다.\n",
    "> - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \\max_{a \\in A}를 계산하는게.. discrete action space에서만 가능. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 빨간줄은... expert's demonstration으로 학습한 것\n",
    "- 파란선은 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> loss를 하나식 빼고 해본다. 어떤 loss가 영향이 있는지 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - export loss에 ddpg loss를 추가함.(즉 expert 선택에 가까워지면서도 ddpg 방법을 섞음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> buckeley\n",
    "- 빨간색이 2 norm loss 추가한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ablation???\n",
    "- expert's loss 를 빼면 파란색과 같이 엉망이 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
