{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 환경에 대한 정보는 transition model과 reward model이다. 그런데 일반적으로 reward model은 $f(s, a)$로 사람이 정하는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 어떤 상태에서 행동을 고를 때 과감한 도전을 해볼 것인가? 아니면 당장 보기에 좋은 선택을 할 것인가? trade-off가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 예시: explor vs exploi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지금까지 배웠떤 알고리즘들은 exploration을 어떻게 하고 있는가? \n",
    "- off-policy 접근: 탐색을 위한 별도의 정책을 마련\n",
    "- on-policy 접근: 현재 정책을 따라 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오늘 다룰 exploration 방법은 위와 같은 것들이 있다.\n",
    "- NE: greedy + epsilon random, stochastic policy \n",
    "- OI: Q 초기화를 굉장히 크게 한다.\n",
    "- OFU: 불확실성이 큰 행동을 선택한다.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\mathcal{A}$: action set\n",
    "- $\\mathcal{R}^{A_t}$: reward prob distribution given action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 지금까지 다뤘던 것들은 S, A, R이 연이어 등장하는 문제였다면, S는 없는(하나뿐인) 문제를 생각해본다.\n",
    "- arm 하나가 하나의 action, 여러 arm이 있다면 어떤 arm을 땡길 것인가?\n",
    "- $\\mathcal{R}^A(R)$: action이 주어졌을 때 reward의 확률분포, 이걸 모른다.\n",
    "- transition prob는 없음.\n",
    "- 누적 보상을 최대로 하는 정책을 찾아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mumti-Armed-Bandit 문제를 위한 새로운 형태의 $Q$함수를 정의. $Q(a) = \\mathbb{E}[r \\mid a]$\n",
    "- gap: $\\Delta_a=V^*-Q(a)$, 특정 행동 $a_t$에 대한 'optimal Q-value와 $Q(a_t)$의 차이\n",
    "- regret: $l_t$, gap의 기댓값\n",
    "- total regret: regret을 시간 축으로 더한 것, 즉 매 time-step의 regret을 더한 것\n",
    "- regret을 줄이는 정책을 찾으면 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (오타) 마지막 regret -> total regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\begin{align}\n",
    "L_t &= \\mathbb{E}\\bigg[ \\sum_\\tau^t v^* - Q(a_\\tau) \\bigg] \\\\\n",
    "&= \\mathbb{E} \\bigg[ N_t(a_1)\\Delta_{a_1} + N_t(a_2)\\Delta_{a_2} + \\cdots \\bigg] \\\\\n",
    "&= \\sum_i \\mathbb{E}[N_t(a_1)] \\Delta_{a_1}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\Delta_a$는 constant이므로 위와 같이 정리된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 첫식 이후... $E[\\sum_i^t v^* - Q(a_i) \\sum_a I(a_i = a)]$\n",
    "- $\\sum_{i=0}^t I(a_i=a)$ : 그 액션을 선택한 빈도\n",
    "- 만약 선택한 action이 optimal action이라면 gap은 0이 될 것이다. 반면 optimal이 아닌 action을 선택한다면 양의 값을 갖는 gap이 쌓이게 될 것이다. 즉 잘못된 행동을 선택할 때마다 gap이 커지게 된다. 또한 (최적이 아닌) 정책이 변하지 않는다면 시간에 따라 gap은 선형적으로 증가할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> total regret\n",
    "- action마다의 gap으로 표현(두번째 식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. random action: linear total regret(무작위 정책을 유지하므로)\n",
    "2. no exploration: linear total regret(정책이 개선되지 않으므로)\n",
    "3. 잘못된 행동 선택이 점차 줄어든다면 sublinear하게 regret이 증가할 것이다. 이런 정책을 찾는 것이 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 잘못된 action을 선택하고 있다면 time-step이 지날때마다 이게 선형적으로 늘어날것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> greedy하게만 행동을 선택하면 잘못된 행동만을 고르게될수도 있다. -> suboptimal\n",
    "- suboptimal을 탈출하려면 당장 optimal이 아닌것 같은 행동도 탐색해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> why linear total regret? 항상 greedy action만 선택하므로 보다 나은 action을 선택할 기회를 잃는다. 따라 정책이 개선되지도 않으므로 시간에 따라 선형적으로 regret이 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 28:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $|\\mathcal{A}|$: size of action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $l_t$: minimum regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 탐색을 하기 때문에 정책이 개선되겠지만, epsilon만큼은 항상 suboptimal action을 선택할 가능성을 갖기 때문에 시간에 선형적으로 regret이 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오타? epsilon-greedy가 되어야?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $d$: 0보다 큰 최소 gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $d$(regret)가 클때, 즉 Q-function을 제대로 학습하지 못했을 때에는 최소 $1=100\\%$ 탐색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> greedy in the limit\n",
    "- epsilon을 점차 줄이는 방법\n",
    "- time-step이 계속될수록 점차 random action을 선택할 확률이 줄어든다. 이 경우 sublinear하다는 것이 알려져 있다. 그런데 epsilon을 그냥 줄이는 것이 아니라, 원칙이 있다..\n",
    "- 이렇게 하면 log t를 따라서 regret이 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그런데 reward 분포를 모르므로 $\\Delta_a$에 대한 정보가 없다. 따라서 사용하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> total regret의 lower bound가 알려져 있다. 즉 어떤 알고리즘을 쓰더라도 이 lower bound 이상의 total regret이 있을수밖에 없다.\n",
    "- $(\\log t) * \\sum$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Q function의 초기값을 잘 줘서, reward가 갖을 수 있는 값 보다 더 큰 값으로 초기화\n",
    "- MC로 업데이트 하면... 너무 큰 것들은 값이 줄어들게 된다.\n",
    "- 다음 선택에서는 선택되지 않은 것들 혹은 덜 선택되었던 행동이 선택된다.\n",
    "- argmax Q로 action을 선택하므로 모두 동일하게 키워놓으면, 탐색안해본 큰 것들을 선택하게 되어 탐색이 쉬워진다. \n",
    "\n",
    "\n",
    "> - discrete action space인 경우 구현하기 쉽다. 하지만 neural net에서는 어려움?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사실 이런 것들을 Q-learning에서 탐색 정책으로 사용할수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 46:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그냥 Q를 찾는 것이 아니라 Q의 분포를 추정한다.\n",
    "- 어떤 행동의 불확실성이 크다면 그 행동이 가장 큰 reward를 줄 가능성이 크다?? (아닐 가능성도 크지 않나..)\n",
    "- 특정 action 분포의 variance가 크다면 이 action을 선택해서 variance를 줄여야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> UCB: Q의 variance와 같은 의미. 어떤 행동을 많이 해보지 않았다면 불확실성이 크다. 반면 많이 해 봤다면 불확실성(UCB)이 작아진다. 즉 여러 행동중 UCB가 가장 큰 행동을 골라 탐색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그렇다면 어떤식으로 Upper Confidence Bound를 정의해야하나? \n",
    "- Hoeffding's Inequality를 이용한 UCB가 있다. 이 Inequality는 진짜 expectation이 sample mean보다 $u$만큼 더 큰 쪽에 있을 확률은 $e^{-2tu^2}$보다 작다는 것이다.\n",
    "- t는 특정 action을 몇번 골랐느냐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $p=e^{-2N_t(a) \\hat{U}_t(a)^2}$가 작을수록 $\\hat{U}_t(a)$는 커진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 각 action을 많이 관측할수록 UCB가 작아지도록 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$a*_t = \\arg\\max_a \\big( \\hat{Q}_t(a) + \\sqrt{\\frac{2\\log{t}}{N_t(a)}} \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 알고리즘들은 total regret이 $N_t(a)$에 선형적이었다. 반면 UCB 알고리즘은 $\\log{t}$에 비례한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> x축이 log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 60:00 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 UCB 접근에서는 Q값의 불확실성을 어떤 어떤 간접적인 수치로 표현했었다. 보다 본격적으로 $\\mathcal{R}$의 분포를 가정는 Bayesian 접근법을 생각해볼 수 있다. 각 행동에 대한 $\\mathcal{R}$의 사전분포가 주어지고, 관측된 데이터의 likelihood를 계산하고 이 둘을 이용해 각 행동에 대한 $\\mathcal{R}$의 사후분포를 계산하는 것이다. (action space가 10이라면 10개 $\\mathcal{R}$의 분포가 존재한다.)\n",
    "\n",
    "> 만약 사후분포(posterior distribution)을 계산하는 것이 쉽지 않다면 Thompson sampling과 같은 sampling 기법을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mu_a$와 $\\sigma_a^2$에 대한 사전정보(분포)를 사용\n",
    "- posterior , = prior, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사전분포가 정규분포이면 사후분포 또한 정규분포인 성질을 이용해 쉽게 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathcal{R}$의 분포가 정규분포 $\\mathcal{N}(r_t;\\mu_a, \\sigma^2)$이고,    \n",
    "$\\mu_a$의 사전분포가 $\\mathcal{N}(\\mu_a; \\mu_0, \\sigma_0^2)$이고,         \n",
    "$h_t = (a_0, r_0, \\cdots, a_{t-1}, r_{t-1})$이 관측되었다면     \n",
    "- $h_t = (a_0, r_0, \\cdots, a_{t-1}, r_{t-1})$이 관측될 likelihood는 $\\prod_t \\mathcal{N}(r_t; \\mu_a, \\sigma^2)$이고\n",
    "- $\\mu_a$의 사후분포는 아래와 같다.\n",
    "$$N(\\mu', \\sigma'^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 두번째 식. reward에 대한 likelihood\n",
    "- mu 분포의 variance 항을 UCB 형태로 사용(마지막식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i번째 action을 취했을 때 reward의 분포\n",
    "\n",
    "> - reward는 1혹은 0\n",
    "- 1이 나올 확률은 $\\mathcal{R}^i$\n",
    "- $\\mathcal{R}^i$의 prior는 beta 분포를 사용.\n",
    "- posterior는 다시 beta가 된다. 이를 다시 prior로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - prior of $\\mathcal{R}^i$: $\\mathcal{R}^i \\sim \\text{Beta}(\\alpha^i, \\beta^i)$\n",
    "- sampling dist: $\\mathcal{r_t^i} \\sim \\text{Bernoulli}(\\mathcal{R}^i) = P(\\mathcal{R}^i; \\alpha^i, \\beta^i)$\n",
    "- likelihood of $(a_0, r_0, \\cdots, a_{t-1}, r_{t-1})$: $\\prod_i^K P(\\mathcal{R}^i; \\alpha^i, \\beta^i)$\n",
    "- posterior of $\\mathcal{R}^i$: $\\text{Beta}(\\alpha^i + \\sum r^i_{\\tau}, ~ \\beta^i + N_i - \\sum{r^i_{\\tau}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위와 같은 형태의 표현 방식을 Plate notation이라고 부른다. https://en.wikipedia.org/wiki/Plate_notation \n",
    "\n",
    "> 일반적으로 random variable을 동그라미로 표현 하고 deterministic variable은 사각형, 점 등으로 표현 방법이 약간씩 다른 것 같다. deterministic variable의 표현은 논문이나 발표자료에 따라 약간씩 다를 수 있지만 random variable을 동그라미로 표현하는 것은 모두가 지키는 룰이다. \n",
    "\n",
    "> 그리고 회색의 의미는 관측되었거나 알고있는 variable을 뜻한다. 반대로 흰색의 의미는 관측되지 않았거나 직접 관측 할 수 없는 variable을 뜻한다. 예제에서는 prior 분포의 hyperparameter $\\theta = { (\\alpha, \\beta) }$와 여태까지 선택한 action $a_t$와 reward $r_t$를 관측 했기 때문에 회색으로 표현 된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3개의 행동 $a_1, a_2, a_3$이 있다고 가정한다. 각 행동을 했을 때 당첨(reward > 0)되는 사건은 각각 $r_1 \\sim \\text{Bern}(\\mathcal{R}_1), r_2 \\sim \\text{Bern}(\\mathcal{R}_2), r_3 \\sim \\text{Bern}(\\mathcal{R}_3)$의 분포를 갖는다. 각 분포를 결정하는 hyperparameter $\\mathcal{R}_1, \\mathcal{R}_2, \\mathcal{R}_3$은 각각 다음과 같은 분포를 갖는다는 사전정보를 갖고 있다.\n",
    "$$\\begin{align}\n",
    "\\mathcal{R}_1 &\\sim \\text{Beta}(2, 5) \\\\\n",
    "\\mathcal{R}_2 &\\sim \\text{Beta}(2, 2) \\\\\n",
    "\\mathcal{R}_3 &\\sim \\text{Beta}(5, 1) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> 3개의 행동 중 어느 행동을 선택할지 결정하기 위해 각 행동에 따른 보상의 분포를 알아야 한다. 보상의 분포를 결정하는 hyperparameter가 상수가 아니라 (사전)분포이므로, 이 사전분포에서 값을 샘플링하여 $r_1, r_2, r_3$의 분포를 결정한다.\n",
    "\n",
    "> 이 예에서는 $\\mathcal{R}_1 \\sim \\text{Beta}(2, 5)$에서 샘플링하여 0.2를, $\\mathcal{R}_2 \\sim \\text{Beta}(2, 2)$에서 샘플링하여 0.5를, $\\mathcal{R}_3 \\sim \\text{Beta}(5, 1)$에서 샘플링하여 0.9를 뽑았고, $r_1, r_2, r_3$의 분포를 아래와 같이 정한다.\n",
    "$$\\begin{align}\n",
    "r_1 &\\sim \\text{Bern}(0.2) \\\\\n",
    "r_2 &\\sim \\text{Bern}(0.5) \\\\\n",
    "r_3 &\\sim \\text{Bern}(0.9) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> $Q(a) = \\mathbb{E}[r_0^i]$라 했고, Bern 분포의 파라미터 값과 같으므로 $Q(a_1)=0.2, Q(a_2)=0.5, Q(a_3)=0.9$이다.    \n",
    "그런데 최적 행동으로 $\\arg\\max_aQ(a)$를 선택하기로 했으므로 $a_3$을 선택한다.\n",
    "\n",
    "> $a_3$을 선택했을 때 당첨될수도 있고 안될 수도 있는데, 우린 당첨되었다.(R:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 $a_3$을 선택하여 당점되었고, 이 정보를 이용해 $\\mathcal{R}_3$의 분포의 사후 분포를 구한다. 사후 분포는 $\\text{Beta}(6, 1)$이고 이를 다음 샘플링에서 $\\mathcal{R}_3$의 사전분포로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다시 $r_1, r_2, r_3$의 사전분포(Beta)에서 hyperparameter를 샘플링하고, $r_1, r_2, r_3$중 기댓값이 가장 큰 행동을 선택하여 보상을 관찰한다. (이 과정을 반복)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (오타) argmax아니라 max임.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 어떤 행동 Q-function을 최대로 하는 행동일 확률에 비례해 그 행동을 뽑는다. 그런데 불확실성이 큰 행동은 그만큼 (긍적적으로 보면) 큰 Q 값을 갖을 확률이 크다고 볼 수 있다. 따라서 불확실성이 큰 확률을 뽑는 것과 같다.\n",
    "\n",
    "> 현재 상태에서 할 수 있는 행동들이 몇가지 있고, 이 행동 각각에 대한 $\\mathcal{R_a}$의 사후분포에서 $r_a$을 샘플링하고, 기댓값이 가장 큰 행동을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dropout = hyper parameter sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 1:22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-a/week07-a-35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 환경 혹은 모델은 reward function과 transition probability를 말한다. \n",
    "- 이 둘을 알고 있다면 MDP를 풀어 optimal policy/value를 찾을 수 있을 것이다.\n",
    "- 보통 reward는 사람이 세팅한다 가정하고, transition probability를 모르는 것을 해결하기 위해 RL에서는 sampling을 이용해 policy나 value를 추정했다.\n",
    " - 이는 환경으로부터 얻은 정보를 이용해 바로 policy나 value를 학습하는 것이다.\n",
    "- 환경으로 부터 얻은 정보로 바로 policy나 value를 학습시키는 것이 아니라 transition probability를 학습시키는 것이 model-based RL의 접근법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (오타) 처음, 마지막 experience 아니고 environment, 가운데건 model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Model-based\n",
    " 1. $D$에서 $s_0$를 뽑고, 내 정책으로 $a_0$를 고른다.\n",
    " 2. 환경에 $s_0, a_0$을 던지고 $s_1, r_0$을 관찰한다.\n",
    " 3. $s_0, a_0, s_1, r_0$을 이용해 내 정책을 수정한다.\n",
    " 4. 1부터 다시 반복\n",
    " \n",
    " \n",
    "> - Model-free\n",
    " 1. $D$에서 $s_0$를 뽑고, 내 정책으로 $a_0$를 고른다.\n",
    " 2. 환경에 $s_0, a_0$을 던지고 $s_1, r_0$을 관찰한다.\n",
    " 3. $s_0, a_0, s_1, r_0$을 이용해 내 모델을 수정한다.\n",
    " 4. 내 모델 정보를 이용해 MDP를 풀어 Value/Policy function을 학습한다.\n",
    " 5. 1부터 다시 반복\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Model-Based RL의 장점은?\n",
    " - transition probability를 학습하는건 $s, a$가 주어졌을 때 $s'$을 타겟으로 지도학습 하는 것이므로 value/policy function을 학습하는것보다 쉽다.\n",
    "\n",
    "\n",
    "> - 단점은?\n",
    " - discrete action space에서는 MDP를 푸는 것이 쉽지만, continuous action space에서는 MDP를 푸는 것이 쉽지 않다. 이때는 model에서 trajectory를 sampling해서 value/policy function을 학습시켜야 한다. 그런데 이렇게 하면 우선 모델을 학습시킬 때 오차가 있고, 모델에서 뽑은 샘플로 value/policy function을 학습시킬 때 또 오차가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모델이란 transition prob, reward func을 의미\n",
    "- 일반적으로 reward는 조건에 따라 사람이 정하는 것이므로 학습시키지 않음.\n",
    " - rewards learning: $s$, $a$에 따른 $r$ 추정 - regression\n",
    " - $s$, $a$가 주어졌을 때 $s'$로 전이될 확률 추정 - density estimation (transition prob은 density estimation을 하는건 드물고, $(s, a) \\rightarrow s'$의 regression을 푸는 식으로 진행)\n",
    " \n",
    "> - density estimation? most basic form is a rescaled histogram\n",
    "\n",
    "\n",
    "> - traisition prob를 학습하는건 Reinforcement learning에서만 사용하는게 아니라 기존의 Optimal Control등 분야에서도 해왔던 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - transition prob은 (s, a)를 입력으로 하고 s'를 출력으로 하는 다양한 형태의 함수추정 방법을 사용\n",
    " - 가장 기초적인 방법은 tablular 문제에서 그냥 샘플에서 $(s, a) \\rightarrow s'$를 카운트하는 방법이 있고\n",
    " - 보통의 지도 학습 문제로 MSE를 최소하는 방향으로 학습하거나, 혹은 transition prob를 gaussian으로 모델링하고 KL divergence loss를 사용할수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> state space와 action space가 모두 **discrete**한 경우 table lookup 방법으로 trainsition prob을 찾을 수 있다. \n",
    "1. table lookup : emphirical 하게 prob를 계산\n",
    "2. transition prob을 알게 되었으므로 MDP를 푼다. 혹은 RL 방법을 사용해서 policy/value function을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **continuous** state action space에서는 density estimation이나 supervised learning으로 transition prob을 학습한다. \n",
    "- supervised learning을 사용하는 경우...\n",
    " 1. 임의로 $s$를 고른다.\n",
    " 2. 현재의 정책으로 $a$를 결정하고 환경에 $(s, a)$를 던져서 $s'$을 얻는다. \n",
    " 3. $(s, a)$를 입력으로 하고 $s'$를 출력으로 하는 모형을 학습시킨다. 그 결과가 transition model이고\n",
    " 4. transition model(probability)을 이용해 MDP를 풀어 내 정책을 개선한다.(Planning)\n",
    " 4. $s'$를 다시 시작점으로 하여 2부터 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 그런데 $s_0$로부터 time-step이 지날수록 예측 오차가 쌓이게 된다. 이런 문제를 해소하기 위해 'Model Predictive Control'에서는 $s, a, s'$ 시퀀스의 초반 액션만을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 학습된 f(s, a)를 이용하는데, 랜덤한 행동을 선택해서 f(s, a)의 출력을 본다. 이런 과정을 반복하면 tractory가 쌓인다. 이런 tracjectory들의 reward를 각각 계산하고, 가장 reward가 큰 궤적을 선택, \n",
    "- 그런데 실제 이 궤적으로 환경에서 실험하면 잘 안되므로, 이 궤적의 초기 행동만 취해서 환경의 결과를 보고 이것으로 model update에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> backprob을 이용한 방식??\n",
    "- tractory\n",
    "- 첫 상태에서 핵션을 하나 고르고, 리워드를 계산하고\n",
    "- 다음 상태를 f(s, a)로 구하고, \n",
    "- 정책을 통과시켜서 행동을 고르고, 리워드 계산하고\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - time-setp $t$에서 $J$를 최대화하는 $a_t$를 찾도록 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-b/week07-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. 34:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> demonstration이란 (s, a, r) 쌍을 의미함\n",
    "- 이것들을 이용해 q network를 잘 학습시켜보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minimize해야하는거 아녀? 아니! 하나의 Q-func에 대한 것이라서 maximize\n",
    "- expert가 선택한 action에 해당하는 Q 값 ($Q(s, a_E$) 보다 \n",
    "현재 Q 함수를 가장 크게 하는 a가 다르다면 gap은 음수일 것이다.\n",
    "- 예를들어 expert가 선택한 best action이 $a_2$이고 현재 Q-function을 최대로 하는 action이 $a_1$이라면, 현재 Q-function을 기준으로 $Q(s, a_2) - \\max_a Q(s, a) = Q(s, a_2) - Q(s, a_1) \\leq 0$이다. 만약 expert의 선택과 현재 Q-functino을 최대로 하는 action이 같다면 $Q(s, a_2) - \\max_a Q(s, a) = 0$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - deepmind's paper: https://arxiv.org/pdf/1704.03732.pdf (Deep Q-learning from Demonstrations)\n",
    "\n",
    "\n",
    "> - expert's loss function: $l(a, a_E) = I[a \\neq a_E]$ --> expert의 행동이 내 Q에 기반한 행동과 다르면 1을 증가시킨다.\n",
    "> - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. 43:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - double DQN loss\n",
    "$$J_{DQ}(Q) = \\big( R(s, a) + \\gamma Q(s_{t+1}, a_{t+1}^{\\max}; \\theta') - Q(s, a; \\theta)  \\big)^2$$\n",
    "- n-step loss\n",
    "$$J_n(Q) = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{n-1} r_{t+n-1} + \\max_a \\gamma^n Q(s_{t+n}, a) - Q(s, a; \\theta)$$\n",
    "- large margin classification loss\n",
    "$$J_E(Q) = \\max_{a \\in A}[Q(s, a) + l(a_E, a)] - Q(s, a_E)$$\n",
    "- L2 regularization loss\n",
    "$$J_{L2}(Q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그런데 'large margin classification loss'를 계산하는건 discrete action space에서만 가능. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - DQfD: 파란줄\n",
    "- Imitation: 빨간줄(expert)\n",
    "- PDD DQN: 녹색줄(Prioritized Dueling Double DQN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> loss를 하나식 빼고 해본다. 어떤 loss가 영향이 있는지 실험(이런 비교를 ablations(어블리에이션) study라 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - continuous action space 문제에 대한 접근 방법들\n",
    " - DDPG에서는 Policy network와 Q network가 있었다. expert's demonstration을 이용하기 위해 Policy network의 output과 expert's choice가 같아지도록 하는 loss를 추가한다.\n",
    "$$J_E(\\theta) = \\mathbb{E}[(\\mu_\\theta(s) - a_E)^2]$$\n",
    " - DDPG loss(gain, 최대화 함)도 사용한다. $\\nabla_\\theta J(\\theta)$\n",
    " - 둘을 결합해 $\\nabla_\\theta J(\\theta) - \\nabla_\\theta J_E(\\theta)$를 최대화 하도록 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ;2. 51:50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> berkeley's research\n",
    "- 빨간색이 2 norm loss 추가한 것\n",
    "- BC(Behavior Cloning)\n",
    "- 파란색은 $J_E$를 빼고 트레이닝\n",
    "- 빨간색이 더 잘됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week07-c/week07-c-9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ablation???\n",
    "- expert's loss 를 빼면 파란색과 같이 엉망이 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
