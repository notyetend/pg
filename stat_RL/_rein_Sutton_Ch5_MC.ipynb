{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter5. Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 에피소드 return의 평균으로 state-value $V(S_t)$를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Monte Carlo Estimation of Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state-value $v_{\\pi}(s)$에 대한 추정치를 이용할 경우 최적 정책을 결정하기 위해 model에 대한 정보(reward function, state transition probability)가 필요하다. model-free가 되기 위해 action-value $q_{\\pi}(s, a)$에 대한 추정치를 사용한다. 그런데 실제 환경에는 존재하지만 에피소드에 등장하지 않은 action-state 쌍이 있을 수 있다. 특정 state에서 가능한 action들 중 최적의 action을 찾아야하는데, 방문하지 않은 경우가 있다면 $q_*$로의 수렴을 보장할 수 없다. \n",
    "\n",
    "이런 문제를 'maintaining exploration'이라 하며 이를 해결하기 위해 '어떤 state-action 쌍이라도 에피소드의 시작점이 될 확률이 0보다 크다.'라는 가정을 두는 방법이 있는데 이를 'exploring starts'라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'exploring start'와 관찰할 수 있는 에피소드가 무한하다는 것을 가정하면, Monte Carlo policy iteration(MC GPI)을 통해서도 $q_*$로 수렴한다.\n",
    "\n",
    "('exploring start'는 유지한 체)에피소드가 무한하다는 가정을 완화하기 위해 매 에피소드마다 action-value를 evaluate & improve하는 방법을 쓸 수 있다. 이 방법을 Monte Carlo ES(Exploring Starts)라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Monte Carlo Control without Exploring Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'exploring start'라는 가정을 완화하기 위한 방법은 크게 on-policy와 off-policy로 나뉜다. on-policy 방법중 하나가 $\\epsilon$-greedy 인데, 이는 $\\epsilon$의 확률로 랜덤한 action을 취하는 것이다. MC ES와 같이 매 에피소드마다 evaluate하는데 improve를 $\\epsilon$-greedy로 하는 것을 **MC Control**이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Off-policy Predicition via Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "control 방법들은 최적 정책을 향해 action-value를 학습시켜야하면서도 당장 보기에는 꼭 최적은 아닌 정책을 따라서 여러 action을 탐색하기도 해야하는 딜레마가 있다. on-policy 방법은 사실 이 두가지 상충하는 목표를 절충한 것으로서 살짝 덜 최적인 정책을 학습(learn about and becomes the optimal policy)과 탐색(exploratory)에 같이 사용하는 것이다. \n",
    "\n",
    "그렇다면 학습을 위한 정책과 탐색을 위한 정책을 별도로 가져가는 방법(off-policy)를 생각해볼 수 있다. 학습을 위한 정책을 target policy $\\pi$라 하며, 탐색을 위한 정책을 behavior policy $b$라 한다. 예를들어 $\\pi$는 greedy policy를 사용하고, $b$는 $\\epsilon$-greedy policy를 사용하는 것이다.\n",
    "\n",
    "학습하는 정책과 탐색하는 정책이 다르다면, 탐색하는 정책으로 생성된 에피소드들을 이용해 어떻게 학습하는 정책을 업데이트 할 수 있을까? 즉 탐색 정책 $b$로 생성된 에피소드로 구해진 return과 실제 업데이트 하려는 학습 정책 $\\pi$와의 간극을 어떻게 줄일 것인가? 이를 보정하는 방법이 바로 Importance Sampling(주표본 기법)이다. 에피소드들로 구해진 return들의 기댓값을 이용해 $V(s)$를 업데이트하는데, '$\\pi$에서 특정 에피소드가 나올 확률과 $b$에서 특정 에피소드가 나올 확률의 비($\\rho_{t:T-1}$)'만큼 return을 가중해서 $V(s)$를 업데이트하는 것이다.\n",
    "\n",
    "여기서 사용하는 Importance sampling은 ordinary importance sampling과 weighted importance sampling으로 나뉘는데 이를 논하기 위해 아래의 표기법을 도입한다.\n",
    "\n",
    "- $t$: 모든 에피소드의 타입 스텝의 순차적 순번\n",
    "- $\\Gamma(s)$ : 상태 $s$를 방문했던 $t$들의 집합(first-visist에 대해서는 각 $s$마다 단 하나의 $t$만 존재할 것이다.\n",
    "- $T(t)$ : $t$를 따라갔을 때 처음만나는 termination의 $t_T$\n",
    "- $G_t$ : $t$를 따라갔을 때 $T(t)$까지의 return\n",
    "- $\\{G_t\\}_{t \\in \\Gamma(s)}$ : 상태 $s$에 해당하는 return들\n",
    "- $\\{ \\rho_{t : T(t)-1}\\}_{t \\in \\Gamma(s)}$ : $\\{G_t\\}_{t \\in \\Gamma(s)}$ 각각에 해당하는 importance-sampling ratio\n",
    "\n",
    "우선 ordinary importance sampling에서는 $V(s)$를 아래와 같이 정의한다.\n",
    "\n",
    "$$V(s) \\doteq \\frac{\\sum_{t \\in \\Gamma(s)} \\rho_{t:T(t)-1}G_t}{\\mid\\Gamma(s)\\mid}$$\n",
    "\n",
    "다음으로 weighted importance sampling에서는 $V(s)$를 아래와 같이 정의한다.\n",
    "$$V(s) \\doteq \\frac{\\sum_{t \\in \\Gamma(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t \\in \\Gamma(s)} \\rho_{t:T(t)-1}}$$\n",
    "\n",
    "weighted importance sampling는 variance가 매우 낮지만 상대적으로 bias가 심한 문제가 있고 ordinary importance sampling는 unbiased 이지만 variance가 심한 문제가 있다. 실전에서는 weighted importance sampling를 많이 쓴다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Off-policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 방법들을 조합하면 'Off-policy Monte Carlo control method based on GPI and weighted importance sampling'이 된다.\n",
    "- action-value\n",
    "- update for each episode\n",
    "- seperated policy for learning($\\pi$) and exploration($b$)\n",
    "- greedy for $\\pi$, $\\epsilon$-greedy for $b$\n",
    "- use importance sampling to update $\\pi(S_t)$ with $G_b$\n",
    "\n",
    "그런데 위 방법은 에피소드의 초반 선택이 greedy 했다면 에피소드의 후반만 학습에 사용되므로??? 학습이 느리다는 문제가 있다. 이를 TD로 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 \\*Discounting-aware Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 \\*Per-decision Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
