{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter7. $n$-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD(one-step)\n",
    " - 에피소드를 끝까지 보지 않고도 학습 가능 ($\\rightarrow$ 온라인 학습 가능)\n",
    " - 종료 상태가 없는(continuing, non-terminating) 환경에서도 사용가능\n",
    " - bias가 클 수 있고 초기값에 민감하다.\n",
    "- MC\n",
    " - 에피소드를 끝까지 봐야한다.($\\rightarrow$ 온라인 학습 불가)\n",
    " - 종료 상태가 있는(episodic, terminating) 환경에서만 사용 가능\n",
    " - bias가 상대적으로 작고 초기값에 덜 민감하다.\n",
    " \n",
    "> MC와 TD(1)는 위와 같은 장단점을 갖고 있다.    \n",
    "MC는 다 보고 업데이트하고, TD(1)은 하나식만 보고 업데이트를 한다.    \n",
    "이는 에피소드를 다루는 방식에 있어서 양 극단이라 할 수 있다.     \n",
    "전부 혹은 하나가 아니라 적당히 $n$개를 보고 업데이트 하는 방식을 생각해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 $n$-step TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MC Prediction**은 각 에피소드의 마지막 상태까지의 반환값(return, $G_t$)을 타겟으로 $V(S_t)$를 업데이트 한다. 이때 $G_t$는 첫 보상($R_{t+1})$부터 마지막 보상($R_T$)까지 점점 더 할인된 값들의 합이다.\n",
    "\n",
    "$$\\begin{align}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha \\bigg[ G_t - V(S_t) \\bigg] \\\\\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | r^{T-t-1} R_T \\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면 **one-step TD Prediction**혹은 TD(0)는 상태 $S_t$의 보상($R_{t+1}$)과     \n",
    "$\\gamma$만큼 할인된 다음 상태 $S_{t+1}$의 가치 추정치($\\gamma V(S_{t+1})$)를 더한 값을 타겟으로           \n",
    "$v_{\\pi}(S_t)$에 대한 추정치 $V(S_t)$를 업데이트 한다.\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\bigg[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\bigg]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0)에서 하나의 다음 상태를 보고 업데이트 했다면 **n-step TD Prediction**은 $n$개의 다음 상태를 보고 업데이트 하겠다는 것이다. 예를들어 2-step TD에서는 아래와 같이 $V(S_t)$업데이트 한다. <font color='grey'>$$V(S_t) \\leftarrow V(S_t) + \\alpha \\bigg[ R_{t+1} + \\gamma R_{t+1} + \\gamma^2 V(S_{t+2}) - V(S_t)\\bigg]$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 n-step TD를 표현하기 위해 몇가지 표기법을 도입한다.     \n",
    "우선 $t$시점의 보상($R_t$)부터 $t+n$ 시점까지의 보상($R_{t+n}$)을 사용하여 추정한 반환값(return)을 $G_{t:t+n}$이라 표기하고,     \n",
    "$t+n$시점에 추정한 $t$시점의 상태($S_t$)의 가치를 $V_{t+n}(S_t)$라 표기한다.\n",
    "\n",
    "$$\\begin{align}\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n}) \\\\\n",
    "V_{t+n}(S_t) &\\doteq V_{t+n-1}(S_t) + \\alpha \\bigg[ G_{t:t+n} - V_{t+n-1}(S_t)\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    " - $V(S_t) \\rightarrow V_{t+n}(S_t)$\n",
    " - $G_t \\rightarrow G_{t:t+n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/figure_7_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 $n$-step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 $n$-step Off-policy Learning by Importence sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 \\*Per-decision Off-policy Methods with Control Variates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 \\*A Unifying Algorithm: n-step $Q(\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
