{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I2A: architecture combining model-free and model-based aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing model-based RL: prescribe how a model should be used to arrive at a policy. \n",
    " - 일반적인 model-based RL에서는 모델의 결과물($R, S' \\leftarrow Model(S, A)$)을 이용해 policy나 value를 업데이트할 방법을 명확히 규정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I2A: learns to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways.\n",
    " - I2A에서는 모델의 출력을 인코딩하고 신경망의 추가적인 입력으로 사용해서, 이것이 어떻게 policy나 value를 업데이트하는지 알수 없다. 대신 이 과정을 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-free RL usually requires large amount of traning data and the resulting policies do not readily generalize to novel tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-based RL aims to address these shortcomings, but suffers from model errors resulting from function approximation. These errors compound during planning, causing over-optimism and poor agent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I2A shows robustness against model imperfections, which use approximate environment models by \"learning to interpret\" their imperfect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The I2A architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imagination core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment model together with rollout poicy $\\hat{\\pi}$ constitute the imagination core module, which predicts next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{o}_{t+1}, \\hat{r}_{t+1} = \\text{IC}(o_t)$\n",
    " - $\\hat{a}_{t} = \\text{PolicyNet}(o_t)$\n",
    " - $\\hat{o}_{t+1}, \\hat{r}_{t+1} = \\text{EnvModel}(o_t, \\hat{a}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{\\pi}$: rollout policy, which is determined by $\\text{PolicyNet}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Single Imagination rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imagination core is used to produce $n$ trajectories $\\hat{\\mathcal{T}}_1, \\dots, \\hat{\\mathcal{T}}_n$. Each imagined trajectory $\\hat{\\mathcal{T}}$ is a sequence of features $(\\hat{f}_{t+1}, \\dots, \\hat{f}_{t+\\tau})$, where $t$ is the current time, $\\tau$ the length of the rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \\hat{f}_{t+i} &= [\\hat{o}_{t+1}, \\hat{r}_{t+1} ] = \\text{IC}(\\hat{o}_{t+i-1}) \\\\\n",
    "\\hat{\\mathcal{T}}_i &= (\\hat{f}_{t+1}, \\dots, \\hat{f}_{t+\\tau})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each rollout $\\hat{\\mathcal{T}}_i$ is encoded as rollout embedding $e_i$, and then embeddings $e_1, \\dots, e_n$ are aggregated as $c_{\\text{ia}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_i = \\mathcal{E}(\\hat{\\mathcal{T}}_i) \\\\\n",
    "c_{\\text{ia}}=\\mathcal{A}(e_1, \\dots, e_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Full I2A Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final componet of the I2A is the policy module, which is a network that takes the information $c_{\\text{ia}}$ from model-based predictions, as well as the output $c_{mf}$ of a model-free path. The I2As learnings to combine information from its model-free and imagination-augmented path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi, V = \\text{FC}(c_{\\text{ia}}, c_{\\text{mf}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Architectural choices and experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Roll strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiments, we perform one rollout for each possible action in the environment. The first action in the $i^{\\text{th}}$ rollout is the $i^{\\text{th}}$ action of the action set $\\mathcal{A}$, and subsequent actions for all rollouts are produced by a shared rollout policy $\\hat{\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training rollout policy $\\hat{\\pi}$\n",
    "- by adding to the total loss 'a cross entropy auxiliary loss' between the imagination-augmented policy $\\pi$ and the policy $\\hat{\\pi}$, both for the current observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 I2A components and environment models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments, the encoder is an LSTM with convolutional layers which sequentially processes a trajectory $\\mathcal{T}$. The features $\\hat{f}_t$ are fed to the LSTM in reverse order to mimic Bellman type backup operations. (but choice of forward, backward bi-directional seems to have little impact on the performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training environment model\n",
    "- pretrain and freeze (this led to faster runtime of the I2A architecture compared to training jointly)\n",
    "- jointly train with full network by adding $l_{\\text{model}}$ to the total loss.   \n",
    "pre-trained env. model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all environments, training data for this environment model was generated from trajectories of a partially trained standard model-free agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Agent training and baseline agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a fixed pretrained env model, remaining I2A parameters are trained with A3C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added entropy regularizer on the policy $\\pi$ to encourage exploration and the auxiliary loss the distill $\\pi$ into the rollout policy $\\hat{\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- standard: A3C (I2A without model-based part)\n",
    "- standard(large): A3C with increased parameters. (slightly larger number of parameters than I2A)\n",
    "- copy-model I2A: replaced env model in the I2A with 'copy' model that simply return the input observation. (same number of parameters and same architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sokoban experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 I2A performance vs. baselines on Sokoban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Learning with imperfect models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Further insights into the workings of the I2A architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Imaginaiton efficiency and comparison with perfect-model planning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Generalization experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Learning one model for many tasks in MiniPacman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
