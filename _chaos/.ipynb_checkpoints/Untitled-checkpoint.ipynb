{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorizing across multiple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for data preparation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = np.where(df[4] == 'Iris-setosa', 1, 0)\n",
    "# print(df.head())\n",
    "\n",
    "X = np.array(df[[0, 1, 2]]).T;\n",
    "Y = np.array(df[[4]]).T\n",
    "\n",
    "J = 0  # total average loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 비디오에서는 '한건의 샘플 데이터가 주어졌을때 for-loop를 이용한 각 레이어의 노드값($z$, $a$) 계산하는 것'을 벡터화하 방법에 대해 알아봤었다. (이 내용은 아래 첫번째 코드가 두번째 코드로 바뀌는 것이다.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for Step 1\n",
    "$$\n",
    "\\begin{align}\n",
    "&m = \\text{number of samples} \\\\\n",
    "&n_h = \\text{number of nodes in the hidden layer} \\\\\n",
    "&n_y = \\text{number of nodes in the output layer} \\\\\n",
    "&\\textbf{for} ~ i=1 ~ \\text{to} ~ m ~ \\textbf{do} \\\\\n",
    "&\\qquad \\textbf{for} ~ j=1 ~ \\text{to} ~ n_h ~ \\textbf{do} \\\\\n",
    "&\\qquad \\qquad z^{[1](i)}_j = W^{[1]T}_j \\cdot x^{(i)} + b^{[1]}_j \\\\\n",
    "&\\qquad \\qquad a^{[1](i)}_j = \\sigma(z^{[1](i)}_j)\n",
    "&\\\\\n",
    "&\\qquad \\textbf{for} ~ j=1 ~ \\text{to} ~ n_y ~ \\textbf{do} \\\\\n",
    "&\\qquad \\qquad z^{[2](i)}_j = W^{[2]T}_j \\cdot z^{[1](i)} + b^{[2]}_j \\\\\n",
    "&\\qquad \\qquad a^{[2](i)}_j = \\sigma(z^{[2](i)}_j)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# using for-loop to compute values of each node\n",
    "# but still using for-loop for each sample\n",
    "\n",
    "m = X.shape[1]  # number of samples (=150)\n",
    "n_x = X.shape[0]  # number of features (=3)\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y.shape[0]  # size of output layer (= 1)\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "for i in range(m):  # for each sample\n",
    "    z1 = np.empty((n_h, 1))  # weighted sum for 1st hidden layer (n_h x 1 column vector)\n",
    "    a1 = np.empty((n_h, 1))  # activation of 1st hidden layer (n_h x 1 column vector)\n",
    "    i = 0\n",
    "    for j in range(n_h):  # for each node in the hidden layer\n",
    "        w1_j = W1[j,:].reshape(-1, 1)  # weights[1] for j'th node (column vector)\n",
    "        z1[j] = np.dot(w1_j.T, X[:, i].reshape(-1, 1)) + b1[j]\n",
    "        a1[j] = np.tanh(z1[j])\n",
    "\n",
    "    z2 = np.empty((n_y, 1))  # weighted sum for 2nd hidden layer (n_y x 1 column vector)\n",
    "    a2 = np.empty((n_y, 1))  # activation of 2nd hidden layer (n_y x 1 column vector)\n",
    "    for j in range(n_y):  # for each node in the output layer\n",
    "        w2_j = W2[j,:].reshape(-1, 1)  # weights[2] for j'th node (column vector)\n",
    "        z2[j] = np.dot(w2_j.T, a1) + b2[j]\n",
    "        a2[j] = sigmoid(z2[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for Step 2\n",
    "$$\n",
    "\\begin{align}\n",
    "&m = \\text{number of samples} \\\\\n",
    "&n_h = \\text{number of nodes in the hidden layer} \\\\\n",
    "&n_y = \\text{number of nodes in the output layer} \\\\\n",
    "&\\textbf{for} ~ i=1 ~ \\text{to} ~ m ~ \\textbf{do} \\\\\n",
    "&\\qquad z^{[1](i)} = W^{[1]} \\cdot x^{(i)} + b^{[1]} \\\\\n",
    "&\\qquad a^{[1](i)} = \\sigma(z^{[1](i)})\n",
    "&\\\\\n",
    "&\\qquad z^{[2](i)} = W^{[2]T}_j \\cdot z^{[1](i)} + b^{[2]} \\\\\n",
    "&\\qquad a^{[2](i)} = \\sigma(z^{[2](i)})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# vectorized computation for each node\n",
    "# but still using for-loop for each sample\n",
    "\n",
    "m = X.shape[1]  # number of samples (=150)\n",
    "n_x = X.shape[0]  # number of features (=3)\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y.shape[0]  # size of output layer (= 1)\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "for i in range(m):  # for each sample\n",
    "    z1 = np.empty((n_h, 1))  # weighted sum for 1st hidden layer (n_h x 1 column vector)\n",
    "    a1 = np.empty((n_h, 1))  # activation of 1st hidden layer (n_h x 1 column vector)\n",
    "    i = 0\n",
    "    \n",
    "    z1 = np.dot(W1, X[:, i].reshape(-1, 1)) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    z2 = np.dot(W2, z1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    loss = -1 * (np.log(a2) * Y[:,i] + (1-Y[:,i]) * np.log(1-a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이렇게 벡터화하더라도 각 데이터 샘플에 대해 위 계산을 반복 하는 것이 필요하다.\n",
    "\n",
    "이번 비디오에서는 (각 노드 연산을 벡터화하는 것에 추가하여) 각 데이터 샘플에 대한 반복 연산을 벡터화 하는 방법에 대해 알아본다.\n",
    "\n",
    "(각 노드 연산을 벡터화 하는 것과 무관하게) 입력값 $X \\in \\mathbb{R}^{n_x \\times m}$가 주어졌을 때 각 데이터 샘플 $x \\in \\mathbb{R}^{n_x \\times 1}$에 대해 $z^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times 1}$, $a^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times 1}$, $z^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1}$, $a^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1}$를 차례로 구하는 과정(+역전파)을 데이터 샘플수(m) 만큼 반복하게 된다.(Step 1과 Step 2 코드에서의 'for i in range(m):')\n",
    "\n",
    "이 반복을 벡터화 하려면 \n",
    "입력 데이터를 n_x by m 행렬 X로 바꾸면 이후 계산 값은 n^{[1]} by m 행렬인 Z^{[1]}, A^{[1]}과 n^{[2]} by m 행렬인 Z^{[2]}, A^{[2]}로 반복 없이 계산 가능하다. bias항 b^{[l]}은 n^{[l]} by 1행렬인데 Z^{[l]}=W^{[l]}X + b^{[l]}를 계산할 때에는 (파이썬 넘파이에 의해) n^{[l]} by m 행렬로 broadcast된다.\n",
    "\n",
    "(파이썬 구현 추가)\n",
    "\n",
    "- Explanation for vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69295259]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step1 : implementation with for-loop, 1 epoch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = np.where(df[4] == 'Iris-setosa', 1, 0)\n",
    "# print(df.head())\n",
    "\n",
    "X = np.array(df[[0, 1, 2]]).T;\n",
    "Y = np.array(df[[4]]).T\n",
    "\n",
    "m = X.shape[1]  # number of samples (=150)\n",
    "n_x = X.shape[0]  # number of features (=3)\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y.shape[0]  # size of output layer (= 1)\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "J = 0  # total average loss\n",
    "for i in range(m):  # for each sample\n",
    "    z1 = np.empty((n_h, 1))  # weighted sum for 1st hidden layer (n_h x 1 column vector)\n",
    "    a1 = np.empty((n_h, 1))  # activation of 1st hidden layer (n_h x 1 column vector)\n",
    "    i = 0\n",
    "    for j in range(n_h):  # for each node in the hidden layer\n",
    "        w1_j = W1[j,:].reshape(-1, 1)  # weights[1] for j'th node (column vector)\n",
    "        z1[j] = np.dot(w1_j.T, X[:, i].reshape(-1, 1)) + b1[j]\n",
    "        a1[j] = np.tanh(z1[j])\n",
    "\n",
    "    z2 = np.empty((n_y, 1))  # weighted sum for 2nd hidden layer (n_y x 1 column vector)\n",
    "    a2 = np.empty((n_y, 1))  # activation of 2nd hidden layer (n_y x 1 column vector)\n",
    "    for j in range(n_y):  # for each node in the output layer\n",
    "        w2_j = W2[j,:].reshape(-1, 1)  # weights[2] for j'th node (column vector)\n",
    "        z2[j] = np.dot(w2_j.T, a1) + b2[j]\n",
    "        a2[j] = sigmoid(z2[j])\n",
    "\n",
    "    loss = -1 * (np.log(a2) * Y[:,i] + (1-Y[:,i]) * np.log(1-a2))\n",
    "    \n",
    "    # skiped back prop\n",
    "    \n",
    "    J = (J * i + loss)/(i+1)  # computing $$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69320526]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step2 : implementation with for-loop, 1 epoch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = np.where(df[4] == 'Iris-setosa', 1, 0)\n",
    "# print(df.head())\n",
    "\n",
    "X = np.array(df[[0, 1, 2]]).T;\n",
    "Y = np.array(df[[4]]).T\n",
    "\n",
    "m = X.shape[1]  # number of samples (=150)\n",
    "n_x = X.shape[0]  # number of features (=3)\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y.shape[0]  # size of output layer (= 1)\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "J = 0  # total average loss\n",
    "for i in range(m):  # for each sample\n",
    "    z1 = np.empty((n_h, 1))  # weighted sum for 1st hidden layer (n_h x 1 column vector)\n",
    "    a1 = np.empty((n_h, 1))  # activation of 1st hidden layer (n_h x 1 column vector)\n",
    "    i = 0\n",
    "    \n",
    "    z1 = np.dot(W1, X[:, i].reshape(-1, 1)) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    z2 = np.dot(W2, z1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    loss = -1 * (np.log(a2) * Y[:,i] + (1-Y[:,i]) * np.log(1-a2))\n",
    "    \n",
    "    # skiped back prop\n",
    "    \n",
    "    J = (J * i + loss)/(i+1)  # computing $$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cost:[ 0.69314718]\n",
      "Execution time for loop version  :  16.011476516723633ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "b = 0;\n",
    "w = np.zeros((n_x, 1))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "J = 0;\n",
    "dw1 = 0;\n",
    "dw2 = 0;\n",
    "db = 0\n",
    "for i in range(m):\n",
    "    xi = X[:, i]\n",
    "    yi = Y[:, i]\n",
    "\n",
    "    zi = np.dot(w.T, xi) + b\n",
    "    ai = sigmoid(zi)\n",
    "    J += -(yi * np.log(ai) + (1 - yi) * np.log(1 - ai))\n",
    "    # if i % 10 == 0:\n",
    "    #    print('Actual: {}, Predicted:{}, Average cost up to now:{}'.format(yi, ai, J/(i+1)))\n",
    "\n",
    "    dzi = ai - yi\n",
    "    dw1 += xi[0] * dzi\n",
    "    dw2 += xi[1] * dzi\n",
    "    db += dzi\n",
    "J = J / m;\n",
    "dw1 = dw1 / m;\n",
    "dw2 = dw2 / m;\n",
    "db = db / m\n",
    "\n",
    "toc = time.time()\n",
    "print('Average cost:{}'.format(J))\n",
    "print('Execution time for loop version  : ', str(1000 * (toc - tic)) + 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
