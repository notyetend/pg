{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = np.where(df[4] == 'Iris-setosa', 1, 0)\n",
    "df_train = df.sample(frac=0.7, random_state=0)\n",
    "df_test = df.sample(frac=0.3, random_state=0)\n",
    "\n",
    "X_train = np.array(df_train[[0, 1]]).T\n",
    "Y_train = np.array(df_train[[4]]).T\n",
    "X_test = np.array(df_test[[0, 1]]).T\n",
    "Y_test = np.array(df_test[[4]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return np.maximum(0, Z)\n",
    "    else:  # derivative of ReLU, {1 if x>0, 0 if x <= 0}\n",
    "        return ((np.sign(Z) + 1)//2).astype(int)\n",
    "\n",
    "def sigmoid(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    else:  # derivative of sigmoid, a(1-a)\n",
    "        return np.multiply(sigmoid(Z), (1-sigmoid(Z)))\n",
    "\n",
    "def forward_propagation(A_l_prev, W_l, b_l, g_l):\n",
    "    Z_l = b_l + np.dot(W_l, A_l_prev)\n",
    "    A_l = g_l(Z_l)\n",
    "    return A_l, Z_l\n",
    "\n",
    "def get_random_b_W(n_l_prev, n_l):\n",
    "    W_l = np.random.randn(n_l, n_l_prev) * 0.01\n",
    "    b_l = np.zeros((n_l, 1))\n",
    "    return b_l, W_l\n",
    "\n",
    "def backward_propagation(dA_l, A_l_prev, W_l, Z_l, g_l):\n",
    "    m = dA_l.shape[1]\n",
    "    assert (m == Z_l.shape[1])\n",
    "\n",
    "    dZ_l = np.multiply(dA_l, g_l(Z_l, is_forward=False))\n",
    "    dW_l = np.dot(dZ_l, A_l_prev.T) / m\n",
    "    db_l = np.sum(dZ_l, axis=1, keepdims=True) / m\n",
    "    dA_l_prev = np.dot(W_l.T, dZ_l)\n",
    "\n",
    "    return dA_l_prev, dW_l, db_l\n",
    "\n",
    "\n",
    "class SimpleFfnn:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, layer_dims, layer_activations, random_seed=None):\n",
    "        np.random.seed(random_seed)\n",
    "        assert (layer_dims[0] == X_train.shape[0])\n",
    "        \n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.n_L = len(layer_dims) - 1  # input layer is not counted.\n",
    "        self.cache_A = [None] * (self.n_L + 1)\n",
    "        self.cache_Z = [None] * (self.n_L + 1)  # cache_Z[0] is not used.\n",
    "        self.cache_W = [None] * (self.n_L + 1)  # cache_W[0] is not used.\n",
    "        self.cache_b = [None] * (self.n_L + 1)  # cache_b[0] is not used.\n",
    "        self.cache_dA = [None] * (self.n_L + 1)\n",
    "        self.cache_dZ = [None] * (self.n_L + 1)  # cache_dZ[0] is not used.\n",
    "        self.cache_dW = [None] * (self.n_L + 1)  # cache_dW[0] is not used.\n",
    "        self.cache_db = [None] * (self.n_L + 1)  # cache_db[0] is not used.\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        for i in range(1, len(layer_dims)):  # 1, 2, ..., n_L\n",
    "            n_l = self.layer_dims[i]\n",
    "            n_l_prev = self.layer_dims[i-1]\n",
    "\n",
    "            self.cache_b[i], self.cache_W[i] = get_random_b_W(n_l_prev, n_l)\n",
    "    \n",
    "    def forward_propagation_deep(self):\n",
    "        self.cache_A[0] = self.X\n",
    "        \n",
    "        for i in range(1, len(layer_dims)):  # 1, 2, ..., n_L\n",
    "            n_l = self.layer_dims[i]\n",
    "            n_l_prev = self.layer_dims[i-1]\n",
    "            A_l_prev = self.cache_A[i-1]\n",
    "            W_l = self.cache_W[i]\n",
    "            b_l = self.cache_b[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "            \n",
    "            self.cache_A[i], self.cache_Z[i] = forward_propagation(A_l_prev, W_l, b_l, g_l)\n",
    "            \n",
    "    def backward_propagation_deep(self):\n",
    "        Y_actual = self.Y\n",
    "        Y_prediction = self.cache_A[self.n_L]\n",
    "        \n",
    "        self.cache_dA[self.n_L] = - (np.divide(Y_actual, Y_prediction) - np.divide(1 - Y_actual, 1 - Y_prediction))\n",
    "        \n",
    "        for i in list(reversed(range(1, len(layer_dims)))):  # n_L, n_L-1, ..., 2, 1\n",
    "            dA_l = self.cache_dA[i]\n",
    "            A_l_prev = self.cache_A[i-1]\n",
    "            W_l = self.cache_W[i]\n",
    "            Z_l = self.cache_Z[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "            \n",
    "            self.cache_dA[i-1], self.cache_W[i], self.cache_b[i] = backward_propagation(dA_l, A_l_prev, W_l, Z_l, g_l)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train.shape[0]\n",
    "\n",
    "layer_dims = [n_x, 4, 4, 1]\n",
    "layer_activations = [None, relu, relu, sigmoid]\n",
    "\n",
    "nn = SimpleFfnn(X_train, Y_train, layer_dims, layer_activations, random_seed=1)\n",
    "nn.forward_propagation_deep()\n",
    "nn.backward_propagation_deep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> activation matrix of l's layer:\n",
      " [[ 0.          0.02917199]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00994349  0.01168037]\n",
      " [ 0.01165128  0.        ]\n",
      " [ 0.          0.        ]] \n",
      "> shape(n_l, m): (5, 2)\n"
     ]
    }
   ],
   "source": [
    "#  TEST CODE\n",
    "np.random.seed(1)  # to make test's result consistent.\n",
    "\n",
    "n_l_prev = 4  # number of nodes in l-1's layer\n",
    "n_l = 5  # number nodes in l's layer\n",
    "m = 2  # sample size for this batch\n",
    "\n",
    "# setting test values\n",
    "W_l = np.random.randn(n_l, n_l_prev) * 0.01\n",
    "b_l = np.zeros((n_l, 1))\n",
    "A_l_prev = np.random.randn(n_l_prev, m)\n",
    "\n",
    "A_l, Z_l = forward_propagation(A_l_prev, W_l, b_l, relu)\n",
    "\n",
    "cache_l = (A_l_prev, W_l, Z_l)  \n",
    "\n",
    "print(\"> activation matrix of l's layer:\\n\", A_l, '\\n> shape(n_l, m):', A_l.shape)\n",
    "\n",
    "#  TEST CODE(continued from test code of forward propagation)\n",
    "dA_l = np.random.randn(n_l, m)  # test acvivation values\n",
    "\n",
    "backward_propagation(dA_l, *cache_l, relu_backward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
