{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://kaggle2.blob.core.windows.net/forum-message-attachments/53646/1539/fast_solution.py?sv=2012-02-12&se=2015-12-04T20%3A40%3A32Z&sr=b&sp=r&sig=qTDaOlHCMWaqBB9aOK6haM6Vo2FmmkfopqtwQaexnC0%3D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "                   Version 2, December 2004\n",
    "\n",
    "Copyright (C) 2004 Sam Hocevar <sam@hocevar.net>\n",
    "\n",
    "Everyone is permitted to copy and distribute verbatim or modified\n",
    "copies of this license document, and changing it is allowed as long\n",
    "as the name is changed.\n",
    "\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "  TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n",
    "\n",
    " 0. You just DO WHAT THE FUCK YOU WANT TO.\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "\n",
    "# parameters #################################################################\n",
    "#train = 'train.csv'  # path to training file\n",
    "#test = 'test.csv'  # path to testing file\n",
    "train = r'C:\\Users\\NYE\\Downloads\\dac.tar\\train.txt'\n",
    "test = r'C:\\Users\\NYE\\Downloads\\dac.tar\\test.txt'\n",
    "\n",
    "D = 2 ** 20   # number of weights use for learning\n",
    "alpha = .1    # learning rate for sgd optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function definitions #######################################################\n",
    "\n",
    "# A. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Apply hash trick of the original csv row\n",
    "# for simplicity, we treat both integer and categorical features as categorical\n",
    "# INPUT:\n",
    "#     csv_row: a csv dictionary, ex: {'Lable': '1', 'I1': '357', 'I2': '', ...}\n",
    "#     D: the max index that we can hash to\n",
    "# OUTPUT:\n",
    "#     x: a list of indices that its value is 1\n",
    "def get_x(csv_row, D):\n",
    "    x = [0]  # 0 is the index of the bias term\n",
    "    for key, value in csv_row.items():\n",
    "        index = int(value + key[1:], 16) % D  # weakest hash ever ;)\n",
    "        x.append(index)\n",
    "    return x  # x contains indices of features that have a value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def get_p(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D. Update given model\n",
    "# INPUT:\n",
    "#     w: weights\n",
    "#     n: a counter that counts the number of times we encounter a feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# OUTPUT:\n",
    "#     w: updated model\n",
    "#     n: updated count\n",
    "def update_w(w, n, x, p, y):\n",
    "    for i in x:\n",
    "        # alpha / (sqrt(n) + 1) is the adaptive learning rate heuristic\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1\n",
    "        w[i] -= (p - y) * alpha / (sqrt(n[i]) + 1.)\n",
    "        n[i] += 1.\n",
    "\n",
    "    return w, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-04 03:16:15.760836\tencountered: 1000000\tcurrent logloss: 0.473722\n",
      "2015-12-04 03:17:58.278966\tencountered: 2000000\tcurrent logloss: 0.468350\n",
      "2015-12-04 03:19:41.453531\tencountered: 3000000\tcurrent logloss: 0.466356\n",
      "2015-12-04 03:21:26.204966\tencountered: 4000000\tcurrent logloss: 0.465526\n",
      "2015-12-04 03:22:57.902531\tencountered: 5000000\tcurrent logloss: 0.464644\n",
      "2015-12-04 03:24:21.518360\tencountered: 6000000\tcurrent logloss: 0.463928\n",
      "2015-12-04 03:25:45.492840\tencountered: 7000000\tcurrent logloss: 0.463666\n",
      "2015-12-04 03:27:09.426853\tencountered: 8000000\tcurrent logloss: 0.463781\n",
      "2015-12-04 03:28:33.313838\tencountered: 9000000\tcurrent logloss: 0.463295\n",
      "2015-12-04 03:29:57.276867\tencountered: 10000000\tcurrent logloss: 0.463704\n",
      "2015-12-04 03:31:21.061795\tencountered: 11000000\tcurrent logloss: 0.463851\n",
      "2015-12-04 03:32:44.879741\tencountered: 12000000\tcurrent logloss: 0.463777\n",
      "2015-12-04 03:34:08.922518\tencountered: 13000000\tcurrent logloss: 0.463684\n",
      "2015-12-04 03:35:32.851917\tencountered: 14000000\tcurrent logloss: 0.464402\n",
      "2015-12-04 03:36:56.798900\tencountered: 15000000\tcurrent logloss: 0.464294\n",
      "2015-12-04 03:38:20.636408\tencountered: 16000000\tcurrent logloss: 0.464274\n",
      "2015-12-04 03:39:44.497379\tencountered: 17000000\tcurrent logloss: 0.464271\n",
      "2015-12-04 03:41:08.383364\tencountered: 18000000\tcurrent logloss: 0.464256\n",
      "2015-12-04 03:42:32.338389\tencountered: 19000000\tcurrent logloss: 0.464048\n",
      "2015-12-04 03:43:56.186352\tencountered: 20000000\tcurrent logloss: 0.464077\n",
      "2015-12-04 03:45:19.976283\tencountered: 21000000\tcurrent logloss: 0.464015\n",
      "2015-12-04 03:46:44.014795\tencountered: 22000000\tcurrent logloss: 0.463479\n",
      "2015-12-04 03:48:08.067876\tencountered: 23000000\tcurrent logloss: 0.463600\n",
      "2015-12-04 03:49:32.003398\tencountered: 24000000\tcurrent logloss: 0.463450\n",
      "2015-12-04 03:50:56.211701\tencountered: 25000000\tcurrent logloss: 0.463238\n",
      "2015-12-04 03:52:20.271785\tencountered: 26000000\tcurrent logloss: 0.463050\n",
      "2015-12-04 03:53:43.829582\tencountered: 27000000\tcurrent logloss: 0.463105\n",
      "2015-12-04 03:55:07.459421\tencountered: 28000000\tcurrent logloss: 0.463153\n",
      "2015-12-04 03:56:31.282370\tencountered: 29000000\tcurrent logloss: 0.463142\n",
      "2015-12-04 03:57:55.050289\tencountered: 30000000\tcurrent logloss: 0.463218\n",
      "2015-12-04 03:59:18.921265\tencountered: 31000000\tcurrent logloss: 0.463161\n",
      "2015-12-04 04:00:42.837268\tencountered: 32000000\tcurrent logloss: 0.463042\n",
      "2015-12-04 04:02:06.771280\tencountered: 33000000\tcurrent logloss: 0.462936\n",
      "2015-12-04 04:03:30.468157\tencountered: 34000000\tcurrent logloss: 0.463195\n",
      "2015-12-04 04:04:54.291107\tencountered: 35000000\tcurrent logloss: 0.463283\n",
      "2015-12-04 04:06:18.273147\tencountered: 36000000\tcurrent logloss: 0.463242\n",
      "2015-12-04 04:07:42.237177\tencountered: 37000000\tcurrent logloss: 0.463310\n",
      "2015-12-04 04:09:06.260240\tencountered: 38000000\tcurrent logloss: 0.463217\n",
      "2015-12-04 04:10:30.067180\tencountered: 39000000\tcurrent logloss: 0.462850\n",
      "2015-12-04 04:11:53.906138\tencountered: 40000000\tcurrent logloss: 0.462564\n",
      "2015-12-04 04:13:17.739094\tencountered: 41000000\tcurrent logloss: 0.462859\n",
      "2015-12-04 04:14:41.790172\tencountered: 42000000\tcurrent logloss: 0.462879\n",
      "2015-12-04 04:16:05.876272\tencountered: 43000000\tcurrent logloss: 0.463001\n",
      "2015-12-04 04:17:29.817289\tencountered: 44000000\tcurrent logloss: 0.463061\n",
      "2015-12-04 04:18:53.650243\tencountered: 45000000\tcurrent logloss: 0.463056\n"
     ]
    }
   ],
   "source": [
    "# training and testing #######################################################\n",
    "\n",
    "# initialize our model\n",
    "w = [0.] * D  # weights\n",
    "n = [0.] * D  # number of times we've encountered a feature\n",
    "\n",
    "# start training a logistic regression model using on pass sgd\n",
    "loss = 0.\n",
    "\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(train)\n",
    "fn = ['Label'] + [ 'I' + str(i) for i in list(range(1,14))] + [ 'C' + str(i) for i in list(range(1,27))]\n",
    "for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):   \n",
    "#for t, row in enumerate(DictReader(open(train))):\n",
    "    y = 1. if row['Label'] == '1' else 0.\n",
    "\n",
    "    del row['Label']  # can't let the model peek the answer\n",
    "    #del row['Id']  # we don't need the Id\n",
    "\n",
    "    # main training procedure\n",
    "    # step 1, get the hashed features\n",
    "    x = get_x(row, D)\n",
    "\n",
    "    # step 2, get prediction\n",
    "    p = get_p(x, w)\n",
    "\n",
    "    # for progress validation, useless for learning our model\n",
    "    loss += logloss(p, y)\n",
    "    if t % 1000000 == 0 and t > 1:\n",
    "        print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "            datetime.now(), t, loss/t))\n",
    "\n",
    "    # step 3, update model with answer\n",
    "    w, n = update_w(w, n, x, p, y)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing (build kaggle's submission file)\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(test)\n",
    "with open('submission1234.csv', 'w') as submission:\n",
    "    submission.write('Predicted\\n')\n",
    "    #for t, row in enumerate(DictReader(open(test))):\n",
    "    for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):\n",
    "        #Id = row['Id']\n",
    "        #del row['Id']\n",
    "        y = 1. if row['Label'] == '1' else 0.\n",
    "        del row['Label']  # can't let the model peek the answer\n",
    "    \n",
    "        x = get_x(row, D)\n",
    "        p = get_p(x, w)\n",
    "        submission.write('%f\\n' % (p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
