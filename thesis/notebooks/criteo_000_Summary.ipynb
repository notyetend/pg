{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write down stuffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "[1] Kevin P. Murphy, \"Machine Learning: A Probabilistic Perspective\", The MIT Press, 2012.  \n",
    "[2] Zoeter, \"Bayesian Generalized Linear Models in a Terabyte World\", IEEE Conference on Image and Signal Processing and Analysis, 2007, pp. 1-6.  \n",
    "[3] Zoeter, Heskes, \"Gaussian Quadrature Based Expectation Propagation\", BNAIC 2005  \n",
    "(BNAIC 2005 - Proceedings of the Seventeenth Belgium-Netherlands Conference on Artificial Intelligence, Brussels, Belgium, October 17-18, 2005; 01/2005)  \n",
    "[4] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, \"Numerical Recipes in C: The Art of Scientific Programming\", Cambridge University Press, 2nd edition, 1992\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related Topics\n",
    " - Generalized LInear Models(GLM)\n",
    " - Dynamic GLM\n",
    " \n",
    " - Expectation Propagation(EP)\n",
    " - Quadrature EP\n",
    " - tailored variants of quadrature EP\n",
    " - first forward pass of fixed point iteration algorithm\n",
    " - UKF(Unscented Kalman Filter)\n",
    " - one-step unscented Kalman filter = first forward pass in the quadrature EP fixed point iteration algorithm\n",
    " - Assumed Density Filter(ADF)\n",
    " - Gaussian quadrature integration\n",
    " - non-stationary domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.0. Generalized Linear Model**  \n",
    "\n",
    "The GLM consists of three component \n",
    "- A dstribution of dependent variable Y from the exponential family\n",
    "- A linear predicator, $\\eta = w^t x$\n",
    "- A link function $g(\\cdot)$ such that $E(Y) = \\mu = g^{-1}(\\eta)$\n",
    "\n",
    "and these three components are combined likes, the expected value, $\\mu$, of dependent variable $Y$ depends on the independent variable, $X$, through the link function, $g(\\cdot)$.\n",
    "$$E(Y) = \\mu = g^{-1}(w^t x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. GLM and Logistic Regression**\n",
    "\n",
    "Logistic Regression is the special case of GLM, whose distribution is binomial and link function is log-odds(logit), $ g(\\mu) = logit(\\mu) =log \\left(\\frac{\\mu}{1-\\mu}\\right)  $\n",
    "\n",
    "so final form is,\n",
    "\n",
    "$$\\begin{align} {}logit(E[Y])  &= logit(P(Y=1))  \\\\&= logit(\\pi(x))  \\\\  &= log \\left( \\frac{\\pi(x)}{1-\\pi(x)}\\right)  \\end{align} \\\\ = w^t x$$\n",
    "\n",
    "$$Y \\sim Binomial(g^{-1}(w^t x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g^{-1}(w^t x) = \\pi^y(1-\\pi)^{(1-y)}$$\n",
    "$$\\pi = P(y=1) = \\frac{1}{1+exp(-w^tx)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.? Gaussian Process(GP)**\n",
    "\n",
    "> Refers to  \n",
    "- [http://www.slideshare.net/JungkyuLee1/gaussian-processes]\n",
    "- Murphy Ch15.\n",
    "\n",
    "\n",
    "$$\\begin{align}P(f|y,X) &= \\frac{P(f, y, X)}{P(y,X)}  \\\\ &= \\frac{P(f, y |X)P(X)}{P(y,X)}  \\\\ &= \\frac{P(f, y|X)}{P(y|X)}   \\\\ &\\propto P(f, y|X)  \\end{align} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.?. Markov Model**\n",
    "\n",
    "순차적인 관측치, $X_1, X_2, ..., X_T$가 있을때, t시점에서의 regression weight vector를 $\\boldsymbol{w}_t$라고 하고, $w_{t,i}$를 t시점의 i번째 regression weight이라고 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.? Hidden Markov Model(HMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 State space model(SSM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. Dynamic Linear Model**\n",
    "\n",
    "> Refers to [P636, Kevin P. Murphy, Machine Learning]\n",
    "\n",
    "\n",
    "If we do let the parameters change over time, we get a so-called dynamic linear model(Harvey 1990; West and Harrison 1997; Petris et al. 2009).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Assumed Density Filtering(ADF)**\n",
    "\n",
    "Unscented Kalman Filter는 Extended Kalman Filter의 새로운 버전이라고 할 수 있다. EKF에서 Gaussian function의 linear approximation을 하는 대신, 선택된 점들(**siama points**)에 대하여 transform을 수행하고, 이 값을 토대로 근사 분포를 구하게 된다.\n",
    "\n",
    "Assumed Density Filter(ADF)에서는 사후분포를 Gaussian과 같은 특정 분포로 근사하는 방법으로서, predict-update-project 과정을 반복한다.(Maybeck 1979).\n",
    "\n",
    "\n",
    "- An approximate **prior**: $$q_{t-1}(\\theta_{t-1}) \\approx p(\\theta_{t-1}|y_{1:t-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The one step ahead **predictive distribution**:\n",
    "$$q_{t|t-1}(\\theta_t) = \\int p(\\theta_t | \\theta_{t-1}) q_{t-1}(\\theta_{t-1}) d\\theta_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The approximate **posterior**:\n",
    "$$\\hat{p}(\\theta_t) = \\frac{1}{Z_t}p(y_t | \\theta_t)q_{t|t-1}(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The normalization constant \n",
    "$$Z_t = \\int p(\\theta_t | \\theta_{t-1})q_{t-1}(\\theta_{t-1})d\\theta_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우 prior가 tactable한 분포라 하더라도, posterior는 더 이상 tactable하지 않은 경우가 많다. 이 경우 $\\hat{p}(\\theta_t)$에 대한 tactable한  근사 분포를 구할 필요가 있다. 여기에서는 아래와 같은 조건의 근사 분포를 구한다.\n",
    "- Projected approximate posterior\n",
    "$$q(\\theta_t) = \\arg\\min_{q \\in Q} \\mathrm{KL}(\\hat{p}(\\theta_t || q(\\theta_t)) $$\n",
    "\n",
    "만약 $q$가 exponential family라면 KL minimization을 moment matching만으로 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉 위의 과정은 prior과 predictive distribution을 이용하여 posterior을 구성하고 이의 근사식인 projected approximate posterior를 구하는 것이다.\n",
    "$$q_{t-1}(\\theta_{t-1}) \\rightarrow q_{t|t-1}(\\theta_t) \\rightarrow \\hat{p}(\\theta_t) \\rightarrow q(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Gaussian approximation for online inference in GLMs**\n",
    "\n",
    "2.1.1. Predictive distribution of $s_t$, $q_{t|t-1}(s_t)$  \n",
    "Let's define deterministic quantity, sum node, $s_t={\\theta_t}^T x_t$. If one-step-ahead-predictive density of $\\theta_t$, $q_{t|t-1}(\\theta_t)$, is $\\prod_i N(\\theta_{t,i};\\mu_{t|t-1,i},\\sigma^2_{t|t-1,i})$, then predictive distribution of $s_t$, $q_{t|t-1}(s_t)$, is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   q_{t|t-1}(s_{t}) &\\equiv N(s_t;m_{t|t-1}, {v}_{t|t-1})\n",
    "\\\\ m_{t|t-1} &= \\sum^N_{i=1}x_{t,i}\\mu_{t|t-1,i}\n",
    "\\\\ {v}_{t|t-1} &= \\sum^N_{i=1}x^2_{t,i}{\\sigma}^2_{t|t-1,i}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2. Posterior distribution of $s_t$, $q_t(s_t)$\n",
    "$$\n",
    "\\begin{align}\n",
    "q_t(s_t) &\\equiv N(s_t; m_t, v_t)\n",
    "\\\\ m_t &= \\int s_t \\frac{1}{z_t} f(y_t|s_t) q_{t|t-1}(s_t)ds_t\n",
    "\\\\ v_t &= \\int s^2_t \\frac{1}{z_t} f(y_t|s_t) q_{t|t-1}(s_t) ds_t - m_t^2\n",
    "\\\\ z_t &= \\int f(y_t|s_t) q_{t|t-1}(s_t)ds_t\n",
    "\\\\ f(y_t|s_t) &\\equiv Ber(y_t;\\pi = sigmoid(s_t))\n",
    "\\\\ & = \\pi^{y_t} (1-\\pi)^{(1-y_t)}, \\quad y_t \\in \\{0,1\\}\n",
    "\\\\ & = \\left(\\frac{1}{1+exp(-s_t)}\\right)^{y_t} \\left(\\frac{exp(-s_t)}{1+exp(-s_t)}\\right)^{(1-y_t)}, \\quad y_t \\in \\{0,1\\}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3. Numerical approximation of $q_t(s_t)$  \n",
    "In the one-step UKF, the integrals above are numerically approximated using Gaussian quadrature[ref-2].\n",
    "\n",
    "By the Gaussian quadrature formulas, integral of \"polynomials times some known function $W(x)$\" is approximated by the weighted sum of functional values.\n",
    "[ref-4, p147]\n",
    "\n",
    "$$\\int^b_a W(x)f(x)dx \\approx \\sum^N_{j=1}w_j f(x_j)$$\n",
    "\n",
    "\n",
    "Particularly, when we approximate the value of integral of the form likes, $\\int^{+\\infty}_{-\\infty}e^{-x^2}f(x)dx$, it is convinent to use Gauss–Hermite quadrature.[wiki - Gauss–Hermite quadrature]\n",
    "\n",
    "In this case, weight function $W(x)$ is $e^{-x^2}, (-\\infty < x < \\infty)$ and $H_{j+1} = 2xH_j - 2jH_{j-1}$\n",
    "\n",
    "\n",
    "> Refers to \n",
    "> - [http://scicomp.stackexchange.com/questions/14667/is-there-a-gauss-laguerre-integration-routine-in-python]\n",
    "> - [http://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.hermite.hermgauss.html#numpy.polynomial.hermite.hermgauss]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can use pre-calculated gauss-hermite points and weights table. In this paper, we will use numpy's \"polynomial.hermite.hermgauss\" library to get sample points and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0000000000000004"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "xxi, wwi = np.polynomial.hermite.hermgauss(90)\n",
    "\n",
    "s_t_v = 1\n",
    "s_t_m = 1\n",
    "\n",
    "wi = wwi / np.sqrt(np.pi)\n",
    "xi = xxi * np.sqrt(2) * s_t_v + s_t_m\n",
    "\n",
    "sum((2*xi + 1) * wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\chi'$ and $\\omega'$ are sample points and weights for Gauss-Hermite, respectively, we can change variable as $\\chi=\\chi'\\sqrt{2}\\sigma_{s_t}+\\mu_{s_t}$ and $\\omega_i = \\frac{\\omega'}{\\sqrt{\\pi}}$ to compute integrals inclusing Gaussian distribution.  So integrals we have approximated as follows:\n",
    "$$\n",
    "\\int^{+\\infty}_{-\\infty} h(s_t) N(s_t; m_t, v_t) = \\sum_{i}h(\\chi_i)\\omega_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this formula, integrals involved in computing the posterior distribution of $s_t$, $q_t(s_t)$ are approximated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_t(s_t) &= N(s_t; \\tilde{m}_t, \\tilde{v}_t)\n",
    "\\\\ \\tilde{m}_t &= \\frac{1}{\\tilde{z}_t} \\sum_i \\chi_i f(y_t; \\chi_i ) \\omega_i\n",
    "\\\\ \\tilde{v}_t &= \\frac{1}{\\tilde{z}_t} \\sum_i \\chi^2_i f(y_t; \\chi_i ) \\omega_i - \\tilde{m}^2_t\n",
    "\\\\ \\tilde{z}_t &= \\sum_i f(y_t; \\chi_i ) \\omega_i\n",
    "\\\\ f(y_t;\\chi_i) &= \\left(\\frac{1}{1+exp(-\\chi_i)}\\right)^{y_t} \\left(\\frac{exp(-\\chi_i)}{1+exp(-\\chi_i)}\\right)^{(1-y_t)} \\quad \\mathrm{for}  \\ \\ y_t \\in \\{0,1\\}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have infered posterior distribution of $s_t$, $q_t(s_t)$, it's time we think of approximated posterior distribution of $\\theta$, $q(\\theta_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can define $\\delta_m$ and $\\delta_v$ as change in $m_t$ and $v_t$ after updating the distribution of $s_t$.\n",
    "$$m_t = m_{t|t-1} + \\delta_{m}$$\n",
    "$$v_t = v_{t|t-1} + \\delta_{v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these $\\delta$, parameters of factored posterior distribution can be demonstrated to be as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   q(\\theta_t,i) &\\sim N(\\theta_{t,i};\\mu_{t,i}, \\sigma^2_{t,i})\n",
    "\\\\ \\mu_{t,i} &= \\mu_{t|t-1,i} + a_i \\delta_m\n",
    "\\\\ \\sigma^2_{t,i} &= \\sigma^2_{t|t-1,i} + a^2_i \\delta_v\n",
    "\\\\ a_i &\\triangleq \\frac{x_{t,i}\\sigma^2_{t|t-1,i}}{\\sum_j x^2_{t,j}\\sigma^2_{t|t-1,i}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "($\\triangleq$: equal by definition.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
