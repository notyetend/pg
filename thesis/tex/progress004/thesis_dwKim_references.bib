@article{Opper1999,
author = {Opper, Manfred},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper - 1999 - A Bayesian approach to on-line learning.pdf:pdf},
journal = {On-line learning in neural networks},
pages = {363----378},
title = {{A Bayesian approach to on-line learning}},
url = {http://books.google.com/books?hl=en{\&}amp;lr={\&}amp;id={\_}w-LFSwUaFIC{\&}amp;oi=fnd{\&}amp;pg=PA363{\&}amp;dq=A+Bayesian+Approach+to+Online+Learning{\&}amp;ots=eSeMhpUua6{\&}amp;sig=Ecc-dA5tLR6qdpwlMdhRs1YfQ00},
year = {1999}
}
@article{McMahan2013,
abstract = {Predicting ad click--through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v2},
author = {McMahan, H Brendan and Holt, Gary and Sculley, D and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
doi = {10.1145/2487575.2488200},
eprint = {arXiv:1301.3781v2},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/McMahan et al. - 2013 - Ad click prediction a view from the trenches.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {data mining,large-scale learning,online advertising},
pages = {1222--1230},
title = {{Ad click prediction: a view from the trenches}},
year = {2013}
}
@article{Zoeter2007,
author = {Zoeter, Onno},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Zoeter - 2007 - Bayesian Generalized Linear Models in a Terabyte World.pdf:pdf},
title = {{Bayesian Generalized Linear Models in a Terabyte World.}},
year = {2007}
}
@article{Minka2013,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P},
eprint = {1301.2294},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Minka - 2013 - Expectation Propagation for approximate Bayesian inference.pdf:pdf},
isbn = {1-55860-800-1},
journal = {Statistics},
month = {jan},
number = {2},
pages = {362--369},
title = {{Expectation Propagation for approximate Bayesian inference}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1319{\&}amp;rep=rep1{\&}amp;type=pdf http://arxiv.org/abs/1301.2294},
volume = {17},
year = {2013}
}
@misc{Lauritzen1992,
abstract = {A scheme is presented for modeling and local computation of exact probabilities, means, and variances for mixed qualitative and quantitative variables. The models assume that the conditional distribution of the quantitative variables, given the qualitative, is multivariate Gaussian. The computational architecture is set up by forming a tree of belief universes, and the calculations are then performed by local message passing between universes. The asymmetry between the quantitative and qualitative variables sets some additional limitations for the specification and propagation structure. Approximate methods when these are not appropriately fulfilled are sketched. It has earlier been shown how to exploit the local structure in the specification of a discrete probability model for fast and efficient computation, thereby paving the way for exploiting probability-based models as parts of realistic systems for planning and decision support. The purpose of this article is to extend this computational scheme to networks, where some vertices represent entities that are measured on a quantitative and some on a qualitative scale. An extension has the advantage of unifying several known techniques, but allows more flexible and faithful modeling and speeds computation as well. To handle this more general case, the properties of (CG) conditional Gaussian distributions are exploited. A fictitious but simple example is used for illustration throughout the paper, concerned with monitoring emissions from a waste incinerator. From optical measurements of the darkness of the smoke, the concentration of CO2—which are both on a continuous scale—and possible knowledge about qualitative characteristics such as the type of waste burned, one wants to infer about the state of the incinerator and the current emission of heavy metals.},
author = {Lauritzen, Steffen L. (University of Aalborg)},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1992.10476265},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Lauritzen - 1992 - Propagation of Probabilities, Means and Variances in Mixed Graphical Association Models.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
number = {420},
pages = {1098--1108},
title = {{Propagation of Probabilities, Means and Variances in Mixed Graphical Association Models}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1992.10476265},
volume = {87},
year = {1992}
}
@techreport{Chapelle2013,
abstract = {Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this paper a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: it is easy to implement and deploy; it is highly scalable (we have trained it on terabytes of data); and it provides models with state-of-the-art accuracy.},
archivePrefix = {arXiv},
arxivId = {1005.3014},
author = {Chapelle, Olivier and Manavoglu, Eren and Rosales, Romer},
booktitle = {people.csail.mit.edu},
doi = {10.1145/0000000.0000000},
eprint = {1005.3014},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley//Chapelle, Manavoglu, Rosales - 2013 - Simple and scalable response prediction for display advertising.pdf:pdf},
isbn = {9781627480031},
issn = {15564681},
number = {212},
pages = {1--34},
title = {{Simple and scalable response prediction for display advertising}},
url = {http://people.csail.mit.edu/romer/papers/TISTRespPredAds.pdf},
volume = {V},
year = {2013}
}
@article{Herbrich2006,
abstract = {We present a new Bayesian skill rating system which can be viewed as a generalisation of the Elo system used in Chess. The new system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. Inference is performed by approximate message passing on a factor graph representation of the model. We present experimental evidence on the increased accuracy and convergence speed of the system compared to Elo and report on our experience with the new rating system running in a large-scale commercial online gaming service under the name of TrueSkill.},
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
doi = {10.2134/jeq2007.0177},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.pdf:pdf},
isbn = {1049-5258},
issn = {00472425},
journal = {Advances in Neural Information Processing Systems},
keywords = {bayesian learning,dynamic difficulty adjustment,match-making},
pages = {569--576},
pmid = {18268290},
title = {{TrueSkill: A Bayesian Skill Rating System}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=67956},
year = {2006}
}
@article{Minka2001,
abstract = {One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, Expectation Propagation, unites and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unication shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction|propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation,to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classiers that is faster and more accurate than any previously known. The resulting classiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classication, via Bayesian model selection.},
author = {Minka, Thomas P},
doi = {10.1016/j.conb.2011.12.004},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Minka - 2001 - A family of algorithms for approximate Bayesian inference.pdf:pdf},
issn = {09594388},
journal = {PhD Thesis},
pmid = {2688543},
title = {{A family of algorithms for approximate Bayesian inference}},
url = {papers2://publication/uuid/37D3C7DD-C308-4279-86AC-52057DE5CB29},
year = {2001}
}
@article{Opper1997,
author = {Opper, Manfred and Winther, Ole},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper, Winther - 1997 - A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 9},
pages = {225--231},
title = {{A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks}},
url = {http://papers.nips.cc/paper/1268-a-mean-field-algorithm-for-bayes-learning-in-large-feed-forward-neural-networks.pdf$\backslash$nfiles/998/Opper ? Winther - 1997 - A Mean Field Algorithm for Bayes Learning in Large.pdf$\backslash$nfiles/999/1268-a-mean-field-algorithm-for-baye},
year = {1997}
}
@article{Salas2015,
abstract = {Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.},
archivePrefix = {arXiv},
arxivId = {1509.02438},
author = {Salas, Arnold and Roberts, Stephen J. and Osborne, Michael A.},
eprint = {1509.02438},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Salas, Roberts, Osborne - 2015 - A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression.pdf:pdf},
month = {sep},
pages = {1--9},
title = {{A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression}},
url = {http://arxiv.org/abs/1509.02438},
year = {2015}
}
@book{Agresti1996,
author = {Agresti, Alan},
edition = {2nd},
publisher = {Wiley},
title = {{An introduction to categorical data analysis}},
year = {1996}
}
@article{Zhang2010,
abstract = {Many real world applications employ multi-variate performance measures and each example can belong to multiple classes. The currently most popular approaches train an SVM for each class, followed by ad hoc thresholding. Probabilistic models using Bayesian decision theory are also commonly adopted. In this paper, we propose a Bayesian online multi-label classification framework (BOMC) which learns a probabilistic linear classifier. The likelihood is modeled by a graphical model similar to TrueSkillTM, and inference is based on Gaussian density fil- tering with expectation propagation. Us- ing samples from the posterior, we label the testing data by maximizing the expected F1-score. Our experiments on Reuters1-v2 dataset show BOMC compares favorably to the state-of-the-art online learners in macro- averaged F1-score and training time.},
author = {Zhang, Xinhua and Graepel, Thore and Herbich, Ralf},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Zhang, Graepel, Herbich - 2010 - Bayesian Online Learning for Multi-label and Multi-variate Performance Measures.pdf:pdf},
issn = {15324435},
journal = {Thirteenth Conference on Artificial Intelligence and Statistics AISTATS 2010},
pages = {956--963},
title = {{Bayesian Online Learning for Multi-label and Multi-variate Performance Measures}},
volume = {9},
year = {2010}
}
@article{Luttinen2014,
abstract = {BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.},
archivePrefix = {arXiv},
arxivId = {1410.0870},
author = {Luttinen, Jaakko},
eprint = {1410.0870},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Luttinen - 2014 - BayesPy Variational Bayesian Inference in Python.pdf:pdf},
journal = {arXiv preprint arXiv:1410.0870},
keywords = {probabilistic pro-,python,variational bayes,variational message passing},
month = {oct},
title = {{BayesPy: Variational Bayesian Inference in Python}},
url = {http://arxiv.org/abs/1410.0870},
year = {2014}
}
@article{Chan2013,
author = {Chan, Hyo and Lee, Hangsuck},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Chan, Lee - 2013 - Estimation of the Expected Loss per Exposure of Export Insurance using GLM.pdf:pdf},
keywords = {export insurance,generalized linear models,glm,loss frequency,loss severity},
pages = {857--871},
title = {{Estimation of the Expected Loss per Exposure of Export Insurance using GLM}},
volume = {26},
year = {2013}
}
@article{Opper2005,
abstract = {We propose a novel framework for approximations to intractable probabilisticmodels which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode different features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Significant improvements are obtained when a spanning tree is used instead.},
author = {Opper, Manfred and Winther, Ole},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper, Winther - 2005 - Expectation Consistent Approximate Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {2177--2204},
title = {{Expectation Consistent Approximate Inference}},
volume = {6},
year = {2005}
}
@article{Zoeter2005,
abstract = {Later},
author = {Zoeter, Onno and Heskes, Tom},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Zoeter, Heskes - 2005 - Gaussian quadrature based expectation propagation.pdf:pdf},
isbn = {097273581X},
journal = {Workshop on Artificial Intelligence and Statistics},
keywords = {theory {\&} algorithms},
pages = {445--452},
title = {{Gaussian quadrature based expectation propagation.}},
url = {http://eprints.pascal-network.org/archive/00000562/},
volume = {10},
year = {2005}
}
@book{Murphy2012,
author = {Murphy, Kevin P.},
publisher = {The MIT Press},
title = {{Machine learning}},
year = {2012}
}
@article{Shi2013,
author = {Shi, Tianlin and Zhu, Jun},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Shi, Zhu - 2013 - Online {\{}Bayes{\}}ian Passive-Aggressive Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1312.3388},
title = {{Online {\{}Bayes{\}}ian Passive-Aggressive Learning}},
url = {http://arxiv.org/abs/1312.3388},
volume = {32},
year = {2013}
}
@article{Hoffman2010,
abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time. 1},
author = {Hoffman, MD and Blei, DM and Bach, Francis},
doi = {10.1.1.187.1883},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Hoffman, Blei, Bach - 2010 - Online learning for latent dirichlet allocation.pdf:pdf},
isbn = {9781450300551},
issn = {08912017},
journal = {Nips},
pages = {1--9},
title = {{Online learning for latent dirichlet allocation}},
url = {http://videolectures.net/site/normal{\_}dl/tag=83534/nips2010{\_}1291.pdf},
year = {2010}
}
@article{Wenta2012,
abstract = {Online algorithms allow data instances to be processed in a sequential way, which is important for large-scale and real-time applications. In this paper, we propose a novel online clustering approach based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions, which can be considered as an extension of the finite GD mixture model to the infinite case. Our approach is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. Moreover, an unsupervised localized feature selection scheme is integrated with the proposed nonparametric framework to improve the clustering performance. By learning the proposed model in an online manner using a variational approach, all the involved parameters and features saliencies are estimated simultaneously and effectively in closed forms. The proposed online infinite mixture model is validated through both synthetic data sets and two challenging real-world applications namely text document clustering and online human face detection. © 2012 W. Fan {\&} N. Bouguila.},
author = {Wenta, Wentao Fan and Bouguila, Nizar},
doi = {10.1016/j.patcog.2013.03.026},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Wenta, Bouguila - 2012 - Online learning of a dirichlet process mixture of generalized dirichlet distributions for simultaneous clusteri.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Clustering,Dirichlet process,Generalized dirichlet mixtures,Localized feature selection,Nonparametric bayesian,Online learning,Variational bayes},
pages = {113--128},
title = {{Online learning of a dirichlet process mixture of generalized dirichlet distributions for simultaneous clustering and localized feature selection}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84876887338{\&}partnerID=tZOtx3y1},
volume = {25},
year = {2012}
}
@article{Lin2013,
abstract = {Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm – random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency – orders of magnitude speed-up compared to the state-of-the-art.},
author = {Lin, Dahua},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Lin - 2013 - Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation.pdf:pdf},
issn = {10495258},
journal = {Advances in Nueral Information Processing Systems 26 (Proceedings of NIPS)},
pages = {1--9},
title = {{Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation}},
year = {2013}
}
@article{Crammer2006,
abstract = {We present a unified view for {\{}$\backslash$em online{\}} classification, regression, and uniclass problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss.},
author = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
doi = {10.1.1.9.3429},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Crammer et al. - 2006 - Online Passive-Aggressive Algorithms(2).pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {551--585},
title = {{Online Passive-Aggressive Algorithms}},
url = {http://eprints.pascal-network.org/archive/00000052/},
volume = {7},
year = {2006}
}
@article{Ghahramani2000,
author = {Ghahramani, Zoubin and Computational, Gatsby and Unit, Neuroscience},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Ghahramani, Computational, Unit - 2000 - Online Variational Bayesian Learning.pdf:pdf},
journal = {Neuroscience},
number = {December},
title = {{Online Variational Bayesian Learning}},
year = {2000}
}
@article{Box2003,
author = {Box, P O and Hut, Fin- and Honkela, Antti and Valpola, Harri},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Box et al. - 2003 - ON-LINE VARIATIONAL BAYESIAN LEARNING Antti Honkela and Harri Valpola Helsinki University of Technology , Neural Net.pdf:pdf},
number = {April},
pages = {803--808},
title = {{ON-LINE VARIATIONAL BAYESIAN LEARNING Antti Honkela and Harri Valpola Helsinki University of Technology , Neural Networks Research Centre}},
year = {2003}
}
@article{Wang2011,
abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a poten- tially infinite number of components. It has been applied widely in probabilistic topic modeling, where the data are documents and the ...},
author = {Wang, Chong and Blei, David M},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Wang, Blei - 2011 - Online Variational Inference for the Hierarchical Dirichlet Process.pdf:pdf},
isbn = {1609258177},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {752--760},
title = {{Online Variational Inference for the Hierarchical Dirichlet Process}},
url = {http://www.cs.cmu.edu/{~}chongw/papers/WangPaisleyBlei2011.pdf},
volume = {15},
year = {2011}
}
@article{Opper1996,
author = {Opper, Manfred},
doi = {10.1103/PhysRevLett.77.4671},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper - 1996 - On-line versus Off-line Learning from Random Examples General Results.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
pages = {4671--4674},
pmid = {10062597},
title = {{On-line versus Off-line Learning from Random Examples: General Results}},
volume = {77},
year = {1996}
}
@article{Winther1998,
abstract = {. In a Bayesian approach to online learning a simple parametric approximate posterior over rules is updated in each online learning step. Predictions on new data are derived from averages over this posterior. This should be compared to the Bayes optimal batch (or offline) approach for which the posterior is calculated from the prior and the likelihood of the whole training set. We suggest that minimizing the difference between the batch and the approximate posterior will optimize the performance of the Bayes online algorithm. This general principle is demonstrated for three scenarios: learning a linear perceptron rule and a binary classification rule in the simple perceptron with binary /continuous weight prior. 1},
author = {Winther, Ole and Solla, SA},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Winther, Solla - 1998 - Optimal Bayesian online learning.pdf:pdf},
journal = {Theoretical Aspects of Neural Computation (TANC- {\ldots}},
number = {section 4},
title = {{Optimal Bayesian online learning}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Optimal+Bayesian+online+learning{\#}0},
year = {1998}
}
@article{Langford2009,
abstract = {We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular {\$}L{\_}1{\$}-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.},
archivePrefix = {arXiv},
arxivId = {0806.4686},
author = {Langford, John and Li, Lihong and Zhang, Tong},
eprint = {0806.4686},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Langford, Li, Zhang - 2009 - Sparse Online Learning via Truncated Gradient.pdf:pdf},
isbn = {9781605609492},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {online learning,regulariza-,sparsity,stochastic gradient descent,truncated gradient},
pages = {777--801},
title = {{Sparse Online Learning via Truncated Gradient}},
url = {http://arxiv.org/abs/0806.4686},
volume = {10},
year = {2009}
}
@article{Pakin2002,
author = {Pakin, Scott},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Pakin - 2002 - T X Symbol List The Comprehensive L E.pdf:pdf},
number = {October},
pages = {1--82},
title = {{T X Symbol List The Comprehensive L E}},
year = {2002}
}
