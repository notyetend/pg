{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 Neural Networks for Machine Learning 강의를 정리한 내용임.   \n",
    "2016.11.14. by Dongwan Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week5. Object recognition with neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture5의 강의 내용은 skip하고, Week5에 포함된 PA2를 살펴보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Week4에서 등장했던 next word predition 문제인데, 3개의 단어 다음의 단어를 예측하는 문제이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 예에서는 word dictionary($\\textrm{d} \\in \\mathbb{R}^{250}$)의 크기를 250으로 제한하고, 단어 대신 단어의 index를 이용하여 데이터를 표현한다. 예를들어 data sample 하나가 (1, 3, 249, 250)라면 이는 (all, just, the, left)를 나타낸다.\n",
    "\n",
    "> $$\\mathbf{d}= \\begin{pmatrix}\n",
    "1:all\\\\\n",
    "2:set\\\\\n",
    "3:just\\\\\n",
    "\\vdots\\\\\n",
    "248:five\\\\\n",
    "249:the\\\\\n",
    "250:left\\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사용되는 데이터는 Training input data($\\mathbf{X \\in \\mathbb{R}^{3 \\times 372500}}$)는 3개 단어 372500쌍, Training target data($\\mathbf{y} \\in \\mathbb{R}^{372500}$)는 372500개 단어이고, Validation data는 46568쌍, Test data는 46568쌍을 사용한다.($n=372500$)\n",
    "\n",
    "> $$\\begin{align}\n",
    "\\mathbf{X} &=\n",
    "\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3}\\\\\n",
    "x_{3,1} & x_{3,2} & x_{3,3}\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "x_{n,1} & x_{n,2} & x_{n,3}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "28  & 26 & 90 \\\\\n",
    "184 & 44 & 249 \\\\\n",
    "183 & 32 & 76 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "76 & 123 & 224\n",
    "\\end{pmatrix}\\\\\n",
    "\\mathbf{y} &=\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_{n}\n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "144 \\\\\n",
    "117 \\\\\n",
    "122 \\\\\n",
    "\\vdots\\\\\n",
    "144\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> input layer에는 3개의 단어가 들어오고, 이들은 각각 Word Embedding 1, 2, 3으로 연결된다. word embedding하나는 50 개의 unit으로 구성되어 있는데, 이것은 50 차원의 word space라고 생각하면 된다. 즉 단어하나가 50 차원 상의 점 하나라고 할 수 있고, 학습이 진행된 후에는 어떤 유사성을 갖는 단어들은 그룹을 형성하여 위치하게 된다.\n",
    "\n",
    "> word dictionary의 총 단어 수가 250개 이므로, 첫 입력 단어(Index of word 1)와 Word embedding1 사이의 weight인 word_embedding_weights는 $\\mathbf{P} \\in \\mathbb{R}^{250 \\times 50}$이다. $\\mathbf{P}$의 $k$번째 row는 word dictionary의 $k$번째 단어($\\mathbf{d}_k$)에 대한 $\\mathbb{R}^{50}$ 상의\n",
    "위치($\\mathbf{p}_k$)라고 할 수 있다. \n",
    "\n",
    ">$$\\mathbf{P} = \\begin{pmatrix} \n",
    "\\mathbf{p}^T_1\\\\\n",
    "\\mathbf{p}^T_2\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{p}^T_{250}\\\\\n",
    "\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 따라서 embedding_layer_state(activation1), $\\mathbf{A_1} \\in \\mathbb{R}^{150 \\times n}$은 아래와 같다. (여기에서 150은 50x3이고, $n$은 number of samples 또는 batch size이고, $\\mathbf{p}_i \\in \\mathbb{R}^{50}$이다.)\n",
    "$$\\mathbf{A_1} = \\begin{pmatrix}\n",
    "\\mathbf{p}_{28} & \\mathbf{p}_{184} & \\mathbf{p}_{183} & \\cdots & \\mathbf{p}_{242} \\\\\n",
    "\\mathbf{p}_{26} & \\mathbf{p}_{44} & \\mathbf{p}_{32} & \\cdots & \\mathbf{p}_{32} \\\\\n",
    "\\mathbf{p}_{90} & \\mathbf{p}_{249} & \\mathbf{p}_{76} & \\cdots & \\mathbf{p}_{223} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다음으로 hidden layer는 200개(numhid2) unit으로 구성되어 있는데, word embedding layer(1, 2, 3)과 hidden layer를 연결하는 weight(embed_to_hid_weights)은 $\\mathbf{Q} \\in \\mathbb{R}^{150 \\times 200}$이다. (여기서 150은 word embedding(50 unit)이 3개 있기 때문임)\n",
    "\n",
    "> 따라서 inputs_to_hidden_units, $\\mathbf{Z_2} \\in \\mathbb{R}^{200 \\times n}$는 아래와 같다.($bias_{1} \\in \\mathbb{R}^{200 \\times n}$)\n",
    "$$\\mathbf{Z_2} = \\mathbf{Q}^T \\mathbf{A_1} + bias_{1}$$\n",
    "\n",
    "> 결국 hidden_layer_state, $\\mathbf{A_2} \\in \\mathbb{R}^{200 \\times n}$은 아래와 같다.\n",
    "$$A_2 = \\frac{1}{1 + exp(-\\mathbf{Z_2})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다음으로 hidden layer와 output layer를 연결하는 weight(hid_to_output_weights)은 $\\mathbf{R \\in \\mathbb{R}^{200 \\times 250}}$이고, inputs_to_softmax $\\mathbf{Z_3}\\in \\mathbb{R}^{250 \\times n}$는 아래와 같다.($bias_{2} \\in \\mathbb{R}^{250 \\times n}$)\n",
    "$$\\mathbf{Z_3} = \\mathbf{R}^T \\mathbf{A_2} + bias_2$$\n",
    "\n",
    "> 마지막으로 output_layer_state, $\\mathbf{A_3} \\in \\mathbb{R}^{250 \\times n}$은 $\\mathbf{Z_3}$의 각 column을 $softmax$에 넣은 출력들이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{A_3} = \\begin{pmatrix}\n",
    "\\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_i} & \\cdots & \\mathbf{a_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$ \\mathbf{a_i} = \\begin{pmatrix}\n",
    "a_{i, 1}\\\\\n",
    "a_{i, 2}\\\\\n",
    "a_{i, 3}\\\\\n",
    "\\vdots\\\\\n",
    "a_{i, j}\\\\\n",
    "\\vdots\\\\\n",
    "a_{i, 250}\\\\\n",
    "\\end{pmatrix}~, ~~ \\sum_{j} a_{i,j} = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이렇게 forward-propagation의 결과($\\mathbf{A_3}$)를 구했고, 이제 cost를 구해보도록 하자. 이를 위해 training data의 target value를 'sparse 1-of-K vector'로 바꿔보자.\n",
    "\n",
    "> 예를들어 첫번째 training data input($x_{1,1}, x_{1,2}, x_{1,3}$)이 (28, 26, 90)일때 target data($y_1$)는 144인데, (첫번째 training data는 'going' 'to' 'be' 다음에 .(컴마)가 등장)     144의 'sparse 1-of-K vector'는 $\\mathbf{s}_{y_1} = \\mathbf{s}_{144} = [I_1, I_2, \\cdots, I_j, \\cdots, I_{250}]^T, I_j = \\begin{cases} 1 & j = 144 \\\\ 0 & j \\neq 144\\end{cases}$이므로\n",
    "\n",
    "> target data에 대한 sparse 1-of-K matrix는 아래와 같다.\n",
    "$$\\mathbf{S} = \\begin{pmatrix}\n",
    "\\mathbf{s}_{y_1} & \\mathbf{s}_{y_2} & \\mathbf{s}_{y_3} & \\cdots & \\mathbf{s}_{y_n}\n",
    "\\end{pmatrix} ~\n",
    ", ~ ~ \\mathbf{y} =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_{n}\n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "144 \\\\\n",
    "117 \\\\\n",
    "122 \\\\\n",
    "\\vdots\\\\\n",
    "144\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> cross-entropy의 input $z_i$에 대한 편미분은 아래와 같기 때문에.\n",
    "$$\\frac{\\partial C}{\\partial z_i} = \\sum_{j} \\frac{\\partial C}{\\partial y_j} \\frac{\\partial y_j}{\\partial z_i} = y_i - t_i$$\n",
    "\n",
    "> 'error_deriv'는 아래와 같다.($\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_3}} \\in \\mathbb{R}^{250 \\times n}$)\n",
    "$$\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_3}} = \\mathbf{A_3} - \\mathbf{S}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다음으로 hidden layer to output weight(hid_to_output_weight), $\\mathbf{R}$이 cost, $\\mathbf{C}$에 주는 영향(hid_to_output_weights_gradient)은 아래와 같다.($\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{R}} \\in \\mathbb{R}^{200 \\times 250}$, $\\frac{\\partial \\mathbf{Z_3}}{\\partial \\mathbf{R}} = \\mathbf{A_2}$)\n",
    "> $$\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{R}} = \\frac{\\partial \\mathbf{Z_3}}{\\partial \\mathbf{R}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_3}} = \\mathbf{A_2} (\\mathbf{A_3} - \\mathbf{S})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다음으로 inputs_to_hidden_units, $\\mathbf{Z_2}$이 cost, $\\mathbf{C}$에 주는\n",
    "영향(back_propagated_deriv_1)은 아래와 같다.($\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_2}} \\in \\mathbb{R}^{200 \\times n}$, $\\frac{\\partial \\mathbf{A_2}}{\\partial \\mathbf{Z_2}} = \\mathbf{A_2} \\odot (1 - \\mathbf{A_2})$, $\\frac{\\partial \\mathbf{Z_3}}{\\partial \\mathbf{A_2}} = \\mathbf{R}$)\n",
    "$$\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_2}} = \\frac{\\partial \\mathbf{A_2}}{\\partial \\mathbf{Z_2}} \\frac{\\partial \\mathbf{Z_3}}{\\partial \\mathbf{A_2}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_3}} = \\mathbf{A_2} \\odot (1 - \\mathbf{A_2}) ~ \\odot \\mathbf{R} (\\mathbf{A_3} - \\mathbf{S})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다음으로 embed_to_hid_weights, $\\mathbf{Q}$가 cost, $\\mathbf{C}$에 주는 영향은 아래와 같다.($\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Q}} \\in \\mathbb{R}^{150 \\times 200}$, $\\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{Q}} = \\mathbf{A_1}$)\n",
    "$$\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Q}} = \\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{Q}} \\frac{\\partial \\mathbf{A_2}}{\\partial \\mathbf{Z_2}} \\frac{\\partial \\mathbf{Z_3}}{\\partial \\mathbf{A_2}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_3}} = \\mathbf{A_1} \\left[ \\mathbf{A_2} \\odot (1 - \\mathbf{A_2}) ~ \\odot \\mathbf{R} (\\mathbf{A_3} - \\mathbf{S}) \\right]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 다음으로 embedding_layer_state($\\mathbf{A_1}$)이 cost, $\\mathbf{C}$에 주는 영향은 아래와 같다.($\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{A_1}} \\in \\mathbb{R}^{150 \\times n}$, $\\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{A_1}} = \\mathbf{Q}$)\n",
    "$$\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{A_1}} = \\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{A_1}} \\frac{\\partial \\mathbf{A_2}}{\\partial \\mathbf{Z_2}} \\frac{\\partial \\mathbf{Z_3}}{\\partial \\mathbf{A_2}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Z_3}} = \\mathbf{Q}\\left[ \\mathbf{A_2} \\odot (1 - \\mathbf{A_2}) ~ \\odot \\mathbf{R} (\\mathbf{A_3} - \\mathbf{S}) \\right]$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
