{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 Neural Networks for Machine Learning 강의를 정리한 내용임.   \n",
    "2016.11.07. by Dongwan Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6. Optimization, How to make the learning go faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leature 6a. Overview of mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 우선 전에 배웠던 error surface를 상기해보면, weight space에 error axis를 추가하여 error를 표현할 수 있었다. 또한 squared error를 사용하는 linear neuran의 경우 error surface가 아래로 오목한 그릇같은 모양이었다. (아래 그림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"06_error_surface_qb.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그런데 아래 그림과 같이 error surface의 횡단면이 많이 찌그러진(elongated) 타원형(ellipse)일 경우 convergence speed 문제가 발생할 수 있다. 예를들어 가파른 방향(아래 그림의 화살표)와 같은 경우 gradient값은 크지만 조금만 이동하는 것이 좋을 것이고, 반대로 아래 그림 화살표와 수직 방향으로는 gradient가 (상대적으로) 작지만 크게 움직이고 싶을 것이다. (앞서 배웠던 대로는 gradient에 비례하여 weight을 조정하기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"06_error_surface.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 또 다른 문제는 learning rate의 크기에 따라 발생할 수도 있는 문제가 있다. learning rate이 지나치게 클 경우 minimum으로 향하는 속도는 빠르겠지만 minimum을 지나쳐 발산하는 문제가 발생할 수 있고, 반대로 learning rate이 지나치게 작을 경우 수렴가능성은 커지지만 수렴하기까지 속도가 느려지는 문제가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"06_error_surface_learning_rate.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 주어진 데이터를 어떻게 사용할 것인가의 문제에 있어서 크게 해석적으로(analytically) 문제를 푸는 방법과 stochastic gradient descent(확률적 경사 하강법)를 이용하는 방법으로 나뉜다. 또한 stochastic gradient descent방법은 full batch, mini-batch, online 으로 나뉜다. online gradient descent의 경우 (물론 유용한 상황이 있겠으나...) 이 강의에서는 별다른 언급이 없다. \n",
    "\n",
    "> full batch 방식을 사용하는 경우 optimization 성능을 향상 시키기 위한 다양한 방법이 존재한다.(예를들어 non-linear conjugate gradient) 그런데 다른 분야에서 연구된 optimization방법들은 multilayer NN에 적용하기가 쉽지 않은 경우가 많다. 또한 mini-batch 방법의 경우 data set의 sample간 중복이 많은 경우 매우 효과적이며 가능한 batch size(한번에 사용하는 데이터의 크기)를 크게 하는 것이 연산 효율이 높다.\n",
    "\n",
    "> mini-batch 방식을 사용할때 learning rate을 조절하는 tip으로는 우선 error가 크게 요동치는 경우 learning rate을 줄이고, 반대로 error가 천천히 줄어들지만 그 속도가 매우 느린 경우 learning rate을 더 크게 하는 것이 좋다. 또한 minimun근처에 이르러서는 learning rate을 줄이는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 6b. A bag of ticks for mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> mini-batch gradient descent를 (속도와 수렴도 측면에서) 향상 시키기 위한 다양한 방법이 존재한다. 우선 cost가 mininum에 점점 다가갈 수록 learning rate을 줄이는 것이 좋다고 하기는 했지만, 너무 빨리 learning rate을 줄일 경우 역효과를 볼 수도 있다.(아래 그림의 빨간 선의 경우 너무 일찍 크게 learning rate을 줄였기 때문에 결과적으로 녹색선에 비해 cost가 떨어지지 않게 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"06_learning_rate1.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> weight값을 어떤 값으로 초기화 하는지도 중요한 문제인데, 만약 두개의 hidden layer가 같은 bias와 (incomming / outgoing) weight을 갖는다면 gradient값도 같기 때문에 learning이 진행되지 않는다. 이런 상황(symmetry)을 해결하기 위해 weight값을 작은 random 값으로 사용하는 바업이 있다. \n",
    "\n",
    "> 또한 hidden unit에 연결된 input unit이 많다면(big fan-in) input weight의 작은 변화에도 overshoot하는 문제가 발생할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 당연한 예기이지만... 다른 machine learning 방법론과 마찬가지로 feature를 shifting(mean normalization)하고 scaling하는 것과 feature간의 correlation을 줄이는 방법(PCA등)이 도움이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> mini-batch learning의 속도를 높이기 위한 방법들이 있다.\n",
    "> - momentun method: (Leature 6c)\n",
    "> - sepaate adaptive learning rates for each parameter: (Lectue 6d)\n",
    "> - rmsprop: (Lecture 6e)\n",
    "> - fancy optimization methods(더 좋은 방법들은 강의에서 다루지 않음..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 6c. The momentum method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> gradient로 weight의 position을 바꾸는 것이 아니라 velocity를 바꾸는 방법이다.\n",
    "기존 방법에서는 아래와 같이 gradient에 비례(learning rate $\\alpha$만큼)하는 값 으로 weight의 위치를 수정했었다. \n",
    "$$w(t) = w(t-1) + \\Delta w(t), ~ \\Delta w(t) = - ~ \\alpha \\frac{\\partial E}{\\partial w}(t)$$\n",
    "\n",
    "> 반면 momentun method에서는 아래와 같이 조금 다른 접근 방법을 택한다.\n",
    "$$\\Delta w(t) = \\alpha \\Delta w(t-1) - \\epsilon \\frac{\\partial E}{\\partial w}(t)$$\n",
    "\n",
    "> 이 방법이 일반적으로 (보통의) gradient descent보다 더 빠른 수렴 속도를 보여준다고 한다. 그리고 보통 작은 momentum값으로 learning을 시작하여 weight이 local minimum에 빠지거나 정체되는 경우 momentum값을 크게하면 local minimum(ravine)에서 빠져나오게 해 준다고 한다.\n",
    "\n",
    "> Nesterov 등이 소개한 더 좋은 momentum 방법도 강의에 소개되나.. 관심있는 분은 논문을 읽어 보기 바람.(Nesterov\t1983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 6d. Adaptive learning rates for each connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 각 weight마다 서로 다른 learning rate을 사용하겠다는 아이디어인데, 어떤 방식으로 다른 값을 사용할지에 관해서도 여러가지 방법이 있다.    \n",
    "> (강의에 소개되는 방법 외에도 좋은 방법들이 많기 때문에 자세한 설명은 생략한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 6e. Rmsprop: Divide the gradient by a running average of its recent magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> learning rate을 최근 gradient의 running average로 나눠 사용하겠다는 방법론이다.    \n",
    "(이 또한 자세한 설명은 생략..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [요약]      \n",
    "> - dataset의 sample 수가 많지 않거나(몇만건..), sample수가 많지만 중복 데이터가 많지 않은 경우 full-batch를 사용하는 것이 좋다. (Conjugate gradient, LBFGS)\n",
    "> - sample 수가 많고 중복이 많은 경우 mini-batch를 사용하는 것이 좋다. 여기에 momentum까지 사용하면 좋고, rmsprop도 사용해 보도록\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
