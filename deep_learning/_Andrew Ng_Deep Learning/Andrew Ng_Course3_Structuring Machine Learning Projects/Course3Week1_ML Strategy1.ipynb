{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coursera | Andrew Ng's Deep Learning Class | Course3. | Week1. ML Strategy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1]. Introduction to ML strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-1]. Why ML Strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Motivating example)    \n",
    "예를들어 아래 슬라이드와 같이 고양이 그림을 인식하는 모형을 만들었고 모형의 정확도가 90%정도 나온다고 하자. 그런데 90%정도로는 충분하지 않아서 어떻게 모형을 개선할 수 있을지 고민중이다. \n",
    "\n",
    "고민해볼 수 있는 것들은 아래와 같이 다양하다.\n",
    " - 데이터를 더 수집한다.\n",
    " - 더 다양한 종류의 훈련 데이터를 수집한다.\n",
    " - gradient descent를 더 오래 학습시킨다.\n",
    " - gradient descent 대신 Adam을 써 본다.\n",
    " - 더 큰 네트워크를 시도해본다.\n",
    " - 더 작은 네트워크를 시도해본다.\n",
    " - dropout을 써 본다.\n",
    " - L2 regularization을 써 본다.\n",
    " - 네트워크 구조를 바꿔본다.(다른 activation function, 히든 유닛의 수 변화 등등)\n",
    " - 이 밖의 다양한 것들...\n",
    "\n",
    "그런데 이 많은 요소들 중 어느것을 시도할지 잘못 선택하게 되면 뒤늦게 해당 접근이 별 효과가 없다는 것을 깨닫게될수도 있다.\n",
    "\n",
    "이 강의에서는 어떤 방식으로 모형을 분석하고 다양한 모형 개선 방법들 중 어느것을 먼저 시도하고 어느것은 시도하지 않는 것이 좋은지에 대해 알아볼 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-2]. Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝 시스템을 만들 때 어려운 점들 중 하나는 모형을 개선하기 위해 시도해볼 수 있는 것들이 너무도 많다는 것이다. 실력있는 머신러닝 전문가들의 특징중 하나는 모형의 어느 한가지 목표를 개선하기 위해 어느 것을 튜닝할지 관찰/판단할 수 있는 능력이 있다는 점이다. 다양한 개선 목표중 어느 것 하나에만 영향을 주는 수단을 찾고 적용하는 것을 ‘orthogonalization’이라 한다. \n",
    "\n",
    "예를들어 아래 슬라이드와 같이 오래된 텔레비전이 있다고 하자. 화면이 찌그러저 나오고 있는데 이를 수정할 수 있는 스위치가 여럿 달려 있고 이들을 조정하여 화면이 잘 나오게 하고 싶다. 예를들어 어떤 스위치는 상하를 조절하고 다른 스위치는 좌우를 또 다른 스위치는 회전을 조절하는 등 다양한 기능을 갖고 있다. 이렇게 각 스위치의 기능이 각각 독립적으로 설계되어 있다면 좋지만, 만약 좌우와 상하를 동시에 조절하는 것과 같이 다양한 화면 요소들을 조절하는 기능을 하나의 스위치로 동작시켜야 한다면 화면을 잘 조절하는 것이 거의 불가능할 것이다. \n",
    "\n",
    "이런 관점에서 orthogonalization은 각 스위치가 화면의 한가지만을 요소를 조절하도록 만드는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Chain of assumptions in ML)    \n",
    "지도학습 모형에서는 크게 4가지 단계에 걸쳐 모형을 학습시키고 평가한다.\n",
    "\n",
    "\n",
    "\t1. 훈련 데이터(test set)에 잘 피팅하여 인간 수준의 성능을 갖도록 하기\n",
    "\t2. 개발 데이터(dev set)에 잘 피팅하기\n",
    "\t3. 테스트 데이터(test set)에 잘 피팅하기\n",
    "\t4. 실제 세상의 문제에도 잘 동작하도록 하기\n",
    "\n",
    "\n",
    "이런 단계에 걸쳐 모형을 개선해가는데, 각 단계마다 문제가 있을 때 사용해야 하는 방법이 다르다. \n",
    "\n",
    "우선 훈련 데이터에 잘 피팅되지 않는다면, \n",
    "- 더 큰 네트워크를 시도\n",
    "- 최적화 방법을 바꿔본다.(adam, RMSProp..)\n",
    "\n",
    "두번째로 개발 데이터에 잘 피팅되지 않는다면, \n",
    "- regularization을 적용해본다.\n",
    "- 더 많은 훈련 데이터를 사용해본다.\n",
    "\n",
    "세번째로 테스트 데이터에 잘 피팅되지 않는다면,\n",
    "- 더 많은 개발 데이터를 사용해본다.(개발 데이터에 과적합되어 있을 수 있으므로)\n",
    "\n",
    "마지막으로 현실 세계의 데이터에 잘 동작하지 않는다면,\n",
    "- 개발 데이터를 바꿔본다.\n",
    "- 비용함수를 바꿔본다.\n",
    "(개발 데이터의 분포가 현실 데이터와 다르거나  사용중인 비용함수가 모형 평가의 좋은 지표가 아닐 수 있기 때문)\n",
    "\n",
    "참고로 (저자) 개인적으로는 early stopping을 잘 사용하지 않는다. 왜냐하면 early stopping은 훈련 데이터 피팅과 개발 데이터 피팅에 모두 영향을 주기 때문이다.\n",
    "\n",
    "\n",
    "앞으로 위 내용에 대해 좀 더 자세히 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2]. Setting up your goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-1]. Single number evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝 시스템을 만드는 것은 아이디어를 구현하고 테스트 하고 수정하는 과정의 반복이다.  즉 다양한 버전의 모형을 적용해보고 성능을 평가 해야한다. 이때 모형의 성능을 잘 설명해주는 한가지 지표가 있다면 모형 평가가 매우 용이해질 것이다. \n",
    "\n",
    "예를들어 두개의 모형 A와 B가 있고 이 두 모형의 Precision과 Recall을 측정했더니 아래 슬라이드의 표과 같은 결과가 나왔다고 하자. 그런데 A모형은 B모형보다 Recall이 높지만 Precision은 높지 않다. 이런 경우 둘중 어떤 모형이 더 좋다고 해야할까? 이런 경우 Precision과 Recall의 harmonic mean을 의미하는 F1 Score를 사용하는 것이 좋다.\n",
    "\n",
    "Precision = TP / (TP + FP) : 예측한것중 진짜의 비율\n",
    "Recall = TP / (TP + FN) : 진짜중 진짜라고 한 비율\n",
    "F1 Score = 1 / ((1/P) + (1/R))\n",
    "\n",
    "이처럼 다양한 판단 기준보다는 전체를 종합적으로 판단할 수 있는 어떤 한가지 지표가 존재한다면 모형을 개선하는 과정을 좀더 빠르게 진행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Another example)   \n",
    "또 다른 예로 각 국가마다 존재하는 지표가 있다면 이들을 종합하여 평균을 기준으로 모형 성능을 평가하면 성능에 대한 판단이 쉬워진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-2]. Satisficing and optimizing metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Another cat classification example)   \n",
    "아래 표와 같이 A, B, C 3개의 모형에 대한 예측 정확도와 실행시간이 있다. 이 두가지 정보를 종합하여 각 모형의 성능을 평가하고자 한다. 그런데 정확도는 가능한 높으면 좋겠으나 실행시간은 대략 1000이하이면 그 값이 얼마인지가 중요하지 않다고 하자.\n",
    "이런 경우 정확도는 최대화 해야 하므로 optimizing metric이고 실행시간은 조건만 만족시키면 되므로 satisfying metric이라 할 수 있다. \n",
    "\n",
    "또 다른 예로 인공지능 스피커가 있을 때 이들의 음성인식 정확도는 가능한 높아야 하고, 잘못 깨어나는 횟수는 하루 최대 1회만 허용되면 좋겠다면 이런 조건을 아래와 같이 기술할 수 있다.\n",
    "maximize accuracy s.t. #false positive <= 1 for every 24 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-3]. Train/dev/test distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training / dev / test set 데이터를 어떻게 구성하는지도 중요하다. 예를들어 여러 국가에서 수집된 고양이 그림들이 있다고 하자. 이때 개발데이터를 상위 몇개 국가들의 사진으로 하고 테스트 데이터는 하위 몇개 국가들의 사진으로 구성한다면 어떨까? 이렇게 하면 개발 데이터와 훈련 데이터의 분포가 다르기 때문에 열심히 과녁 맞추기를 연습한 후 평가할 때에는 과녁을 옮기는 것과 같은 꼴이 된다. 보다 나은 접근은 여러 국가의 데이터에서 랜덤하게 샘플링하여 개발데이터와 테스트 데이터를 구성하는 것이다. 즉 개발 데이터와 테스트 데이터의 분포를 갖도록 구성하는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-4]. Size of dev and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 설명한 것 처럼 개발데이터와 훈련데이터의 분포가 같아야 한다. 그럼 각 데이터셋의 크기는 어느정도로 하는 것이 좋을까? 이런 가이드라인은 과거의 머신러닝과 요즘의 딥러닝에서 차이가 있다. \n",
    "과거에는 7:3으로 훈련데이터와 테스트 데이터를 나누거나, 6:2:2로 훈련데이터/개발데이터/테스트 데이터를 나누는 가이드라인이 일반적이었다. \n",
    "그런데 요즘에는 사용할 수 있는 데이터의 크기가 아주 커지면서 이런 가이드라인에도 변화가 있다. 예를들어 100만 건의 데이터를 사용할 수 있다면 대략 98% 데이터를 훈련에 사용하고, 나머지 1%를 개발에 1%를 테스트에 사용하는 해도 충분하다. 데이터가 커지면서 개발데이터와 테스트 데이터에 사용될 데이터의 비율이 작아도 충분히 그 목적을 다할 수 있다는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Size of test set)    \n",
    "테스트 데이터의 목적은 모형을 완성한 후 모형의 객관적인 성능을 평가하는 것이다. 그렇기 때문에 테스트 데이터가 커지면 커질 수록 이 평가의 신뢰성이 높아진다. 역으로 생각하면 모형의 성능에 대한 지표의 정확성이 크게 중요하지 않다면 테스트 데이터에 많은 비율의 데이터를 사용할 필요가 없다는 것이다. \n",
    " 혹은 상황에 따라 테스트 데이터가 필요 없는 경우도 있다. 이런 경우 데이터를 훈련 데이터와 개발 데이터로만 나누는데, 이때의 개발 데이터를 테스트 데이터로 부르는 경우도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개발 데이터(dev set)과 평가지표(evaluation metric)는 모형 개발/개선에 있어서 과녁/목표와 같은 역할을 한다. 그런데 프로젝트가 꽤 진행된 후 뒤늦게 그 과녁이 틀렸다는걸 깨닫는 경우도 있다. 즉 dev set이 현실 데이터를 잘 반영하지 못했다거나 평가지표가 좋은 모형 평가 지표로서 역할을 못했다거나 하는 것이다.\n",
    "이런 경우 늦게라도 잘못된 과녁을 수정하는 일이 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Cat dataset examples)      \n",
    "예를들어 고양이를 분류하는 모형 A와 B를 만들었고 오분류율을 모형 평가지표로 하여 이 지표가 낮은 모형을 더 좋은 모형이라고 평가한다고 하자. 그런데 A알고리즘은 오분류 비율이 3%로서 B모형보다 좋지만 종종 포르노 이미지를 고양이라고 보여준다. 반면 B 모형은 오분류 비율은 5%로서 A모형보다 나쁘지만 포르노 이미지를 노출시키는 일은 없다. 이런 경우 모형 평가지표가 잘못되었다고 할 수 있다.\n",
    "\n",
    "최초의 평가지표인 오류비율을 아래와 같이 표현할 수 있다.\n",
    "$$\\text{Error:} ~ \\frac{1}{m}\\sum_{i=1}^{m_{\\text{dev}}} \\mathcal{1} \\left( y_{\\text{predicted}}^{(i)} \\neq y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "위 평가지표를 아래와 같이 수정하여 포르노 이미지에 대한 패널티를 포함시킬 수 있다.\n",
    "$$\\text{Error:} ~ \\frac{1}{\\sum_{i=1} w^{(i)}}\\sum_{i=1}^{m_{\\text{dev}}} w^{(i)} \\mathcal{1} \\left( y_{\\text{predicted}}^{(i)} \\neq y^{(i)} \\right) \\\\\n",
    "\\text{where} ~ w^{(i)} = \\begin{cases}\n",
    "1 \\qquad \\text{if} x^{(i)} ~ \\text{is non-porno} \\\\\n",
    "10 \\qquad \\text{if} x^{(i)} ~ \\text{is porno}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "위 평가지표의 구체적인 사항은 상황에 따라 바뀔 수 있는 부분이고, 중요한 점은 평가지표에 문제가 있다는 것을 발견했다면 주저하지 말고 새로운 평가지표를 사용해야한다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Orthogonalization for cat pictures: anti-porn)    \n",
    "머신러닝 프로젝트를 진행할 때 전체 일을 구성하는 작은 작업들을 그 성격에 따라 단계별로 나누는 것이 좋다. 앞서 논의했던 사항들도 마찬가지인데, 모형을 잘 평가할 수 있는 지표를 만드는 단계와 더 좋은 평가지표값이 나오도록 모형을 개선하는 단계를 분리하여 진행하는 것이 좋다. (예를들어 과녁을 적절한 곳에 놓는 것과 그 과녁의 중앙을 맞추도록 활을 쏘는 것을 분리하여 생각하라는 것이다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Another example)    \n",
    "평가지표가 잘못 설정된 경우 뿐만아니라 개발데이터(dev set)이 잘못 설정된 경우도 있다. 예를들어 고양이 분류 모형을 만들었고 고해상도 이미지를 잘 분류하도록 모형을 학습시켰다. 그런데 실제 모형이 사용되는 환경에서는 고해상도 이미지보다는 저해상도 이미지가 더 많이 사용되고 있다. A모형의 경우 고해상도 이미지에 대한 오분류율이 3%에 불과했지만 실제 사용 환경에서는 B모형이 A모형보다 더 잘 분류하는 것으로 나타났다. 이런 상황은 개발 데이터가 실제 사용환경에서의 데이터를 잘 반영하지 못하는 경우이다.\n",
    "\n",
    "즉 개발데이터에서의 성능이 실제 사용환경에서의 성능을 보장해주지는 않는다는 것이다. 또한 이런 상황이 발견되었다면 개발데이터를 바꾸는 것을 고려야해한다.\n",
    "\n",
    "머신러닝 프로젝트를 빠르고 성공적으로 진행하기 위해서는 평가지표와 개발데이터가 필수적이다. 물론 없어도 진행이 되겠지만 더 오랜 시간과 시행착오가 따를 것이다. 또한 설정한 평가지표와 개발데이터가 처음부터 좋은 수는 없기 때문에 우선 설정하고 이터레이션을 진행하면서 개선해나가는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3]. Comparing to human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3-1]. Why human-level performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최근들어 머신러닝 시스템의 성능을 인간 수준과 비교하곤한다. 이렇게된 이유는 딥러닝 기술의 발달로 모형의 성능이 인간의 수준에 근접하거나 오히려 초월하는 결과도 나오고 있기 때문이다. 또 다른 이유는 인간 또한 할수 있는 어떤 일을 머신러닝으로 구현하는 ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Comparing to human-level performance)   \n",
    "예를들어 음성인식 모형을 만든다고 할 때 그 성능은 여러 개선 과정을 거쳐 점점 좋아지고 인간 수준을 넘어 bayes optimal error에 근접해 간다. 여기서 bayes optimal error는 최대로 도달 할 수 있는 정확도 수준으로서 데이터에 포함된 노이즈나 현상을 정확히 반영하고 있지 못함으로 인해 생기는 원천적인 한계를 의미한다. \n",
    "모형 성능 개선은 인간의 수준을 넘어서면서 그 개선 폭이 점점 작아진다. 그 이유는 (예를들어 이미지 인식과 같은 문제에서) 인간의 수준이 bayes optimal error에서 그리 떨어져 있지 않기 때문이다. 또한 모형의 성능이 인간 수준 아래 일 때에는 인간 수준 정도로 모형 성능을 올릴 수 있는 다양한 방법들이 존재하지만 인간 수준을 넘어서면 그렇지 않은 것도 이유이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Why compare to human-level performance)   \n",
    "만약 어떤 모형의 성능이 인간 수준에 미치지 못하고 있다면, 모형을 개선하기 위해 사람이 직접 레이블링한 데이터를 이용하거나, 왜 사람은 맞췄고 모형은 그렇지 못했는지를 관할거나, 사람 수준과 비교하여 모형의 bias/variance문제를 분석해볼 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3-2]. Understanding human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝/딥러닝 논문에서 human level performance란 키워드가 종종 사용된다. 이 키워드가 어떤 의미인지 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Human-level error as a proxy for Bayes error)   \n",
    "앞선 영상에서 Human-level error를 이용해 Bayes error(이상적인 수준의 error)가 어느정도 수준일지 대략 가늠할 수 있다고 했었다. 그런 이 human-level이란걸 좀더 엄밀히 정의할 필요가 있다.\n",
    "\n",
    "예를들어 아래 슬라이드와 같은 엑스레이 사진이 주어져 있고 어떤 상병에 해당하는지 아닌지를 분류하는 문제가 주어져 있다고 하자. 그리고 여러 상황의 human-level error를 측정했다.\n",
    "\n",
    "\t1. 그냥 보통 사람들은 3% 에러\n",
    "\t2. 보통 의사들은 1% 에러\n",
    "\t3. 경험 많은 의사들은 0.7% 에러\n",
    "\t4. 경험 많은 의사들이 팀을 이룬 경우 0.5%에러\n",
    "\n",
    "이렇게 human-level에도 다양한 경우가 존재하는데 어떤 것을 human-level error로 사용해야 할까?\n",
    "\n",
    "두가지 관점에서 human-level error를 정할수 있는데. 우선 human-level error가 Bayes error의 근사치라는 관점에서는 위 예시에서  Bayes error가 0.5% 혹은 그 이하의 값일 것이므로 human-level error를 0.5%로 정할 수 있다. 두번째로 모형이 대체하려는 인간의 수준이 존재한다면 그 수준을 human-level error로 정의하는데, 예시에서는 영상 판독 시스템이 일반적인 의사들을 돕는데 사용된다면 이때의 human-level error는 1%가 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Error analysis example)   \n",
    "human-level error와 training error의 차이가 크지 않고, training error와 dev error의 차이도 크지 않은 경우 human-level error를 어느 수준으로 정하느냐가 중요진다. 즉 human-level error를 0.5%로 하느냐 혹은 1%로 하느냐에 따라 high bias상황인지 high variance 상황인지가 달라질 수 있다. 이런 경우 가능한 높은 수준의 human-level error를 기준으로 하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Summary of bias/variance with human-level performance)   \n",
    "인간이 꽤 잘하는 문제를 풀 때에는 Human-level error를 Bayes error에 대한 추정치로 사용할 수 있다. 이런 경우 Human-level error와 training error의 차이는 bias의 크기를 의미하며, training error와 dev error의 차이는 variance의 크기를 의미한다.\n",
    "\n",
    "일반적으로 training error가 0%에서 얼마나 떨어져 있는지를 bias의 척도로 사용하는데, 측정의 한계 혹은 어플리케이션의 특성 등의 상황에 따라 bayes error가 0%가 아닌 경우도 있다. 이런 경우 추정 가능한 bayes error와 training error를 비교하여 bais의 정도를 평가하는 것이 옳다.\n",
    "\n",
    "이렇게 human-level error를 bayes error의 근사치로 보고 모형을 개선해나가는 방식은 모형의 성능이 human-level 보다 떨어질때에는 효과적이다. 그런데 점차 모형의 성능이 human-level을 넘어서면 또 다른 접근법이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3-3]. Surpassing human level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모형의 성능이 human-level을 넘어서게되면 모형 성능을 개선하는 것이 점점 어려워진다. 예를들어 어떤 영상 인식 문제에서 몇명이 팀으로 판별하는 경우 에러가 0.5%이고, 한명이 판별하는 경우 에러가 1%라고 가정해보자. (이 경우 bayes error는 0.5%이다.)\n",
    "이때 training error가 0.6%이고 dev error가 0.8%라면 bias는 0.1%이고, variance가 0.2%라고 할 수 있다. \n",
    "그런데 만약 training error가 0.3%이고 dev error가 0.4%라면 bias의 크기를 얼마라고 해야할까? 사실 이때 bayes error가 0.1%인지 0.2%인지 얼마인지 알 수 없기 때문에 bias의 크기를 판단하기 어렵다. 또한 이런 상황에서는 모형이 잘 판단하지 못하는 샘플들을 인간의 그것과 비교하여 모형의 문제점을 찾아내는 것도 어려워진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Problems where ML significantly surpasses human-level performance)   \n",
    "머신러닝이 인간보다 잘 해결하는 많은 문제들이 있다. 예를들어 온라인 광고(어떤 사람이 어떤 광고를 클릭할 확률이 얼마인가?), 상품 추천(어떤 사람에 적절한 상품은 어떤 것인가?), 물류문제(목적지에 도달하는 최적의 경로는? 얼마나 걸릴 것인가?), 대출 승인(이 사람에게는 얼마나 대출해주는 것이 좋은가?) 와 같은 문제들이 그러하다. 그런데 이런 문제들의 특징은 모두 구조화된 데이터(structured data)를 사용한다는 것이다. 또한 이런 문제들은 인지 문제(natural perception problem)은 아니라는 공통점이 있다. 사실 영상/음성인식이나 자연어처리와 같은 인식 문제는 일반적으로 인간이 컴퓨터 보다 잘 해낸다. 물론 최근에는 인지문제에 있어서도 인간의 수준을 넘어서는  모형들이 속속 등장하고 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3-4]. Improving your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orthogonalization 기법, dev / test 데이터를 나누는 법, human-level performance를 이용해 bayes error 수준을 정하는 법, bias/variance의 크기를 판단하는 법 등을 다뤘다. 이것들을 종합해 모형의 성능을 개선하는  방법에 대한 가이드라인을 제시하고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The two fundamental assumptions of supervised learning)    \n",
    "어떤 지도학습 모형이 잘 동작한다는건 아래 두가지 목표를 이뤘다는 것을 의미한다. \n",
    "첫번째는 모형이 훈련데이터에 잘 적합되어 있다. 즉 bias가 크지 않다는 것이다.\n",
    "두번째는 훈련데이터에 적합한 모형이 개발/테스트 데이터에도 잘 일반화된다는 것이다. 즉 variance가 크지 않다는 것이다.\n",
    "\n",
    "orthogonalization이란 bias를 판단하는 지표와 variance를 판단하는 지표로 서로 독립적인 지표를 사용하라는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Reducing avoidable bias and variance)    \n",
    "Human-level error와 훈련 데이터 에러를 비교해서 bias 의 크기를 판단할 수 있고, 훈련 데이터 에러와 개발데이터 에러를 비교해서 variance의 크기를 판단할 수 있다.\n",
    "\n",
    "bias 문제를 개선하기 위한 접근법은 아래와 같은 것들이 있다.\n",
    "- 더 큰 신경망을 사용\n",
    "- 더 나은 최적화 알고리즘을 사용(momentun, RMSProp, Adam...)\n",
    "- 다른 구조의 신경망을 사용(RNN, CNN, ...)\n",
    "- 하이퍼파라미터 튜닝\n",
    "\n",
    "variance 문제를 개선하기 위한 접근법은 아래와 같은 것들이 있다.\n",
    "- 더 많은 훈련 데이터를 사용\n",
    "- regularization을 사용(dropout, L2, data augmentation)\n",
    "- 다른 구조의 신경망을 사용(RNN, CNN, ...)\n",
    "- 하이퍼파라미터 튜닝"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
