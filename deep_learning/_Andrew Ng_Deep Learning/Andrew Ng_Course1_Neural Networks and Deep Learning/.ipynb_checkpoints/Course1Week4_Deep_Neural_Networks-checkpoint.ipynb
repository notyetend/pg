{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coursera  | Andrew Ng's Deep Learning Class | Course1. Neural Networks and Deep Learning | Week4. Deep_Neural_Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1]. Deep Neural Nework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-1]. Deep L-Layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀모형은 신경망 관점에서 출력 레이어 하나 뿐 이므로 매우 얕은(shallow) 신경망이라 할 수 있다. 여기에 히든 레이어가 추가될 수 있고 깊은 신경망을 구성할 수 있다. 보다 깊은 신경망 모형은 얕은 신경망이 학습하지 못하는 패턴을 학습할 있는 경우가 많다. 신경망의 레이어 수를 셀때는 입력에이어를 제외하고 세는 것이 전통이다. 아래 슬라이드의 우측 하단 레이어의 경우 입력 레이어를 제외하고 히든 레이어가 5개 출력 레이어가 1개인 6레이어 신경망이다.\n",
    "\n",
    "내 데이터를 위한 최적의 히든 레이어 수를 미리 알수는 없다. 최적의 히든 레이어 수를 찾기 위한 최선의 방법은 다양한 경우의 히든 레이어 수를 홀드 아웃 교차검증 데이터(hold-out cross validation set)에 적용해 보고 가장 좋은 성능을 보이는 레이어 수를 선택하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L01_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로 딥뉴러넷을 표현하기 위한 표기법을 정리하고 넘어간다.\n",
    "\n",
    "- $L$ : 레이어 수, 아래 슬라이드에서는 4개(입력 레이어는 세지 않는다. 혹은 입력 레이어는 0번째 레이어라 부르고, 출력 레이어는 $L$번째 레이어라 한다.)\n",
    "- $n^{[l]}$ : $l$번째 레이어의 유닛(동그라미)수, 가령 아래 예에서 $n^{[0]}=n_x=3$, $n^{[1]}=5$, $n^{[2]}=5$, $n^{[3]}=3$, $n^{[4]}=n^{[L]}=1$\n",
    "- $n_x$ : 입력 데이터의 변수 수, 혹은 입력 레이어의 유닛 수\n",
    "- $a^{[l]}$ : $l$번째 레이어의 활성값(activation value), $g^{[l]}(z^{[l]})$로 계산된다.\n",
    "- $w^{[l]}$ : $z^{[l]}$ 계산에 사용되는 가중치\n",
    "- $b^{[l]}$ : $z^{[l]}$ 계산에 사용되는 bias항\n",
    "- 입력값 $x$를 0번째 레이어의 활성값이라고 생각할수도 있다. $x=a^{[0]}$\n",
    "- 마지막 레이어(출력 레이어)의 활성값 $a^{[L]}$은 예측값으로 사용되며 $\\hat{y}$로도 표기한다. (일반적으로 예측값에 $\\hat{hat}$을 붙인다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L01_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-2]. Forward Propagation in a Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "히든 레이어가 3개(총 레이어는 출력 레이어를 포함하여 4개)일 경우 순전파(forward propagation) 과정은 아래와 같다. 사실 지난주 내용과 큰 차이는 없으며 입력값 $x$를 $a^{[0]}$로 표기 했다.\n",
    "$$\\begin{align}\n",
    "&\\textbf{for}\\text{ each sample}\\textbf{ do} \\\\\n",
    "&\\qquad z^{[1]} = W^{[1]}a^{[0]} + b^{[1]} \\\\\n",
    "&\\qquad a^{[1]}=g^{[1]}(z^{[1]}) \\\\\n",
    "&\\qquad z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\\\\n",
    "&\\qquad a^{[2]}=g^{[2}(z^{[2]}) \\\\\n",
    "&\\qquad z^{[3]} = W^{[3]}a^{[2]} + b^{[3]} \\\\\n",
    "&\\qquad a^{[3]}=g^{[3}(z^{[3]}) \\\\\n",
    "&\\qquad z^{[4]} = W^{[4]}a^{[3]} + b^{[4]} \\\\\n",
    "&\\qquad a^{[4]}=g^{[4}(z^{[4]}) \\\\\n",
    "&\\textbf{end for}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "각 샘플에 대한 for-loop를 아래와 같이 벡터화 할 수 있다.     \n",
    "위와 아래의 차이점은 $z^{[l]}$, $a^{[l]}$의 shape은 ($n^{[l]}$, $1$)이고 $Z^{[l]}$, $A^{[l]}$의 shape은 ($n^{[l]}$, $m$)이라는 것이다. $W^{[l]}$와 $b^{[l]}$의 shape은 각각 ($n^{[l]}$, $n^{[l-1]}$)와 ($n^{[l]}$, $1$)으로 변함 없다. 숙지할 점은 아래의 벡터화된 버전에서는 $b^{[l]}$를 더할 때 $b^{[l]}$의 shape이 ($n^{[l]}$, $m$)으로 (파이썬 numpy에 의해) 자동 변환(broadcasting)된다는 것이다.\n",
    "$$\\begin{align}\n",
    "& Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} \\\\\n",
    "& A^{[1]}=g^{[1]}(Z^{[1]}) \\\\\n",
    "& Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "& A^{[2]}=g^{[2}(Z^{[2]}) \\\\\n",
    "& Z^{[3]} = W^{[3]}A^{[2]} + b^{[3]} \\\\\n",
    "& A^{[3]}=g^{[3}(Z^{[3]}) \\\\\n",
    "& Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]} \\\\\n",
    "& A^{[4]}=g^{[4}(Z^{[4]}) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L01_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-3]. Getting your matrix dimensions right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 구현할 때 코드에 문제가 없는지 확인하는 가장 좋은 방법은 각 행렬/벡터의 차원(shape)을 확인해보는 것이다. \n",
    "\n",
    "아래 신경망은 레이어 (입력 레이어를 제외한)5개 이므로 $L=5$이고, 각 레이어의 유닛 수는 아래와 같다.\n",
    "\n",
    "- $n^{[0]} = n_x = 2$\n",
    "- $n^{[1]} = 3$\n",
    "- $n^{[2]} = 5$\n",
    "- $n^{[3]} = 4$\n",
    "- $n^{[4]} = 2$\n",
    "- $n^{[5]} = n^{[L]} = 1$\n",
    "\n",
    "\n",
    "첫번째 순전파는 $z^{[1]} = W^{[1]}\\cdot x + b^{[1]}$, $a^{[1]}=\\sigma(z^{[1]})$ 이다. 이때 각 행렬의 shape은 아래와 같다.\n",
    "\n",
    "- $z^{[1]}$ : (3, 1) = ($n^{[1]}$, 1)\n",
    "- $W^{[1]}$ : (3, 2) = ($n^{[1]}$, $n^{[0]}$)\n",
    "- $x = a^{[0]}$ : (2, 1) = ($n^{[0]}$, 1) \n",
    "- $b^{[1]}$ : (3, 1) = ($n^{[1]}$, 1)\n",
    "- $a^{[1]}$ : (3, 1) = ($n^{[1]}$, 1)\n",
    "\n",
    "두번째 순전파는 $z^{[2]} = W^{[2]}\\cdot a^{[1]} + b^{[2]}$, $a^{[2]}=\\sigma(z^{[2]})$ 이고, 각 행렬의 shape은 아래와 같다.\n",
    "\n",
    "- $z^{[2]}$ : (5, 1) = ($n^{[2]}$, 1)\n",
    "- $W^{[2]}$ : (5, 3) = ($n^{[2]}$, $n^{[1]}$)\n",
    "- $a^{[1]}$ : (3, 1) = ($n^{[1]}$, 1) \n",
    "- $b^{[2]}$ : (5, 1) = ($n^{[2]}$, 1)\n",
    "- $a^{[2]}$ : (5, 1) = ($n^{[2]}$, 1)\n",
    "\n",
    "... \n",
    "\n",
    "$l$ 번째 레이어의 순전파는 $z^{[l]} = W^{[l]}\\cdot a^{[l-1]} + b^{[l]}$, $a^{[l]}=\\sigma(z^{[l]})$ 이고, 각 행렬의 shape은 아래와 같다.\n",
    "\n",
    "- $z^{[l]}$ : = ($n^{[l]}$, 1)\n",
    "- $W^{[l]}$ : = ($n^{[l]}$, $n^{[l-1]}$)\n",
    "- $a^{[l-1]}$ : = ($n^{[l-1]}$, 1) \n",
    "- $b^{[l]}$ : = ($n^{[l]}$, 1)\n",
    "- $a^{[l]}$ : = ($n^{[l]}$, 1)\n",
    "\n",
    "역전파에 등장하는 $dW$와 $db$의 shape은 각각 $W$와 $b$의 shape과 같다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L02_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 $m$개 샘플에 대한 for-loop를 벡터화 한 경우 $l$ 번째 레이어 순전파는 $Z^{[l]} = W^{[l]}\\cdot A^{[l-1]} + b^{[l]}$, $A^{[l]}=\\sigma(Z^{[l]})$이며 각 행렬의 shape은 아래와 같다.\n",
    "\n",
    "- $Z^{[l]}$ : = ($n^{[l]}$, $m$)\n",
    "- $W^{[l]}$ : = ($n^{[l]}$, $n^{[l-1]}$)\n",
    "- $A^{[l-1]}$ : = ($n^{[l-1]}$, $m$)\n",
    "- $b^{[l]}$ : = ($n^{[l]}$, 1)\n",
    "- $A^{[l]}$ : = ($n^{[l]}$, $m$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L02_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-4]. Why deep representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥뉴러넷이 많은 문제에 효과적인 이유는 뭘까? 이 질문에 답하려면 뉴러넷의 각 레이어가 뭘 학습하는지 알아야 한다. 아래 예와 같은 어떤 이미지를 인식하는 신경망이 있다고 할 때, 앞단의 레이어 단순한 형태를 학습하고, 다음 레이어는 얼굴의 특정 부위를 인식하고, 다음 레이어는 좀 더 전체적인 부분을 학습하게 된다. 이런 계측적인 학습 구조는 단순히 이미지 뿐만 아니라 다른 형태의 데이터에도 유사하게 적용되는데, 예를들어 음성 인식의 경우 앞단의 레이어는 음성의 단순한 형태만을 학습하고 다음 레이어는 자모를 인지하고 다음 레이어는 단어를 인식하는 식으로 깊은 레이어일 수록 복잡한 형태를 학습하게 된다. 이런 단계적인 인식은 인간의 뇌가 사물을 인지하는 방식과 유사하다고 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L03_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥뉴러넷이 왜 잘 동작하는지를 설명할 때 자주 사용하는 다른 예로, 적은 수의 유닛으로 구성된 레이어가 다수인 신경망으로 계산되는 연산이 있을 때, 히든 레이어를 하나만 사용하도록 제한할 경우 필요한 유닛의 수가 무수히 많아진다는 것이다. 아래 그림과 같이 좌측의 XOR 회로의 경우 $n$개 변수의 XOR 연산을 구현하기 위해 적은 수의 유닛을 다 층으로 구성할 경우 $\\log n$개의 XOR 유닛으로 계산 가능하다. 하지만 아래 우측의 XOR 회로의 경우 히든 레이어를 하나로 제한할 경우 XOR 유닛이 $2^n$개 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L03_02_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-5]. Building blocks of deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$l$번째 레이어의 순전파(forward-propagation)**는 $a^{[l-1]}$를 입력으로 받아 $a^{[l]}$를 출력으로 내놓는 과정이다. 이때 $z^{[l]}$을 구하기 위해 $W^{[l]}$과 $b^{[l]}$이 사용되며, $a^{[l]}$를 구하기 위해 $z^{[l]}$가 사용된다. \n",
    "$$\\begin{align}\n",
    "&z^{[l]}=W^{[l]} \\cdot a^{[l-1]} + b^{[l]} && \\text{shape: } (n^{[l]}, 1) = (n^{[l]}, n^{[l-1]}) \\cdot (n^{[l-1]}, 1) + (n^{[l]}, 1)\\\\\n",
    "&a^{[l]}=g^{[l]}(z^{[l]}) && \\text{shape: } (n^{[l]}, 1) = g^{[l]}\\left( (n^{[l]}, 1) \\right) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "또한 이 과정을 아래 슬라이드 우측 상단의 상자 형태로 표현할 수 있다. 단순히 순전파는 '$a^{[l-1]}$를 입력으로 받아 $a^{[l]}$를 출력으로 내놓는 과정'으로 설명된다. 이 과정에서 ($W^{[l]}$, $z^{[l]}$, $a^{[l-1]}$)은 나중에 역전파 계산시 사용되므로 실제 구현에서는 이 값들을 어딘가에 저장(cache)해 놓는다. 이 순전파의 출력 $a^{[l]}$이 다음 레이어 순전파의 입력으로 사용되고 $a^{[l+1]}$을 계산하는 식의 과정이 반복된다.\n",
    "\n",
    "**$l$번째 레이어의 역전파(backward-propagation)**는 $da^{[l]}$를 입력으로 받아 $da^{[l-1]}$를 출력으로 내놓는 과정이며, 이때 ($W^{[l]}$, $z^{[l]}$, $a^{[l-1]}$)이 사용되며 계산 과정은 아래와 같다. \n",
    "$$\\begin{align}\n",
    "&dz^{[l]} = da^{[l]} * {g^{'[l]}}(z^{[l]}) && \\text{shape: } (n^{[l]}, 1) = (n^{[l]}, 1) * g^{'[l]}\\left((n^{[l]}, 1)\\right)\\\\\n",
    "&dW^{[l]} = dz^{[l]} \\cdot a^{T[l-1]}       && \\text{shape: } (n^{[l]}, n^{[l-1]}) = (n^{[l]}, 1) \\cdot (n^{[l-1]}, 1)^T\\\\\n",
    "&db^{[l]} = dz^{[l]}                       && \\text{shape: } (n^{[l]}, 1) = (n^{[l]}, 1)\\\\\n",
    "&da^{[l-1]} = {W^{[l]}}^T \\cdot dz^{[l]}   && \\text{shape: } (n^{[l-1]}, 1) = (n^{[l]}, n^{[l-1]})^T \\cdot (n^{[l]}, 1)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "또한 이 과정을 아래 슬라이드 우측 하단의 상자 형태로 표현할 수 있다. 단순히 역전파는 $da^{[l]}$를 입력으로 받아 $da^{[l-1]}$를 출력으로 내놓는 과정'으로 설명된다. 이 과정에서 계산된 $dz^{[l]}$를 이용해 실제 파라미터 수정에 사용되는 $dW^{[l]}$과 $db^{[l]}$를 구하게 된다. 그리고 출력 $da^{[l-1]}$를 다음 레이어 역전파의 입력으로 사용해 $da^{[l-2]}$를 구하는 식의 과정이 반복된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**만약 각 샘플에 대한 for-loop를 벡터화한 경우 위 내용은 아래와 같이 달라진다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$l$번째 레이어의 순전파(forward-propagation)**는 $A^{[l-1]}$를 입력으로 받아 $A^{[l]}$를 출력으로 내놓는 과정이다. 이때 $z^{[l]}$을 구하기 위해 $W^{[l]}$과 $b^{[l]}$이 사용되며, $a^{[l]}$를 구하기 위해 $Z^{[l]}$가 사용된다. ($+ b^{[l]}$는 $(n^{[l-1]}, m)$로, $\\cdot 1$은 $(m, 1)$로 broadcast된다. $\\cdot 1$은 사실 각 행의 모든 값을 더하여 열벡터를 만드는 것과 같다.)\n",
    "$$\\begin{align}\n",
    "&Z^{[l]}=W^{[l]} \\cdot A^{[l-1]} + b^{[l]} && \\text{shape: } (n^{[l]}, m) = (n^{[l]}, n^{[l-1]}) \\cdot (n^{[l-1]}, m) + (n^{[l]}, 1)\\\\\n",
    "&A^{[l]}=g^{[l]}(Z^{[l]}) && \\text{shape: } (n^{[l]}, m) = g^{[l]}\\left( (n^{[l]}, m) \\right) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "또한 이 과정을 아래 슬라이드 우측 상단의 상자 형태로 표현할 수 있다. 단순히 순전파는 '$A^{[l-1]}$를 입력으로 받아 $A^{[l]}$를 출력으로 내놓는 과정'으로 설명된다. 이 과정에서 ($W^{[l]}$, $Z^{[l]}$, $A^{[l-1]}$)은 나중에 역전파 계산시 사용되므로 실제 구현에서는 이 값들을 어딘가지 저장(cache)해 놓는다. 이 순전파의 출력 $A^{[l]}$이 다음 레이어 순전파의 입력으로 사용되고 $A^{[l+1]}$을 계산하는 식의 과정이 반복된다.\n",
    "\n",
    "**$l$번째 레이어의 역전파(backward-propagation)**는 $dA^{[l]}$를 입력으로 받아 $dA^{[l-1]}$를 출력으로 내놓는 과정이며, 이때 ($W^{[l]}$, $Z^{[l]}$, $A^{[l-1]}$)이 사용되며 계산 과정은 아래와 같다. \n",
    "$$\\begin{align}\n",
    "&dZ^{[l]} = dA^{[l]} * {g^{'[l]}}(Z^{[l]}) && \\text{shape: } (n^{[l]}, m) = (n^{[l]}, m) * g^{'[l]}\\left((n^{[l]}, m)\\right)\\\\\n",
    "&dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot A^{T[l-1]}       && \\text{shape: } (n^{[l]}, n^{[l-1]}) = (n^{[l]}, m) \\cdot (n^{[l-1]}, m)^T\\\\\n",
    "&db^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot 1                      && \\text{shape: } (n^{[l]}, 1) = (n^{[l]}, m) \\cdot (m, 1)\\\\\n",
    "&dA^{[l-1]} = {W^{[l]}}^T \\cdot dZ^{[l]}   && \\text{shape: } (n^{[l-1]}, m) = (n^{[l]}, n^{[l-1]})^T \\cdot (n^{[l]}, m)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "또한 이 과정을 아래 슬라이드 우측 하단의 상자 형태로 표현할 수 있다. 단순히 역전파는 $dA^{[l]}$를 입력으로 받아 $dA^{[l-1]}$를 출력으로 내놓는 과정'으로 설명된다. 이 과정에서 계산된 $dZ^{[l]}$를 이용해 실제 파라미터 수정에 사용되는 $dW^{[l]}$과 $db^{[l]}$를 구하게 된다. 그리고 출력 $dA^{[l-1]}$를 다음 레이어 역전파의 입력으로 사용해 $dA^{[l-2]}$를 구하는 식의 과정이 반복된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고로 $dW^{[l]}$를 $dz^{[l]} \\cdot a^{T[l-1]}$와 같이 계산하는 것은 아래 행렬곱과 같이 설명될 수 있고,      \n",
    "($*$는 shape이 같은 행렬의 element-wize 곱)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dL}{dW} = \\frac{dL}{dz} \\frac{dz}{dW} = \n",
    "\\begin{pmatrix}\n",
    "z^{[l]}_1 & z^{[l]}_1 & \\cdots & z^{[l]}_1 \\\\\n",
    "z^{[l]}_2 & z^{[l]}_2 & \\cdots & z^{[l]}_2 \\\\\n",
    "\\vdots    & \\vdots    & \\ddots & \\vdots \\\\\n",
    "z^{[l]}_{n^{[l]}} & z^{[l]}_{n^{[l]}} & \\cdots & z^{[l]}_{n^{[l]}} \\\\\n",
    "\\end{pmatrix} \n",
    "*\n",
    "\\begin{pmatrix}\n",
    "a^{[l-1]}_1 & a^{[l-1]}_2 & \\cdots & a^{[l-1]}_{n^{[l-1]}}\\\\\n",
    "a^{[l-1]}_1 & a^{[l-1]}_2 & \\cdots & a^{[l-1]}_{n^{[l-1]}}\\\\\n",
    "\\vdots    & \\vdots    & \\ddots & \\vdots \\\\\n",
    "a^{[l-1]}_1 & a^{[l-1]}_2 & \\cdots & a^{[l-1]}_{n^{[l-1]}}\\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "z^{[l]}_1\\\\\n",
    "z^{[l]}_2\\\\\n",
    "\\vdots\\\\\n",
    "z^{[l]}_{n^{[l]}}\\\\\n",
    "\\end{pmatrix} \n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "a^{[l-1]}_1 & a^{[l-1]}_2 & \\cdots & a^{[l-1]}_{n^{[l-1]}}\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 샘플에 대한 for-loop를 벡터화한 경우, $dW^{[l]}$를 $\\frac{1}{m} dZ^{[l]} \\cdot A^{T[l-1]}$와 같이 계산하는 것은 $m$개 샘플에 대한 $\\frac{dL}{dW}$를 구하고 이들의 평균값을 사용하는 것과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L04_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l$번째 레이어 역전파가 끝나면 $dW^{[l]}$와 $db^{[l]}$가 계산되고, 이를 이용해 아래와 같이 각 파라미터를 업데이트 한다.\n",
    "$$\n",
    "\\begin{align}\n",
    "&W^{[l]} := W^{[l]} - \\alpha ~ dW^{[l]} \\\\\n",
    "&b^{[l]} := b^{[l]} - \\alpha ~ db^{[l]} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L04_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-6]. Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 바와 같이 $l$번째 레이어의 순전파는 $A^{[l-1]}$을 입력으로 받아 $A^{[l]}$을 출력하는 과정이며, $l$번째 레이어의 역전파는 $dA^{[l]}$을 입력으로 받아 $dA^{[l-1]}$을 출력하는 과정으로 요약할 수 있었다. 이제 이것들을 실제로 어떻게 구현할 수 있을지 알아보기로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 $l$번째 레이어에서의 순전파는 아래와 같이 간단히 구현해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return np.maximum(0, Z)\n",
    "    else:  # derivative of ReLU, {1 if x>0, 0 if x <= 0}\n",
    "        return ((np.sign(Z) + 1) // 2).astype(int)\n",
    "\n",
    "\n",
    "def sigmoid(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    else:  # derivative of sigmoid, a(1-a)\n",
    "        return np.multiply(sigmoid(Z), (1 - sigmoid(Z)))\n",
    "    \n",
    "def forward_propagation(A_l_prev, W_l, b_l, g_l):\n",
    "    Z_l = b_l + np.dot(W_l, A_l_prev)\n",
    "    A_l = g_l(Z_l)\n",
    "    return A_l, Z_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_l [[ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.54535125  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.49049219]]\n",
      "Z_l [[-1.9471864  -1.99240038]\n",
      " [-0.72844526 -0.63249489]\n",
      " [ 0.54535125 -0.18506169]\n",
      " [-1.13214839 -3.4092434 ]\n",
      " [-0.60183743  0.49049219]]\n"
     ]
    }
   ],
   "source": [
    "#  TEST CODE\n",
    "np.random.seed(1)\n",
    "A_l_prev = np.random.randn(4, 2)\n",
    "W_l = np.random.randn(5, 4)\n",
    "b_l = np.random.randn(5, 1)\n",
    "A_l, Z_l = forward_propagation(A_l_prev, W_l, b_l, relu)\n",
    "print('A_l', A_l)\n",
    "print('Z_l', Z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L04_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 $l$번째 레이어에서의 역전파를 구현해보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(A_l_prev, W_l, Z_l, g_l, dA_l):\n",
    "    m = dA_l.shape[1]\n",
    "    assert (m == Z_l.shape[1])\n",
    "\n",
    "    dZ_l = np.multiply(dA_l, g_l(Z_l, is_forward=False))\n",
    "    dW_l = np.dot(dZ_l, A_l_prev.T) / m\n",
    "    db_l = np.sum(dZ_l, axis=1, keepdims=True) / m\n",
    "    dA_l_prev = np.dot(W_l.T, dZ_l)\n",
    "\n",
    "    return dA_l_prev, dW_l, db_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_l_prev [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW_l [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db_l [[-0.05729622]]\n"
     ]
    }
   ],
   "source": [
    "#  TEST CODE\n",
    "np.random.seed(2)\n",
    "dA_l = np.random.randn(1, 2)\n",
    "A_l_prev = np.random.randn(3, 2)\n",
    "W_l = np.random.randn(1, 3)\n",
    "b_l = np.random.randn(1, 1)  # Do not delete this line, or random state will be wired.\n",
    "Z_l = np.random.randn(1, 2)\n",
    "\n",
    "dA_l_prev, dW_l, db_l = backward_propagation(A_l_prev, W_l, Z_l, g_l=sigmoid, dA_l=dA_l)\n",
    "\n",
    "print('dA_l_prev', dA_l_prev)\n",
    "print('dW_l', dW_l)\n",
    "print('db_l', db_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L04_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 네트워크에 대한 학습 과정을 아래와 같이 구현해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return np.maximum(0, Z)\n",
    "    else:  # derivative of ReLU, {1 if x>0, 0 if x <= 0}\n",
    "        return ((np.sign(Z) + 1) // 2).astype(int)\n",
    "\n",
    "\n",
    "def sigmoid(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    else:  # derivative of sigmoid, a(1-a)\n",
    "        return np.multiply(sigmoid(Z), (1 - sigmoid(Z)))\n",
    "\n",
    "\n",
    "def forward_propagation(A_l_prev, W_l, b_l, g_l):\n",
    "    Z_l = b_l + np.dot(W_l, A_l_prev)\n",
    "    A_l = g_l(Z_l)\n",
    "    return A_l, Z_l\n",
    "\n",
    "\n",
    "def get_random_b_W(n_l_prev, n_l):\n",
    "    W_l = np.random.randn(n_l, n_l_prev) * 0.01\n",
    "    b_l = np.zeros((n_l, 1))\n",
    "    return b_l, W_l\n",
    "\n",
    "\n",
    "def backward_propagation(A_l_prev, W_l, Z_l, g_l, dA_l):\n",
    "    m = dA_l.shape[1]\n",
    "    assert (m == Z_l.shape[1])\n",
    "\n",
    "    dZ_l = np.multiply(dA_l, g_l(Z_l, is_forward=False))\n",
    "    dW_l = np.dot(dZ_l, A_l_prev.T) / m\n",
    "    db_l = np.sum(dZ_l, axis=1, keepdims=True) / m\n",
    "    dA_l_prev = np.dot(W_l.T, dZ_l)\n",
    "\n",
    "    return dA_l_prev, dW_l, db_l\n",
    "\n",
    "\n",
    "def get_cross_entropy_cost(Y_actual, Y_prediction):\n",
    "    assert Y_actual.shape == Y_prediction.shape\n",
    "\n",
    "    m = Y_actual.shape[1]\n",
    "    loss_positive = -1.0 * np.multiply(Y_actual, np.log(Y_prediction))\n",
    "    loss_negative = -1.0 * np.multiply((1 - Y_actual), np.log(1 - Y_prediction))\n",
    "    cost = np.sum(loss_positive + loss_negative) / m\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "class SimpleFfnn:\n",
    "    def __init__(self, layer_dims, layer_activations, random_seed=None):\n",
    "        assert len(layer_dims) == len(layer_activations)\n",
    "\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations  # 0's activation function is not used\n",
    "        self.n_L = len(layer_dims) - 1  # input layer is not counted.\n",
    "        self.cache_A = [None] * (self.n_L + 1)\n",
    "        self.cache_Z = [None] * (self.n_L + 1)  # cache_Z[0] is not used.\n",
    "        self.cache_W = [None] * (self.n_L + 1)  # cache_W[0] is not used.\n",
    "        self.cache_b = [None] * (self.n_L + 1)  # cache_b[0] is not used.\n",
    "        self.cache_dA = [None] * (self.n_L + 1)\n",
    "        self.cache_dZ = [None] * (self.n_L + 1)  # cache_dZ[0] is not used.\n",
    "        self.cache_dW = [None] * (self.n_L + 1)  # cache_dW[0] is not used.\n",
    "        self.cache_db = [None] * (self.n_L + 1)  # cache_db[0] is not used.\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.learning_rate = None\n",
    "        self.epoch = 0\n",
    "        self.costs = []\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        for i in range(1, len(self.layer_dims)):  # 1, 2, ..., n_L\n",
    "            n_l = self.layer_dims[i]\n",
    "            n_l_prev = self.layer_dims[i - 1]\n",
    "\n",
    "            self.cache_b[i], self.cache_W[i] = get_random_b_W(n_l_prev, n_l)\n",
    "\n",
    "    def forward_propagation_deep(self, X_train, Y_train):\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        assert (self.layer_dims[0] == X_train.shape[0])\n",
    "        assert (X_train.shape[1] == Y_train.shape[1])\n",
    "        self.cache_A[0] = self.X\n",
    "\n",
    "        for i in range(1, len(self.layer_dims)):  # 1, 2, ..., n_L\n",
    "            A_l_prev = self.cache_A[i - 1]\n",
    "            W_l = self.cache_W[i]\n",
    "            b_l = self.cache_b[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "\n",
    "            self.cache_A[i], self.cache_Z[i] = forward_propagation(A_l_prev, W_l, b_l, g_l)\n",
    "\n",
    "    def backward_propagation_deep(self):\n",
    "        Y_actual = self.Y\n",
    "        Y_prediction = self.cache_A[self.n_L]\n",
    "\n",
    "        self.cache_dA[self.n_L] = - (np.divide(Y_actual, Y_prediction) - np.divide(1 - Y_actual, 1 - Y_prediction))\n",
    "\n",
    "        for i in list(reversed(range(1, len(self.layer_dims)))):  # n_L, n_L-1, ..., 2, 1\n",
    "            dA_l = self.cache_dA[i]\n",
    "            A_l_prev = self.cache_A[i - 1]\n",
    "            W_l = self.cache_W[i]\n",
    "            Z_l = self.cache_Z[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "\n",
    "            self.cache_dA[i - 1], self.cache_dW[i], self.cache_db[i] = backward_propagation(A_l_prev=A_l_prev,\n",
    "                                                                                            W_l=W_l,\n",
    "                                                                                            Z_l=Z_l,\n",
    "                                                                                            g_l=g_l,\n",
    "                                                                                            dA_l=dA_l)\n",
    "\n",
    "    def update_parameters_deep(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for i in range(1, len(self.layer_dims)):\n",
    "            self.cache_W[i] -= learning_rate * self.cache_dW[i]\n",
    "            self.cache_b[i] -= learning_rate * self.cache_db[i]\n",
    "\n",
    "    def training(self, X_train, Y_train, learning_rate=0.1, num_iteration=10):\n",
    "        self.epoch += num_iteration\n",
    "\n",
    "        for i in range(num_iteration):\n",
    "            self.forward_propagation_deep(X_train, Y_train)\n",
    "\n",
    "            cost = get_cross_entropy_cost(Y_train, self.cache_A[-1])\n",
    "            self.costs.append(cost)\n",
    "\n",
    "            self.backward_propagation_deep()\n",
    "            self.update_parameters_deep(learning_rate)\n",
    "\n",
    "    def predict(self, X_test, Y_actual=None):\n",
    "        m = X_test.shape[1]\n",
    "\n",
    "        tmp_cache_A = [X_test]\n",
    "\n",
    "        for i in range(1, len(self.layer_dims)):  # 1, 2, ..., n_L\n",
    "            A_l_prev = tmp_cache_A[i - 1]\n",
    "            W_l = self.cache_W[i]\n",
    "            b_l = self.cache_b[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "\n",
    "            A_l, _ = forward_propagation(A_l_prev, W_l, b_l, g_l)\n",
    "            tmp_cache_A.append(A_l)\n",
    "\n",
    "        accuracy = None\n",
    "        Y_prediction_probability = tmp_cache_A[-1]  # last activation\n",
    "        Y_prediction = (Y_prediction_probability >= 0.5).astype(int)\n",
    "        if Y_actual is not None:\n",
    "            assert X_test.shape[1] == Y_actual.shape[1]\n",
    "            accuracy = np.sum((Y_actual == Y_prediction))/m\n",
    "\n",
    "        return Y_prediction, Y_prediction_probability, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, array([[-0.00212439, -0.01112217, -0.01382928,  0.00691154],\n",
       "        [ 0.00884851,  0.01709764,  0.00049486, -0.00404841],\n",
       "        [-0.00674801, -0.00993885,  0.01087566, -0.00915139]]), array([[-0.01535535,  0.00226026,  0.00944473]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CODE\n",
    "np.random.seed(3)\n",
    "X_train = np.random.randn(4, 2)\n",
    "Y_train = np.array([[1, 0]])\n",
    "\n",
    "s1 = SimpleFfnn([4, 3, 1], [None, relu, sigmoid])\n",
    "learning_rate = 0.01\n",
    "num_iteration = 100\n",
    "s1.training(X_train, Y_train, learning_rate, num_iteration)\n",
    "s1.cache_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L04_05.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-7]. Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 모형 학습과정은 예측 오차를 최소화 하는 $W$, $b$와 같은 파라미터(parameter)를 찾는 과정이다. 그런데 실제로 모형 학습을 위해서는 모형의 특성을 결정하는 아래와 같은 하이퍼 파라미터(hyper-parameter)를 정해야 한다. (사용하는 방법에 따라 더 많은 요소들이 있을 수 있다.)\n",
    "\n",
    "- $W$, $b$를 변화시키는 속도를 결정하는 값인 learning rate $\\alpha$\n",
    "- 동일한 데이터를 $W$와 $b$ 변화에 몇번이나 사용할 것인지(num of iteration)\n",
    "- 히든 레이어는 몇개를 사용할 것인지(num of hidden layer, $L$)\n",
    "- 각 히든 레이어에 유닛은 몇개를 사용할 것인지(num of units for each hidden layer, $n^{[1]}, ~ n^{[2]}, \\cdots, n^{[L]}$)\n",
    "- 활성 함수로는 ReLU, Sigmoid, tanh 등 어떤 함수를 사용할 것인지(activation function)\n",
    "- 최적화에 모멘텀 방법을 이용할 경우 모멘텀의 정도를 결정하는 값\n",
    "- mini-batch 크기\n",
    "- 정규화 상수            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L05_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적의 하이퍼파라미터를 갖기 위해서는 다양한 값을 테스트 해 보고 가장 좋은 성능을 보이는 값을 사용하면 된다. (아직까지는 최적의 값을 찾는 특별한 방법이 있다기 보다 경험적으로 테스트 해보고 좋은 값을 골라야 한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L05_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-8]. What does this have to do with the brain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/400px-Neuron_Hand-tuned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 혹은 딥러닝을 논할 때 모형의 학습/동작 과정이 두뇌의 그것을 모사 했다는 식의 논리가 등장하곤 한다. 이는 딥러닝의 순전파(아래 좌측 박스)가 '수상돌기에서 신호를 받아 축삭말단에 출력값을 내놓는 것'과 유사하며, 역전파는 '어떤 시행착오로부터 뉴런이 최적화 되는 과정'과 유사하다는 점 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W4L06_01.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
