{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Reward를 어떻게 디자인 할 것인가? 앞서 RL에서는 reward를 사람이 정하는 것이라 했었다. 그런데 사람이 정하는 것이 과연 최선인가? 사실 reward 디자인에 따라 전혀 다른 정책이 학습된다.          \n",
    "- IRL(Inverse Reinforcement Learning)에서는 reward를 데이터로부터 찾아내고 싶다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. MDP: optimal policy/value를 찾기 위해 transition model $P(s' \\mid s, a)$를 알아야 했다.\n",
    "2. RL: 현실에서는 transition model를 모르기 때문에 이걸 sampling으로 대체 했었다. 그런데 예를들어 자동자 운전을 학습시킨다면 위험한 주행도 트라이 해 보고 여러가지 해보고 학습시키기는 부적절하지 않은가?\n",
    "3. IRL: transition model도 모르고, reward를 그냥 세팅하는 것이 아니라 데이터로부터 최적의 reward setting을 찾아낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 보석은 1점, 불구덩이는 -1점, 나머지 상태에서의 R은 -0.01인 상황\n",
    "- expectation of sum of reward를 최대화 하고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Goal state에 도달하면 +1의 reward, hazard에 빠지면 -1의 reward(penalty)를 받게 된다. 또한 다른 상태(otherwise)에 도달하면 -0.01의 reward(penalty)를 받게 된다.\n",
    "- 이런 reward setting은 goal state에 도달해야 하고 ,hazard에는 가지 말아야 하고, 다른 상태에서 많이 배회 하지는 말라는 것이다. 그런데 이런 reward setting 밖에 없을까? reward setting에 따라서 학습되는 정책이 달라지지는 않을까?\n",
    "\n",
    "\n",
    "> - 또한 이런 세팅에서는 reward(function)을 사람이 정하고, optimal policy를 찾고 있다. 이때 optimal policy를 찾는 방법은 transition prob이 주어져 있다면 MDP를 풀 것이고, 그렇지 않다면 RL(sampling을 이용한..)을 이용해 optimal value/policy를 찾을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - otherwise의 reward를 바꿔가면서 학습시켜봤더니, optimal policy가 바뀐다. 위&좌의 경로는 모두 같으나 나머지 경로에서의 정책은 reward setting에 따라 달라진다. otherwise의 penalty가 커질수록 world 탐색을 최대할 줄이는 정책이 학습된다.극단적으로 R(otherwise)=-2.0이면 -1로도 그냥 가려하는 정책이 학습된다.\n",
    "\n",
    "> - 즉 reward setting 또한 학습되는 정책에 많은 영향을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 예를들어 주행을 학습시킨다면, 주행을 위한 다양한 feature가 존재할텐데, 차선, 속도, 앞차와의 거리, 사각지대접근등을 생각해볼 수 있다. 이때 어떤 feature를 더 중요하게 볼 것인지를 weight으로 컨트롤 하여 가중합으로 전체 reward를 설계하게 된다. 만약 $w_1$이 매우 크다면 차선만을 아주 잘 지키는 정책이 학습될 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - transition prob이 주어져 있다면 reward를 우선 세팅하고 MDP를 풀어 정책을 확인하고, reward setting을 수정하고, 다시 MDP를 풀고 정책을 확인하여 reward setting을 바꾸는 과정을 반복할 수 있을 것이다. \n",
    "- 하지만 현실은 그렇게 단순하지 않다. 우선 transition prob이 주어져 있지 않기 때문에 RL로 문제를 푼다면, 예를들어 자율 주행을 학습시킨다면 reward를 임의로 정하고, 학습을 위해 (위험할 수도 있는) 여러 형태의 주행을 테스트 하고 정책을 학습시킨 후 이 정책을 평가 하기까지 대단히 많은 비용이 들어간다. 따라서 이런 과정으로 reward setting을 변경하는 것은 거의 불가능하다. \n",
    "\n",
    "> - 따라서 보다 똑또갛게 reward를 세팅하는 방법이 필요하다. $\\rightarrow$ IRL로 접근해본다.(expert의 선택을 이용해 reward func을 구한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert는 optimal policy를 아는 사람을 의미한다. (그 대상을 정말 잘하는 사람)\n",
    "- expert가 각 상태에서 어떤 행동을 선택했는지가 expert's episode이고, 이런 episode를 모아 놓은 것이 expert's demonstrations(set of episode)이다.\n",
    "\n",
    "> - expert's demo를 주고 expert의 policy를 학습하는 것 즉 expert와 유사한 행동을 generate하는걸 Imitation Learning 혹은 Learning from demonstration이라 하고, 이 범주 안에 reward와 policy를 같이 학습하는 걸 Inverse Reinforcement Learning(IRL)이라 한다. (IRL이 Imitation Learning의 범주에 들어가는 이유는 IRL이 demonstration을 사용하기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 어떤 상태에서 어떤 행동을 할 것인가, 상태를 입력으로 하고 행동을 출력으로 하는 식의 지도학습 문제를 풀 수 있다. action이 discrete하다면 classification, conutious하다면 regression 문제가 된다. 이런 방식을 크게 **BC(Behavior Cloning)**이라 한다. robotics에서는 많이 쓴다. \n",
    "\n",
    "\n",
    "> - IRL은 demo로부터 reward func을 구하고, 이를 이용해 MDP나 RL을 풀어서 optimal policy를 구한다.\n",
    " - IRL의 기본 전제는 expert의 policy가 optimal policy가 되도록하는 reward를 찾는 것이다. 이게 무슨말이냐고? 다음에 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - BC: Find policy dist maximizing likelihood\n",
    "- IRL: Find reward function? > yes\n",
    " - Q-function이 아니라 reward function을 구한단 말이야? > yes\n",
    " - reward function을 알면 Q-function을 바로 알수있나? > 아니 reward func를 이용해 MDP나 RL을 푼다.\n",
    " - 우선 reward function을 어떻게 구하지? > 이후에 나옴\n",
    " - BC와 독립적인 것인가? > 전혀 다르다.\n",
    " - transition prob는 필요 없나? > reward를 구한후 transition prob이 있다면 MDP를, 없다면 RL을 풀 것이다.\n",
    " - reward function으로 policy dist를 어떻게 구하지? > MDP나 RL을 푼다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 표기법: $\\mathbb{E}_{\\pi_E}[r(s, a)] = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t r_t(s, a) \\mid \\pi]$\n",
    "\n",
    "\n",
    "> - (expert의 policy를 이용하는)expert's performance와 (나의 여러 policy들 중 가장 좋은 것을 이용하는)optimal policy's performance가 같아지도록 reward function을 세팅하고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 그런데 이 문제는 Ill posed problem이다. 즉 솔루션이 유일하지 않다. 예를들어 $r(s, a)=0$이면 equation이 항상 성립한다. 따라서 어떤 다른 조건이 추가로 필요하다. \n",
    "- 또한 scale problem도 있다. equation 양변에 상수를 곱해줘도 똑같이 성립현다. 이 문제를 해결하기 위해 regularization을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - scale?\n",
    "- regularization? >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Pieter Abbeel, 2004, Apprenticeship Learning via Inverse Reinforcement Learning: https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf\n",
    "> - Nathan Ratliff, 2006, Maximum Margin Planning: http://martin.zinkevich.org/publications/maximummarginplanning.pdf\n",
    "\n",
    "\n",
    "> - Brian Ziebart et el, 2008, Maximum Entropy Inverse Reinforcement Learning: https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\n",
    "\n",
    "\n",
    "> - Jonathan Ho, 2016, Generative Adversarial Imitation Learning: https://arxiv.org/abs/1606.03476"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL 분야는 크게 두가지 방향으로 발전해왔다. $\\mathbb{E}_{\\pi_E}[r(s, a)] = \\max_\\pi \\mathbb{E}_\\pi [r(s, a)]$를 푸는 건 같은데 다른 두개의 접근 방법이 있었다.\n",
    " - Large margin based: 처음 Kalman이 Inverse Optimal Control 혹은 Inverse Optimality Design이란 이름으로 이 아이디어를 제시 했다. 그 후 2000년에 Andrew Ng이 Kalman의 아이디어를 기반으로 expert의 demo를 이용해 reward를 찾는 접근 방법을 만들어 IRL이란 이름으로 소개한다. 그 후 Pieter Abbeel등 연구가 계속 된다. \n",
    " history... blabla.... ...\n",
    " - Maximum entropy: Brian Ziebard 혼자 다 함. \n",
    " \n",
    " \n",
    "> - 그런데 2년전 앞서 두 방향이 결국 많은 공통점을 갖고 있으며 이 두 접근을 더한 GAIL 방법을 제시한다. 여기에는 GAN의 아이디어가 많이 사용된다.(네가 알고 있는 그 GAN이다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\rho_{\\pi_{\\theta}}$는 시간에 대한 정보를 모두 포함하게 하는 수학적 trick으로서 최근에 특정 상태 $s$에 있을 확률을 의미한다.\n",
    "> - $P_{\\pi_{\\theta}}(S_t = s)$: 특정 time-step $t$에 특정 상태 $s$에 있을 확률, $\\mathbb{E}\\big[ \\mathbb{I}_{\\{S_t = s\\}} \\big]$와 같다.\n",
    "$$\\begin{align}\n",
    "\\rho_{\\pi_\\theta}(s) &= \\mathbb{E}\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{I}_{\\{S_t = s\\}} \\bigg] \\\\\n",
    "&= \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E} \\big[ \\mathbb{I}_{\\{S_t = s\\}} \\big] \\\\\n",
    "&= \\sum_{t=0}^\\infty \\gamma^t P_{\\pi_\\theta}(S_t = s) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 앞서 PG에서는 $1/1-\\gamma$가 $\\mathbb{E}$앞에 붙어 있었는데 이걸 뗀걸 visitation이라 부른다. $\\mathbb{I}_{S_t=s}$을 보면 알 수 있듯이 time-step $t$에 상태 $s$를 방문했으면 (discounted)1을 더해준다. 반면 방문하지 않았다면 더하는 값이 없다. 즉 특정 상태 $s$의 (discounted) 방문 빈도라 할 수 있다. (이때 $S_t$가 확률변수이므로 첫줄과 같이 전개된다.)\n",
    "- discount가 없으면 무한대가 되어 버린다. 물론 특성 time-step이후에는 그 상태를 다시 방문하지 않는다면 discount factor가 없어도 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - PG에서는 특정 상태를 방문할 빈도(state visitation) $\\rho_{\\pi_\\theta}(s)$만 생각했었는데, 이제 특성 상태에서 특정 행동을 하는 빈도(state action visitation) $\\rho_{\\pi_\\theta}(s, a)$에 대해 생각해볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $P_{\\pi_\\theta}(S_t=s, A_t=a)$: 시간 $t$에 상태 $s$에 있고 행동 $a$를 할 확률\n",
    "> - $P_{\\pi_\\theta}(S_t=s, A_t=a) = P_{\\pi_\\theta}(A_t=a \\mid S_t = s) P_{\\pi_\\theta}(S_t=s)$(bayes rule)\n",
    "> - $P_{\\pi_\\theta}(A_t=a \\mid S_t = s)$는 policy distribution $\\pi_\\theta(a \\mid s)$이다.\n",
    "- policy distribution은 시간에 따라 변하지 않으므로(업데이트 주기 내에서 겠지?..) $\\sum$ 밖으로 나가고 아래와 같이 정리된다.\n",
    "$$\\begin{align}\n",
    "\\rho_{\\pi_\\theta}(s, a) &= \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s, A_t=a) \\\\\n",
    "&= \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(A_t=a \\mid S_t = s) P_{\\pi_\\theta}(S_t=s) \\\\\n",
    "&= \\sum_t^\\infty \\gamma^t \\pi_\\theta(a \\mid s) P_{\\pi_\\theta}(S_t=s) \\\\\n",
    "&= \\pi_\\theta(a \\mid s) \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s) \\\\\n",
    "&= \\pi_\\theta(a \\mid s) \\rho_{\\pi_\\theta}(s)\n",
    "\\end{align}$$\n",
    "- 즉 특정 (state, action)에 대한 visitation은 '그 state에 대한 visitation'과 '그 state에서 특정 action을 확률(정책)'을 곱한 것과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\sum_t^{\\infty}\\gamma^t P_{\\pi_\\theta}(S_t = s) = \\rho_{\\pi_\\theta}(s)$ : discounted accumulated state  prorability at time $t$.\n",
    "- $\\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s, A_t=a) = \\rho_{\\pi_\\theta}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - policy를 정하면 state action visitation이 unique하게 정해진지고, 반대로 state action visitation을 정해놓으면 그걸 만들어내는 policy는 unique하다. (one to one 관계), (모든 state action space의 모든 (s, a) 쌍에 대한 visitation을 주어지는 것을 말한다.)\n",
    "\n",
    "\n",
    "> - 이 관계를 이용하면 '정책'을 찾는 문제를 state action visitation을 찾는 문제로 바꿀 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 아래 **RL의 objective function**의 전개중 $\\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t  P(s, a) r(s, a, s') P(s' \\mid s, a)$에서 첫 $\\sum$에서 $s, a, s'$를 고정했고, 두번째 $\\sum$은 모든 time-step에 대해 더한다. 따라서 안쪽 $\\sum$의 항들에서 $r(s, a, s')P(s' \\mid s, a)$를 묶어낼 수 있다.\n",
    "\n",
    "> $$\\begin{align}\n",
    "\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t R_t \\mid \\pi] &= \\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t r(s, a, s') P(s, a, s') \\\\\n",
    "&= \\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t r(s, a, s') P(s' \\mid s, a) P(s, a) \\\\\n",
    "&= \\sum_{s, a, s'} \\sum_{t=0}^\\infty \\gamma^t  P(s, a) r(s, a, s') P(s' \\mid s, a) \\\\\n",
    "&= \\sum_{s, a, s'} \\rho_{\\pi_\\theta}(s, a) r(s, a, s') P(s' \\mid s, a) \\\\\n",
    "&= \\sum_{s, a, s'} r(s, a, s') P(s' \\mid s, a) \\rho_{\\pi_\\theta}(s, a) \\\\\n",
    "&= \\sum_{s, a} r(s, a) \\rho_{\\pi_\\theta}(s, a) \\\\\n",
    "&= \\langle r, \\rho_{\\phi_\\theta} \\rangle\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\sum_t^{\\infty}\\gamma^t P_{\\pi_\\theta}(S_t = s) = \\rho_{\\pi_\\theta}(s)$ : discounted accumulated state  prorability at time $t$.\n",
    "- $\\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s, A_t=a) = \\rho_{\\pi_\\theta}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 결국 objective function이 (시간에 무관하게) 'state action visitation' $\\rho_{\\pi_\\theta}(s, a)$에 선형적이다.\n",
    " - 반면 정책이 바뀌는건 시간에 대해 선형적이지 않을 수 있다.\n",
    "> - IRL의 objective function은 두 벡터의 내적으로 표현됨.\n",
    "> - $\\pi$가 달라지면 $\\rho_\\pi$가 달라짐.\n",
    "-  $\\sum_{s, a, s'}r(s, a) \\rho_\\pi(s, a)$를 벡터 $r$과 벡터 $\\rho_\\pi$의 내적으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL objective function 식에서 $\\sum_{s, a, s'}$은 $\\sum_{s, a}$가 되어야 할 듯. 이건 sutton 책 38쪽을 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL에서는 왜 $P(s' \\mid s, a)$가 없지? s'에 대해 더하면 1이 된다?\n",
    "- $\\sum_{s, a, s'}r(s, a) \\rho_\\pi(s, a)$를 벡터 $r$과 벡터 $\\rho_\\pi$의 내적으로 표현했네?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 결국 objective function을 maximize하는건 $\\langle r, \\rho_\\pi \\rangle$를 최대화 하는 $\\pi$를 찾는 것과 같다. 두 벡터의 내적을 최대화 하는 policy 혹은 reward를 찾아야 함. \n",
    " - 즉 $r$이 정해져 있을 때, 여러 정책들 각각에 대응되는 $\\rho_\\pi$들 중 $r$에 투영선의 길이가 가장 긴 것이 가장 좋은 정책이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert의 policy를 $\\pi_E$라 할 때 이에 대응되는 state action visitation은 $\\rho_{\\pi_E}$이다.\n",
    " - $\\langle r, \\rho_{\\pi_E} \\rangle = \\max_\\pi \\langle r, \\rho_\\pi \\rangle$을 만족하는 $r$을 찾으려면 우선 랜덤하게 $r$을 정하고 $\\rho_{\\pi_E}$와 여러 $\\rho_{\\pi}$들을 $r$에 투영한다. 이때 $\\rho_{\\pi}$들의 투영중 가장 긴것보다 $\\rho_{\\pi_E}$의 투영이 더 길어야 한다는 조건을 만족할 때까지 $r$을 바꿔보는 것이다.\n",
    " - $\\rho_{\\pi}$ set을 위 그림의 녹색 원으로 표시한다면 $r$을 바꿔가면서 여기에 포함된 모든 $\\rho_{\\pi_E}$을 $r$에 투영해서 위 조건을 만족하는 $r$을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - BC vs IRL: IRL의 장점은 reward를 알아낼 수 있다는 것이다. reward를 다른 agent에 transfer시키면 다른 agent들도 이를 기반으로 학습을 할 수 있다. 또한 예를들어 내 차선에 전방 장애물이 있을 때 좌나 우로 피하야하는데, BC를 하면 좌/우를 smoothing하게 되서 가운데를(직진) 학습하게 된다. 반면 IRL에서는 직진은 reward가 작다고 판단하여 좌나 우에 높은 reward를 부여하여 좌/우로 피한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1; 54:00 @@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\phi: S \\rightarrow [0, 1]^k$는? a vector of features는 각 state의 feature를 0 혹은 1로 구성된 $k$차원 벡터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Pieter Abbeel's apprenticeship learning paper (https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)\n",
    "\n",
    "> - reward 함수를 linear function으로 modeling, 나중에는 NN으로 발전.\n",
    "\n",
    "> - 그런데 이 문제는 Ill posed problem이므로 해가 유일하지 않다. 그래서 추가적인 조건으로 regularization을 사용하는데, weight 절대값의 합이 1보다 작아져야한다는 조건($\\lVert w^* \\lVert_1 \\leq 1$을 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $S \\rightarrow [0, 1]^k$는 $S$가 0과 1 사이의 값을 갖는 길이가 $k$인 벡터라는 거지? >> 그냥 feature의 차원을 의미함\n",
    "- star $*$는 무슨 의미이지? transpose? >> ground truth\n",
    "- 왜 $R^*(s)$의 차원이 $k$이지? 그냥 scalar 값 아닌가? >> 차원은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "> - $w$: reward weight vector\n",
    "- $\\phi(s)$: feature vector of state $s$, $\\phi: s \\rightarrow [0, 1]^k$\n",
    "- $R(s_t) = w \\cdot \\phi(s_t)$ : reward\n",
    "\n",
    "\n",
    "> - $\\rho_\\pi(s)$: state visitation under policy $\\pi$\n",
    "$$\\rho_{\\pi_\\theta}(s) = \\mathbb{E}\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{I}(S_t = s) \\bigg] = \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s)$$\n",
    "\n",
    "> - $\\rho_\\pi(s, a)$: state action visitation under policy $\\pi$\n",
    " $$\\begin{align}\n",
    " \\rho_{\\pi_\\theta}(s, a) &= \\mathbb{E}\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{I}_{\\{S_t=s, A_t=a\\}}\\bigg]\\\\\n",
    " &= \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t = s, A_t = a)\\\\\n",
    " &=\\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(A_t = a \\mid S_t = s) P_{\\pi_\\theta}(S_t = s) \\\\\n",
    " &=\\pi(a \\mid s) \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t = s) \\\\\n",
    " &=\\pi(a \\mid s) \\rho_\\pi(s)\n",
    "\\end{align}$$\n",
    " \n",
    "> - $\\mu(\\pi)$: feature expectation\n",
    "$$\\mu(\\pi) = \\sum_s \\rho_\\pi(s) \\phi(s)$$\n",
    "\n",
    "> - value of state $s$ under policy $\\pi$\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{s\\sim D}\\big[ V^\\pi(s_0) \\big] &= \\color{blue}{\\mathbb{E} \\big[ \\sum_{t=0}^\\infty \\gamma^t} \\color{red}{R(s_t)} \\color{blue}{\\mid \\pi \\big]} \\\\\n",
    "&= \\color{blue}{\\mathbb{E} \\big[ \\sum_{t=0}^\\infty \\gamma^t} \\color{red}{w \\cdot \\phi(s_t)} \\color{blue}{\\mid \\pi \\big]} \\\\\n",
    "&= \\color{red}{w} \\cdot \\color{blue}{\\mathbb{E} \\big[ \\sum_{t=0}^\\infty \\gamma^t} \\color{red}{\\phi(s_t)} \\color{blue}{\\mid \\pi \\big]} \\\\\n",
    "&= \\color{red}{w} \\cdot \\color{blue}{\\sum_s \\rho_\\pi(s)} \\color{red}{\\phi(s)} \\\\\n",
    "&= \\color{red}{w} \\cdot \\color{green}{\\mu(\\pi)}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 결국 $R(s_t) = w \\cdot \\phi(s_t)$ 이면         \n",
    "$$\\begin{align}\n",
    "E_{s\\sim D}[V^\\pi(s_0)] &= w \\cdot \\sum_s \\rho_\\pi(s) \\phi(s) \\\\\n",
    "&= w \\cdot \\mu(\\pi)\n",
    "\\end{align}$$ 라는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $D$는 state distribution\n",
    "\n",
    "\n",
    "> - $w \\cdot \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\mid \\pi] = w \\cdot \\sum_s \\rho_{\\pi}(s) \\phi(s)$ 이 부분만 보면...\n",
    "$$\\begin{align}\n",
    "w \\cdot \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\mid \\pi] &= w \\cdot \\sum_s \\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) P_\\pi(S_t=s)\\\\\n",
    "&=w \\cdot \\sum_s \\rho_{\\pi}(s) \\phi(s) \\\\\n",
    "&\\because \\sum_t^{\\infty}\\gamma^t P_{\\pi_\\theta}(S_t = s) = \\rho_{\\pi_\\theta}(s)\n",
    "\\end{align}$$\n",
    "\n",
    "> - 이때 $\\sum_s \\rho_\\pi(s) \\phi(s) = \\mu(\\pi)$를 'Feature expectation'(=feature visitation) $\\mu(\\pi) \\in R^k$ 이라 한다. 이것은 feature vector들의 할인된 합계의 기댓값라 할 수 있다.(expected discounted accumulated feature vector)\n",
    "- 결국 reward를 linear model로 정의하면 performance도 parameter에 대해 linear해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(S_t) \\mid \\pi] 에서 \\sum_s I[S_t=s])$는 $S_t=s$일때만 값이 있으므로 $\\phi(s_t)$가 $\\phi(s)$가 됨. 이걸 $\\mathbb{E}$ 밖으로 뺄수 있다. $\\mathbb{E}$는 두번째 sum앞으로 간다.\n",
    "\n",
    "> - $D$: initial distribution of state\n",
    "\n",
    "> - 결국 performance가 parameter에 대해 linear하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert's feature expectation $\\hat{\\mu}(\\pi_E)$은 샘플링을 통해 'average of discounted feature vector'로 구하게 된다.\n",
    "\n",
    "\n",
    "> - $R(s_t) = w \\cdot \\phi(s_t)$ 이면 performance는 아래와 같으므로,      \n",
    "expert's performance는 $w \\cdot \\hat{\\mu}(\\pi_E)$가 된다.\n",
    "$$\\begin{align}\n",
    "E_{s\\sim D}[V^\\pi(s_0)] &= w \\cdot \\sum_s \\rho_\\pi(s) \\phi(s) \\\\\n",
    "&= w \\cdot \\mu(\\pi)\n",
    "\\end{align}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 오타) transpose아니고 내적임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Inverse RL step\n",
    "- expert's performance $w \\cdot \\mu(\\pi_E)$는 키우고, 내 정책중 최고 정책(my optimal policy)의 performance $\\max_j w \\cdot \\mu(\\mu_j)$ 와의 차이를 가장 크게 하고 싶다. 즉 앞에건 키우고 뒤에건 작게 할 것이다. 즉 $w$(reward vector direction)를 잘 조절해서 expert's perfoance를 가장 크게 표현하는 reward function을 찾겠다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞서 문제를 약간 다른 관점에서 보면 maximum margin 문제로 바꿀 수 있다.\n",
    "\n",
    "> - $$t = w^t \\mu(\\pi_E) - \\max_j w^t \\mu(\\pi_j) \\\\\n",
    "\\max_j w^t \\mu(\\pi_j) + t = w^t \\mu(\\pi_E)$$  SVM으로 이런 문제를 풀 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy를 하나식 try해 보고, 각 margin을 최대화 하는 w를 찾음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - x축은 demo의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - IRL은 ill pose problem이라 할 수 있다. 즉 솔루션이 하나가 아닌 문제라 할 수 있다. 앞서 다뤘던 large-margin method를 사용하더라도 feature expectation을 sampling으로 근사하기 때문에 estimation error가 있을수밖에 없다. 이런 이유들 때문에 이런 접근 방법들로 구한 솔루션에 명확하지 않은 점들이 많다. \n",
    "- 이 문제를 풀기 위한 또 다른 접근법은 'Principle of maximum entropy'를 이용한 방법이다. 이 방법은 결론적으로 feature expectation은 expert와 비슷하게 맞추고, 남는 자유도를 이용해서는 entropy가 최대가 되도록 하는 행동을 선택한다는 것이다. entropy는 분포가 uniform해질수록 커지기 때문에, expert의 선택이 제시된 부분에 대해서는 expert를 따라가지만 나머지 행동에 대해서는 확률적으로 선택하는 정책을 학습하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Maximum Entropy IRL에서는 entropy를 이용해 문제를 정의한다.\n",
    " - 우선 기본적인 entropy는 아래와 같이 정의되며\n",
    " $$h(p) = \\sum -p_i \\log p_i$$\n",
    " - 여기서 사용할 entropy는 아래와 같다. (특정 time-step의 상태 $s_t$에서 action prob distribution의 entropy)\n",
    " $$h(\\pi(\\cdot \\mid s_t)) = \\sum_i -\\pi(a_i \\mid s_t) \\log \\pi(a_i \\mid s_t)$$\n",
    " - IRL 문제의 objective function을 아래와 같이 정의하고 이것을 최대화 하는 $\\pi$를 찾아야 한다.(즉 expectation for the sum of discounted entropy를 최대화 하는 것이다.) 다만 두가지 조건이 있는데 첫번째는 $\\pi(a \\mid s)$가 확률분포라는 것이고, 두번째는 내 정책을 따랐을 때와 expert의 정책을 따랐을 때 feature expectation의 합이 같아야 한다는 것이다.\n",
    " $$H(\\pi) = \\mathbb{E}[\\sum_{t=0}^\\infty r^t h(\\pi(\\cdot \\mid s_t))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - entropy의 특성은 분포가 한쪽으로 몰리면 entropy가 낮아지고, uniform해지면 entropy가 높아진다. max entropy한다는 것은 분포를 uniform하게 한다는 것임. 그냥 uniform하게 하는 건 아니고 constraint를 추가하는데, expert's feature exteaction과 내가 학습시키는 정책의 expectation이 같아지도록 하는 조건"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - expert의 feataure 중 앞차와의 거리가 얼마라는 게 있다면, 내 정책에서도 이 조건을 맞추고 나머지 선택에 대해서는 entropy를 높이는 정책을 선택한다. 즉 거리가 동일하면 여러 방향으로 갈 수 있도록 비교적 uniform한 정책(multi modal)을 선택하게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 앞서 두가지 제약조건이 있을 때 $\\max_\\pi \\alpha H(\\pi)$를 푸는 문제를 lagrangian method를 적용하면 위와 같은 $\\min \\max$ 문제로 바뀐다.\n",
    "\n",
    "> - $\\sum_{s, a}\\mu_\\pi(s, a) = \\sum_{s, a}\\mu_E$를 $\\sum$ - $\\sum$ = 0으로 만들고, lagrangian multiplier $\\theta$를 곱해서 objective function식에 넣어줌\n",
    "\n",
    "\n",
    "\n",
    "> - 그런데 다시 보니 $\\theta^T \\mu_\\pi$가 앞서 다뤘던 weight * feature expectation와 같은 형태이므로 $\\theta$를 reward weight vector라 생각할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 그런데 $H(\\pi)$만을 최대화 하면 정말 uniform한 정책이 학습된다. 더불어 expert의 정책을 따라가야 한다는 조건도 필요하므로 아래 제약조건이 포함됨 $\\sum_{s, a}\\theta^\\intercal \\mu_\\pi(s, a) - \\sum_{s, a}\\theta^\\intercal \\mu_E (s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **recall**\n",
    "\n",
    "> - $w$: reward weight vector\n",
    "- $\\phi(s)$: feature vector of state $s$, $\\phi: s \\rightarrow [0, 1]^k$\n",
    "- $R(s_t) = w \\cdot \\phi(s_t)$ : reward\n",
    "\n",
    "\n",
    "> - $\\rho_\\pi(s)$: state visitation under policy $\\pi$\n",
    "$$\\rho_{\\pi_\\theta}(s) = \\mathbb{E}\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{I}(S_t = s) \\bigg] = \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t=s)$$\n",
    "\n",
    "> - $\\rho_\\pi(s, a)$: state action visitation under policy $\\pi$\n",
    " $$\\begin{align}\n",
    " \\rho_{\\pi_\\theta}(s, a) &= \\mathbb{E}\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{I}_{\\{S_t=s, A_t=a\\}}\\bigg]\\\\\n",
    " &= \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t = s, A_t = a)\\\\\n",
    " &=\\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(A_t = a \\mid S_t = s) P_{\\pi_\\theta}(S_t = s) \\\\\n",
    " &=\\pi(a \\mid s) \\sum_t^\\infty \\gamma^t P_{\\pi_\\theta}(S_t = s) \\\\\n",
    " &=\\pi(a \\mid s) \\rho_\\pi(s)\n",
    "\\end{align}$$\n",
    " \n",
    "> - $\\mu(\\pi)$: feature expectation\n",
    "$$\\mu(\\pi) = \\sum_s \\rho_\\pi(s) \\phi(s)$$\n",
    "\n",
    "> - value of state $s$ under policy $\\pi$\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{s\\sim D}\\big[ V^\\pi(s_0) \\big] &= \\color{blue}{\\mathbb{E} \\big[ \\sum_{t=0}^\\infty \\gamma^t} \\color{red}{R(s_t)} \\color{blue}{\\mid \\pi \\big]} \\\\\n",
    "&= \\color{blue}{\\mathbb{E} \\big[ \\sum_{t=0}^\\infty \\gamma^t} \\color{red}{w \\cdot \\phi(s_t)} \\color{blue}{\\mid \\pi \\big]} \\\\\n",
    "&= \\color{red}{w} \\cdot \\color{blue}{\\mathbb{E} \\big[ \\sum_{t=0}^\\infty \\gamma^t} \\color{red}{\\phi(s_t)} \\color{blue}{\\mid \\pi \\big]} \\\\\n",
    "&= \\color{red}{w} \\cdot \\color{blue}{\\sum_s \\rho_\\pi(s)} \\color{red}{\\phi(s)} \\\\\n",
    "&= \\color{red}{w} \\cdot \\color{green}{\\mu(\\pi)}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 빨간 박스를 잘 보면... policy에 대해 최대화 한다.         \n",
    "이 부분만 보면 \n",
    " - $\\theta$는 (reward)weight vector이고\n",
    " - 내 정책을 다랐을 때 feature expectation $\\mu$와 곱해서 performance가 된다.\n",
    " - 참고로 (reward)weight vector $w$에 feature vector $\\phi$를 곱하면 reward가 되고, (reward)weight vector $w$에 feature expectation $\\mu$를 곱하면 performance가 된다. 또한 feature expectation은 visitation $\\rho$와 feature vector $\\phi$를 곱한 것이다.\n",
    " \n",
    " - , 이고 ?sms feature expectation이고, feature exp는 \\phi * \\rho이고, 이건 ?? 기댓값이다. 즉 MDP를 푸는 것과 같아진다.(expected sum of reward)\n",
    "- 이런 문제는 soft MDP라 함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 우리가 처음 배웠던 MDP는 entropy H 가 없었다.\n",
    "- entropy가 추가된 MDP를 soft mdp이고, 여기에 대응되는 bellman eq를 soft bellman eq라 한다.\n",
    "- 그리고 이때의 policy는 q 값의 softmax가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - soft MDP를 풀면 $\\max_\\pi$가 해결되고, $\\theta$에 대한 부분은 $\\theta$로 미분해서 gradient descent로 푼다. 전체 항이 theta에 대해 linear하다. \n",
    "- $\\theta$에 대한 gradient를 구해서 $\\theta$를 업데이트(reward setting이 바뀜)하고, 이 결과를 이용해 다시 안쪽의 $\\max_pi$를 soft MDP로 푼다. 이 결과로 다시 $\\min_\\theta$를 풀고 이 과정을 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-a/week08-a-33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 좌상: expert의 policy가 주어지고, 나의 랜덤한 현재 정책이 존재한다.\n",
    "- (1): 위와 별개로 랜덤하게 설정한 reward function($\\theta$)을 설정한다.\n",
    "- (2), 좌하: 이 reward function이 주어졌을 때 soft MDP를 (현재 최적) 정책을 구한다.\n",
    "- (3), 우하: 위에서 갱신된 정책으로 visitation을 구하고, 이를 이용해 reward function을 갱신한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> path위의 숫자는 cost를 의미한다. 즉 expert's path는 cost가 낮고, 내 path는 cost가 높다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2, 19:37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: dashed rgb(0,255,0) 1.0px;background-color: rgb(170,0,0);height: 5.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 게일? 가일?\n",
    "- 앞서 두 방법이 결국 같은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2, 23:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - margin based IRL에서 두번째 식으로 넘어올때 부호가 바뀌면서 max가 min으로 바뀜. 첫 식 안쪽 max는 policy에 대한 것이고, 밖으로 나올 수 있음.\n",
    "\n",
    "> - 두 개념이 서로 대응됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - MB, ME의 제약조건이 서로 다름.\n",
    "- Unified framework에서는 두 조건을 모두 포함시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> policy에 대한 max를 먼저 품. 그리고 theta에 대한 gradient를 구하고 이 과정을 반복\n",
    "- 그런데 매번 MDP를 풀어야 하므로 매번 RL을 푸는게 부담됨. (tracjectory를 한번 sampling 해야하고, policy를 구하고 theta를 업데이트 해야한다. )\n",
    "- traj를 뽑아서 theta도 policy도 같이 update 하고 싶다. 즉 policy를 아주 많이 update해야하고 sampl이 너무 많이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - discriminator는 classification문제를 풀고 있고, generator는 random vector로부터 진짜 같은 vector를 만드는 문제를 풀고 있음\n",
    "- generator의 목표는 discriminant score가 1이 되도록 G를 만듬. \n",
    "- discimi는 진짜면 1 가짜면 0을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - p_data는 진짜 데이터\n",
    "- 진짜 데이터에는 1을 주도록 학습\n",
    "- p_z는 generator, 1을 주도록 하겠다. 뒤 항을 최소화 하려면 1이 되어야 함.\n",
    "- D는 discriminator\n",
    "\n",
    "\n",
    "\n",
    "> - 진짜 데이터는 검은 선, 초록색은 genetator, 파란점선은 discrimi이다. real과 같은 영역에서는 높은 discr score가 나오고, 아닌 영역에서는 낮은 score를 주고 있다.\n",
    "- 파라미터를 업데잇 할 때마다 점차 generator의 출력이 real data와 같아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GAN과 IRL의 공통점. 구조.\n",
    "- Reward : Discriminator\n",
    "- Policy : Generator\n",
    "\n",
    "- matching안되는건 inside, outside가 다름. 이걸 바꾸고 싶은데 말이야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - geneator min, discr max\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - max, min 문제는 안장에서 saddle point가 하나만 있으면 min, max의 순서가 상관 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 좌측은 policy net\n",
    "- 우측은 discri\n",
    "\n",
    "> - score를 할당하고, 경로에 대해 점수를 할당해준다. 경로에 대한 점수가 있으므로 이걸로 policy를 학습시킨다. 이때는 trpo나 ppo를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 요즘에는 TRPO대신 PPO를 많이 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/fastcampus/week08-b/week08-b-13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - BC와 비교함.\n",
    "- x축은 demo sample 의 갯수인데, BC는 sampl이 많아지면 성능이 좋아진다. 그런데 데이터가 적으면 잘 안된다. \n",
    "- 반면 게일은 데이터가 적어도 잘 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 실습 - maximum entropy irl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - soft MDP를 푸는 부분 : Softmax and Log-Sum-Exp\n",
    "- numerical stability를 위해 max를 빼줌(def logsumexp, def softmax)\n",
    "- def soft_value_iteration: soft MDP를 풀어줌. 왜 soft이냐면.. bellman backup operator가 조금 다름.(Soft Bellman backup operator)\n",
    " - max 대신 log-sum을 사용함.\n",
    " - MDP 풀어서 optinal policy를 구함(expert demo 역할을 할 녀석임)\n",
    " - MDP를 풀어 Q를 구하고 policy를 구할 때는 max가 아니라 softmax가 사용됨(soft MDP의 특성)\n",
    "- State visitation의 E[]를 실제 구할일은 없고, 현실에서는 sampling으로 구함.\n",
    " - 어떤 상태에 오는 방법은 그 상태에서 시작하거나 다른 상태에서 전이되서 오는 경우가 있다.\n",
    " $$d(s)+\\gamma\\sum_{s',a'}P(s|s',a')\\rho_{\\pi}(s',a')$$\n",
    "- maximum entropy 방법에서는 state visitation을 구해야 함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Maximum Entropy Inverse Reinforcement Learning\n",
    " - step2: \\max_pi \\theta^T \\mu(\\pi)를 계산한 것임.\n",
    " \n",
    "> - transition prob가 필요하다는 문제\n",
    "- continuous action을 풀고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 - maximum_entropy_irl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define Frozen Lake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from custum_frozen_lake_env import CustumFrozenLakeEnv  # what was changed?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_space: Discrete(64) \n",
      "n_state: 64 \n",
      "act_space: Discrete(4) \n",
      "n_act: 4\n"
     ]
    }
   ],
   "source": [
    "env = CustumFrozenLakeEnv(map_name=\"8x8\")\n",
    "\n",
    "obs_space = env.observation_space\n",
    "n_state = obs_space.n\n",
    "\n",
    "act_space = env.action_space\n",
    "n_act = act_space.n\n",
    "\n",
    "print('obs_space:', obs_space,\n",
    "      '\\nn_state:', n_state,\n",
    "      '\\nact_space:', act_space, \n",
    "      '\\nn_act:', n_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_value_iteration(env,rewards=None,gamma=0.99,epsilon=1e-6,scale=1e-5):\n",
    "    obs_space = env.observation_space\n",
    "    n_state = obs_space.n\n",
    "    act_space = env.action_space\n",
    "    n_act = act_space.n\n",
    "    \n",
    "    P = np.zeros((n_state, n_act, n_state))\n",
    "    r = np.zeros((n_state, n_act, n_state))\n",
    "    \n",
    "    for s in env.unwrapped.P.keys():  # for all states\n",
    "        for a in env.unwrapped.P[s].keys():  # for all actions\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]:  # for all next states\n",
    "                P[s][a][next_s] = prob\n",
    "                if rewards is None:  # if there is no given reward matrix\n",
    "                    r[s][a][next_s] = reward\n",
    "                else:\n",
    "                    r[s][a][next_s] = rewards[s][a][next_s]  # if is it, use env's reward setting\n",
    "    \n",
    "    value = np.random.uniform(size=(n_state,))  # state value function\n",
    "    \n",
    "    while True:\n",
    "        q = np.sum(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = np.sum(\n",
    "            (\n",
    "                r + gamma * np.tile(\n",
    "                    value[np.newaxis,np.newaxis,:], reps=(n_state,n_act,1)\n",
    "                )\n",
    "            ) * P,axis=2\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3639874 ,  0.82623967,  0.82577718])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(size=(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{P} \\in \\mathbb{R}^{n_s \\times n_a \\times n_s}$$\n",
    "$$\\mathbf{r} \\in \\mathbb{R}^{n_s \\times n_a \\times n_s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- env.unwrapped.P : Dict(state: Dict(action: List(transitionP, nextState, reward, done))) \n",
    "\n",
    "\n",
    "{0: // starting state\n",
    "         {0: // action\n",
    "             [\n",
    "                 (0.3333333333333333 // trantion probability\n",
    "                 , 0 // nest state\n",
    "                 , 0 // reward\n",
    "                 , False // done\n",
    "                 ),\n",
    "             (0.3333333333333333, 0, 0, False),\n",
    "             (0.3333333333333333, 8, 0, False)],\n",
    "         1: [(0.3333333333333333, 0, 0, False),\n",
    "             (0.3333333333333333, 8, 0, False),\n",
    "             (0.3333333333333333, 1, 0, False)],\n",
    "         2: [(0.3333333333333333, 8, 0, False),\n",
    "             (0.3333333333333333, 1, 0, False),\n",
    "             (0.3333333333333333, 0, 0, False)],\n",
    "         3: [(0.3333333333333333, 1, 0, False),\n",
    "             (0.3333333333333333, 0, 0, False),\n",
    "             (0.3333333333333333, 0, 0, False)]},\n",
    "     \n",
    "1: // starting state \n",
    "         {0: // action\n",
    "             [(0.3333333333333333, 1, 0, False),\n",
    "             (0.3333333333333333, 0, 0, False),\n",
    "             (0.3333333333333333, 9, 0, False)],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logsumexp(x, scale = 1):\n",
    "    x = np.array(x)/scale\n",
    "    max_x = np.max(x,axis=1,keepdims=True)\n",
    "    lse_x = max_x[:,0] + np.log(np.exp(x-max_x).sum()) # Numerical Stability\n",
    "    lse_x = scale*lse_x\n",
    "    return lse_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array([[1, 2], [3, 4]]), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2], [3, 4]])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- demo 데이터의 비율이 너무 높으면 accuracy가 너무 높게 나온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy, val_func은 PPO\n",
    "- rew_func은 discrimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define three networks\n",
    "policy = GaussPolicy(obs_dim, act_dim, epochs=10, hdim=64, lr=3e-4, clip_range=0.2, seed=seed)\n",
    "val_func = Value(obs_dim, epochs=20, hdim=32, lr=1e-3, seed=seed)\n",
    "rew_func = Reward(obs_dim, act_dim, epochs=10, hdim=32, lr=1e-4, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
