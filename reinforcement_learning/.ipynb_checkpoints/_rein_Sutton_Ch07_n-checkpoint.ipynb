{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter7. $n$-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bootstrap이란 표본에서 표본을 뽑아 어떤 지표를 더 잘 알아내는 것\n",
    "\n",
    "- Bootstrap은 1979년 Bradley Efron의 논문(Bootstrap methods: another look at the jackknife)에서 소개된 방법으로서 표본에서 표본을 복원 추출(resampling with replacement)하여 모집단의 어떤 모수 추정량에 대한 특성(bias, variance, ci, ...)을 알아내는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 강화학습에서의 Bootstrap은 현재 얻어진 에피소드(혹은 time-step 단위로)를 이용해 $v_{\\pi}(s_t)$ 혹은 $q_{\\pi}(s_t, a_t)$를 추정하기 할때 이전 에피소드로 학습된(혹은 초기에 무작위로 설정된) $V(S_t)$ 혹은 $Q(S_t, A_t)$를 다시 이용하는 것을 말한다. 에피소드가 끝날때까지 기다리지 않고 에피소드의 일부정보만을 이용해 $V(S_t)$ 또는 $Q(S_t, A_t)$를 업데이트하는 DP와 TD가 이에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주의할점은 강화학습에 사용하는 '샘플링'과 구분해야한다. 강화학습에서의 '샘플링'은 에피소드를 추출하고 이를 이용해 학습을 진행하는 것으로서 MC와 TD가 이에 해당한다.(물론 Sarsa와 Q-learning도 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 이번장에서는 n-step TD를 다룬다.\n",
    "\n",
    "- 이번장에서는 MC와 TD(0)를 일반화한 방법인 **n-step TD**를 다룬다. 즉 MC와 TD(0)는 n-step TD의 특별한 경우이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TD(0)의 문제점은 time-step의 굴레에 얽매인다는 것이다.\n",
    "\n",
    "- TD(0)의 문제점은 'action을 수정하는 주기'와 'bootstrapping하는 주기'가 모두 한 time-step으로 같다는 점이다. 물론 action은 그때 그때 빠르게 수정하는 것이 좋지만 bootstrapping의 경우 state에 유의미한 변화가 있을 때 하는 것이 그 성능에 좋다.\n",
    "\n",
    "\n",
    "- n-step TD를 이용하면 'action을 수정하는 주기'와 'bootstrapping하는 주기'를 달리 가져갈 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 $n$-step TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (비교) MC Prediction은 에피소드를 끝까지 보고 업데이트 한다.\n",
    "\n",
    "- **MC Prediction**은 각 에피소드의 마지막 상태까지의 반환값(return, $G_t$)을 타겟으로 $V(S_t)$를 업데이트 한다. 이때 $G_t$는 첫 보상($R_{t+1})$부터 마지막 보상($R_T$)까지 점점 더 할인된 값들의 합이다.\n",
    "\n",
    "$$\\begin{align}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha \\bigg[ G_t - V(S_t) \\bigg] \\\\\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + r^{T-t-1} R_T \\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/p76_mc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (비교) one-step TD Prediction은 한개의 time-step만 보고 업데이트 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 반면 **one-step TD Prediction**혹은 TD(0)는 상태 $S_t$의 보상($R_{t+1}$)과     \n",
    "$\\gamma$만큼 할인된 다음 상태 $S_{t+1}$의 가치 추정치($\\gamma V(S_{t+1})$)를 더한 값을 타겟으로           \n",
    "$v_{\\pi}(S_t)$에 대한 추정치 $V(S_t)$를 업데이트 한다.\n",
    "\n",
    "$$\\begin{align}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha \\bigg[ G_t - V(S_t) \\bigg] \\\\\n",
    "G_t &\\doteq R_{t+1} + \\gamma V(S_{t+1}) \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/p96_td0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### n-step TD Prediction는 적절히 n개를 보고 업데이트 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD(0)에서 하나의 다음 상태를 보고 업데이트 했다면 **n-step TD Prediction**은 $n$개의 다음 상태를 보고 업데이트 하겠다는 것이다. 예를들어 2-step TD에서는 아래와 같이 $V(S_t)$업데이트 한다. <font color='grey'>$$V(S_t) \\leftarrow V(S_t) + \\alpha \\bigg[ R_{t+1} + \\gamma R_{t+1} + \\gamma^2 V(S_{t+2}) - V(S_t)\\bigg]$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n-step TD를 표현하기 위해 몇가지 표기법을 도입한다.     \n",
    " - $G_{t:t+n}$: $t$시점에 구한 반환값(return)으로서 $R_{t+1}$부터 $R_{t+n}$까지는 에피소드 관측치로 구하고 그 이후의 보상은 $V_{t+n}(S_t)$일 것이라 생각하고 계산한 값이다.(이때 각 단계의 보상은 마다 $\\gamma^{n-1}$만큼 discount하고 $V_{t+n}(S_t)$은 $\\gamma^n$만큼 discount한다.)\n",
    " - $V_{t+n}(S_{t+n})$: $t+n$시점에 추정한 상태($S_{t+n}$)의 가치\n",
    " - $V_{t+n}(S_t)$: $t+n$시점에 추정한 상태($S_t$)의 가치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 표기법을 이용해 n-step TD의 업데이트를 표현하면 아래와 같다.\n",
    "\n",
    "$$\\begin{align}\n",
    "V_{t+n}(S_t) &\\doteq V_{t+n-1}(S_t) + \\alpha \\bigg[ G_{t:t+n} - V_{t+n-1}(S_t)\\bigg] , ~~~~~~~ 0 \\leq t < T \\\\\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  error reduction property: n-step return의 기댓값은 $v_{\\pi}$에 대한 좋은 추정치이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 업데이트 식에서 n-step return $G_{t:t+n}$의 기댓값과 $V_{t+n-1}(S_t)$은 모두 $v_{\\pi}$에 대한 추정치로서, 업데이트를 통해 $V_{t+n}(S_t)$이 $v_{\\pi}$으로 수렴하기 위해서는 $G_{t:t+n}$의 기댓값이 $V_{t+n-1}(S_t)$ 보다 나은 $v_{\\pi}$에 대한 추정치이어야만 한다.\n",
    "\n",
    "\n",
    "- 다행히 $G_{t:t+n}$의 기대값과 $v_{\\pi}$의 차이는 $V_{t+n-1}(S_t)$과 $v_{\\pi}$의 차이와 $\\gamma^n$의 곱 보다 작거나 같다. 즉 $G_{t:t+n}$의 기대값이 $V_{t+n-1}(S_t)$보다 나은 $v_{\\pi}$에 대한 추정치이다.\n",
    "\n",
    "$$\\max_s \\bigg| \\mathbb{E}_{\\pi} \\big[ G_{t:t+n} \\mid S_t = s\\big] - v_{\\pi}(s) \\bigg| \\leq \\gamma^n \\max_s \\bigg| V_{t+n-1}(s) - v_{\\pi}(s) \\bigg|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TD(0)와 MC의 중간에 위치한  backup-diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-step TD부터 n-step TD, MC의 backup-diagram은 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/figure_7_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-step TD\n",
    "$$\\begin{align}\n",
    "V_{t+1}(S_t) &\\doteq V_{t}(S_t) + \\alpha \\bigg[ G_{t:t+1} - V_{t}(S_t) \\bigg] \\\\\n",
    "G_{t:t+1} &\\doteq R_{t+1} + \\gamma V_{t}(S_{t+1}) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2-step TD\n",
    "$$\\begin{align}\n",
    "V_{t+2}(S_t) &\\doteq V_{t+1}(S_t) + \\alpha \\bigg[ G_{t:t+2} - V_{t+1}(S_t) \\bigg] \\\\\n",
    "G_{t:t+2} &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V_{t+1}(S_{t+2}) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-step TD\n",
    "$$\\begin{align}\n",
    "V_{t+3}(S_t) &\\doteq V_{t+2}(S_t) + \\alpha \\bigg[ G_{t:t+3} - V_{t+2}(S_t) \\bigg] \\\\\n",
    "G_{t:t+3} &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 V_{t+2}(S_{t+2}) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example 7.1 - n값에 따라 업데이트되는 $V$혹은 $Q$의 범위가 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림과 같은 5개의 상태가 있고, C에서 출발하는 환경에서 'C 0 D 0 E 1'라는 에피소드를 가정할 때, n-step TD 업데이트는 n에 따라 어떻게 다를까?     \n",
    "(초기값은 $V_0(A) = V_0(B) = V_0(C) = V_0(D) = V_0(E) = 0.5$ 이다.)\n",
    "\n",
    "\n",
    "![Local image](./images/sutton/example_6_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '1-step TD'는 $V(E)$만 업데이트 한다.\n",
    "\n",
    "> - $t=0$\n",
    " - 에피소드 : C 0 D\n",
    " - $V_0(A) = V_0(B) = V_0(C) = V_0(D) = V_0(E) = 0.5$\n",
    " - $S_0:C$, $~~S_1:D$\n",
    " - $R_1 = 0$\n",
    " - $G_{0:1} = R_{1} + V_0(S_1:D) = 0 + 0.5$\n",
    " - $V_1(S_0:C) = V_0(S_0:C) + \\alpha \\bigg[ G_{0:1} - V_0(S_1:D) \\bigg] = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $t=1$\n",
    " - 에피소드 : C 0 D 0 E\n",
    " - $V_1(A) = V_1(B) = V_1(C) = V_1(D) = V_1(E) = 0.5$\n",
    " - $S_0:C$, $~~S_1:D$, $~~S_2:E$\n",
    " - $R_2 = 0$\n",
    " - $G_{1:2} = R_{2} + V_1(S_2:E) = 0 + 0.5$\n",
    " - $V_2(S_1:D) = V_1(S_1:D) + \\alpha \\bigg[ G_{1:2} - V_1(S_1:D) \\bigg] = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $t=2$\n",
    " - 에피소드 : C 0 D 0 E 1 T\n",
    " - $V_2(A) = V_2(B) = V_2(C) = V_2(D) = V_2(E) = 0.5$, $V_2(T)=0$\n",
    " - $S_0:C$, $S_1:D$, $S_2:E$, $S_3:T$\n",
    " - $R_3=1$\n",
    " - $G_{2:3} = R_{3} + V_2(S_3:T) = 1 + 0$\n",
    " - $V_3(S_2:E) = V_2(S_2:E) + \\alpha \\bigg[ G_{2:3} - V_2(S_:E) \\bigg] = 0.5 + \\alpha \\bigg[ 1 - 0.5 \\bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '2-step TD'는 $V(D)$와 $V(E)$를 업데이트 한다.\n",
    "\n",
    "> - $t=0$\n",
    " - 에피소드 : C 0 D\n",
    " - $V_0(A) = V_0(B) = V_0(C) = V_0(D) = V_0(E) = 0.5$\n",
    " - $S_0:C$, $~~S_1:D$\n",
    " - $R_1 = 0$\n",
    " - $\\text{No update}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $t=1$\n",
    " - 에피소드 : C 0 D 0 E\n",
    " - $V_1(A) = V_1(B) = V_1(C) = V_1(D) = V_1(E) = 0.5$\n",
    " - $S_0:C$, $~~S_1:D$, $~~S_2:E$\n",
    " - $R_1 = 0$, $~~R_2 = 0$\n",
    " - $G_{0:2} = R_{1} + R_{2} + V_1(S_2:E) = 0 + 0 + 0.5$\n",
    " - $V_2(S_0:C) = V_1(S_0:C) + \\alpha \\bigg[ G_{0:2} - V_1(S_0:C) \\bigg] = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $t=2$\n",
    " - 에피소드 : C 0 D 0 E 1 T\n",
    " - $V_2(A) = V_2(B) = V_2(C) = V_2(D) = V_2(E) = 0.5$, $V_2(T)=0$\n",
    " - $S_0:C$, $S_1:D$, $S_2:E$, $S_3:T$\n",
    " - $R_1 = 0$, $~~R_2 = 0$, $~~R_3=1$\n",
    " - $G_{1:3} = R_{2} + R_{3} + V_2(S_3:T) = 0 + 1 + 0$\n",
    " - $V_3(S_1:D) = V_2(S_1:D) + \\alpha \\bigg[ G_{1:3} - V_2(S_1:D) \\bigg] = 0.5 + \\alpha \\bigg[ 1 - 0.5 \\bigg]$\n",
    " - $G_{2:4} = R_{3} = 1$\n",
    " - $V_3(S_2:E) = V_2(S_2:E) + \\alpha \\bigg[ G_{2:4} - V_2(S_2:E) \\bigg] = 0.5 + \\alpha \\bigg[ 1 - 0.5 \\bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '3-step TD'는 $V(C)$, $V(D)$, $V(E)$를 업데이트 한다.\n",
    "\n",
    "> - $t=0$\n",
    " - 에피소드 : C 0 D\n",
    " - $V_0(A) = V_0(B) = V_0(C) = V_0(D) = V_0(E) = 0.5$\n",
    " - $S_0:C$, $~~S_1:D$\n",
    " - $R_1 = 0$\n",
    " - $\\text{No update}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $t=1$\n",
    " - 에피소드 : C 0 D 0 E\n",
    " - $V_1(A) = V_1(B) = V_1(C) = V_1(D) = V_1(E) = 0.5$\n",
    " - $S_0:C$, $~~S_1:D$, $~~S_2:E$\n",
    " - $R_1 = 0$, $~~R_2 = 0$\n",
    " - $\\text{No update}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $t=2$\n",
    " - 에피소드 : C 0 D 0 E 1 T\n",
    " - $V_2(A) = V_2(B) = V_2(C) = V_2(D) = V_2(E) = 0.5$, $V_2(T)=0$\n",
    " - $S_0:C$, $S_1:D$, $S_2:E$, $S_3:T$\n",
    " - $R_1 = 0$, $~~R_2 = 0$, $~~R_3=1$\n",
    " - $G_{0:3} = R_1 + R_{2} + R_{3} + V_2(S_3:T) = 0 + 1 + 0$\n",
    " - $V_3(S_0:C) = V_2(S_0:C) + \\alpha \\bigg[ G_{0:3} - V_2(S_0:C) \\bigg] = 0.5 + \\alpha \\bigg[ 1 - 0.5 \\bigg]$\n",
    " - $G_{1:4} = R_{2} + R_{3} + 0 + 0 = 1$\n",
    " - $V_3(S_1:D) = V_2(S_1:D) + \\alpha \\bigg[ G_{1:3} - V_2(S_1:D) \\bigg] = 0.5 + \\alpha \\bigg[ 1 - 0.5 \\bigg]$\n",
    " - $G_{2:5} = R_{3} = 0 + 1 + 0 + 0 = 1$\n",
    " - $V_3(S_2:E) = V_2(S_2:E) + \\alpha \\bigg[ G_{2:4} - V_2(S_2:E) \\bigg] = 0.5 + \\alpha \\bigg[ 1 - 0.5 \\bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 가장 좋은 $n$값은? 절절히 잘 선택..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 random walk는 상태가 5개 인데, 상태가 19개인 경우로 확장하고 10개 에피소드가 주어졌을 때, 서로 다른 $n$과 $\\alpha$에 따른 RMS(Root mean square) error를 살펴보면 아래 그래프와 같다. 즉 $\\alpha$를 잘 조절한다는 가정하에 좌측의 $n$이 큰 경우는 MC라 생각할 수 있고 우측의 $n=1$인 경우는 TD(0)라고 볼 수 있다. 즉 적절한 $n$을 선택한 경우 최적의 성능을 낼 수 있다. (이 예에서는 $\\alpha$를 잘 정했다고 가정할 때 n=4가 가장 좋다.)\n",
    "※ $x_{\\text{RMS}} \\doteq \\sqrt{\\frac{x_1^2 + x_2^2 + \\cdots + x_n^2}{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/figure_7_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 $n$-step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 7.1에서는 n-step TD Prediction을 다뤘고, 이제 Control 문제에 대해 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (비교) 그냥 Sarsa는? On-policy one-step TD control이다.\n",
    "\n",
    "- (6장을 회상해보면) Sarsa는 \n",
    "\n",
    "\n",
    "1. 'On-policy TD(0) control'이다. 즉 이전 시점에 도달한 $\\big(S_t, A_t\\big)$에서 시작하여 바로 보상 $R_{t+1}$을 얻고, 다음 상태 $S_{t+1}$에 도달한다. 다음에 취할 행동 $A_{t+1}$은 현재의 정책 $\\pi_t$를 기준으로 $\\epsilon$-greedy로 결정한다. 이때의 상태 $S_{t+1}$과 선택한 행동 $A_{t+1}$에 해당하는 (이전 시점에 추정한)$Q$함수값 $Q(S_{t+1}, A_{t+1})$을 얻을 수 있다. \n",
    "- 이렇게 구한 $R_{t+1}$과 $Q(S_{t+1}, A_{t+1})$를 이용해 아래와 같이 $Q(S_t, A_t)$를 업데이트 한다.\n",
    "$$Q_{t+1}(S_t, A_t) \\leftarrow Q_t(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t)\\bigg]$$\n",
    "- 이 과정을 각 에피소드에 대해 매 time-step마다 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Backup-diagram for Sarsa(0)\n",
    "\n",
    "![Local image](./images/sutton/sarsa1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### n-step Sarsa는 n개의  S.A.R.S.A 쌍을 보고 나서 업데이트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-step Sarsa에서는 $\\big( S_t, A_t, R_t, S_{t+1}, A_{t+1}\\big)$마다 $Q_(S_t, A_t)$를 업데이트 했다면,      \n",
    "n-step Sarsa에서는 $\\big( S_t, A_t, R_t, S_{t+1}, A_{t+1}, \\cdots, S_{t+n-1}, A_{t+n-1}, R_{t+n-1}, S_{t+n}, A_{t+n}\\big)$ 마다 $Q_(S_t, A_t)$를 업데이트 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "Q_{t+n}(S_t, A_t) &\\doteq Q_{t+n-1}(S_t, A_t) + \\alpha \\bigg[ G_{t:t+n} - Q_{t+n-1}(S_t, A_t)\\bigg], ~~ 0 \\leq t < T \\\\\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}), ~~ n \\geq 1, ~ 0 \\leq t < T-n \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backup-diagram up to n-step Sarsa, MC and n-step Expected Sarsa\n",
    "\n",
    "![Local image](./images/sutton/figure_7_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 7.4 : 1-step Sarsa에 비해 n-step Sarsa가 더 잘 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 목표 $G$를 찾아가는 문제로서 어떤 에피소드가 관측되었는데, 가는 모든 과정에서는 보상이 없었고, 마지막에 $G$에 도달하는 행동에만 아주 큰 보상이 있었다고 하자.\n",
    "- 가운데 그림은 one-step Sarsa를 이용해 각 위치의 action-value를 업데이트 했을 때의 상황으로서, 마지막 action-value만 강화된다.\n",
    "- 오른쪽 그림은 10-step Sarsa를 이용한 경우로서 10개의 action-value가 강화되었다. 즉 한 에피소드로 더 많은 action-value가 정보를 얻게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/figure_7_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### n-step Expected Sarsa : Q-learning의 Max를 기댓값으로 바꾸면 Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우선 6장을 회상해보면 **Q-learning**은 $S_t$에서 시작하여 현재 정책 $\\pi_t$를 이용해 $\\bigg( S_t, A_t, R_{t+1}, S_{t+1}\\bigg)$를 생성하고, $S_{t+1}$에서 가능한 $Q(S_{t+1}, a)$들 중 가장 큰 값($\\max_a Q(S_{t+1}, a)$)을 이용해 $Q(S_t, A_t)$를 업데이트 한다.\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma \\max_{\\alpha} Q(S_{t+1}, a) - Q(S_t, A_t) \\bigg]$$\n",
    "$S_{t+1}$에서 $A_{t+1}$을 선택하는 것이 greedy가 아니라 $\\epsilon$-greedy라면      \n",
    "실제로 $S_{t+1}$에서 할 행동 $A_{t+1}$과      \n",
    "$Q(S_t, A_t)$를 업데이트할 때 사용한 행동 $\\text{argmax}_a Q(S_{t+1}, a)$이 다를 수 있기 때문에 off-policy이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-learning에서는 $\\max_a Q(S_{t+1}, a)$를 이용해 타겟을 잡았는데, $Q(S_{t+1}, a)$의 최대값이 아니라     \n",
    "기댓값 $\\mathbb{E} \\big[Q(S_{t+1}, A_{t+1}) \\mid S_{t+1} \\big]=\\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a)$을 이용하는 방법이 **Expected Sarsa**이다.       \n",
    "$$\\begin{align}\n",
    "Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma \\mathbb{E} \\big[Q(S_{t+1}, A_{t+1}) \\mid S_{t+1} \\big] - Q(S_t, A_t) \\bigg] \\\\\n",
    "&\\leftarrow Q(S_t, A_t) + \\alpha \\bigg[ R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \\bigg]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expected Sarsa를 on-policy로 사용하면 평균적으로 Sarsa와 같아진다.\n",
    "- Expected Sarsa를 off-policy 특히 학습 정책은 greedy / 탐색 정책은 $\\epsilon$-greedy로 선택하면 Q-learning과 같다. (greedy 하게 정책을 선택하는 경우 $\\text{argmax}_a Q(S_{t+1}, a)$을 제외하고는 모든 $\\pi(a \\mid S_{t+1})$가 0 이므로 $\\max_a Q(S_{t+1}, a)$와 $\\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a)$가 같아진다.)\n",
    "\n",
    "![Local image](./images/sutton/figure_6_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n-step Expected Sarsa는 아래 backup-diagram과 같이 $t+n-1$ step까지는 감가된 보상을 사용하고, $t+n$ step에서는 $Q_{t+n-1}(S_{t+n}, a)$의 기댓값을 감가하여 사용한다.\n",
    "\n",
    "$$G_{t:t+n} \\doteq R_{t+1} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\sum_a \\pi(a\\mid S_{t+n}) Q_{t+n-1}(S_{t+n}, a), ~~~ n \\geq 1, ~ 0 \\leq t \\leq T-n$$\n",
    "![Local image](./images/sutton/p117_expected_sarsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 $n$-step Off-policy Learning by Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (회고) Importance Sampling를 이용한 MC Prediction : 에피소드 전체에서 $\\pi$와 $b$라는 두개의 정책의 차이를  보정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Control 방법들은 최적 정책을 향해 action-value를 학습시켜야하면서도 당장 보기에는 꼭 최적은 아닌 정책을 따라서 여러 action을 탐색하기도 해야하는 딜레마가 있다. on-policy 방법은 사실 이 두가지 상충하는 목표를 절충한 것으로서 살짝 덜 최적인 정책을 학습(learn about and becomes the optimal policy)과 탐색(exploratory)에 같이 사용하는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면 학습을 위한 정책과 탐색을 위한 정책을 별도로 가져가는 방법(**off-policy**)를 생각해볼 수 있다. 학습을 위한 정책을 target policy $\\pi$라 하며, 탐색을 위한 정책을 behavior policy $b$라 한다. 예를들어 $\\pi$는 greedy policy를 사용하고, $b$는 $\\epsilon$-greedy policy를 사용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약 $\\pi$와 $b$가 같았다면 $b$를 따라 생성된 에피소드에서 구해진 보상(return, $G_t$)을 이용해 아래와 같이 $V(S_t)$를 업데이트 하면 된다. \n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\bigg[ G_t - V(S_t) \\bigg]$$ 하지만 Off-policy에서는 $\\pi$와 $b$가 다르므로 그냥 이렇게 업데이트할 수 없다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\pi$와 $b$의 차이를 보정하기 위해 '$b$를 따라 생성된 에피소드'가 $b$에서 나왔을 확률과 $\\pi$에서 나왔을 확률의 비를 $G_t$에 곱해주고, 이 값들의 평균을 $v_{\\pi}(S_t)$에 대한 추정치로 사용한다. (이렇게 Importance Sampling이 적용됨)\n",
    "$$\\mathbb{E}\\big[ \\rho_{t:T-1} G_t \\mid S_t \\big] = v_{\\pi}(S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <u>'$b$를 따라 생성된 에피소드'가 $b$에서 나왔을 확률과 $\\pi$에서 나왔을 확률의 비</u>는 아래와 같이 구할 수 있는데, $p(S_{k+1} \\mid S_k, A_k)$가 분자 분모에서 약분(canceled)되어 그 수식이 간단해진다.\n",
    "$$\\rho_{t:T-1} \\doteq \\frac{\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k)}{\\prod_{k=t}^{T-1} b(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k)} = \\frac{\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k) }{\\prod_{k=t}^{T-1} b(A_k \\mid S_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 평균 $\\mathbb{E}\\big[ \\rho_{t:T-1} G_t \\mid S_t \\big]$을 구하는 방식이 두가지가 있는데, 두번째 방법이 더 선호된다.\n",
    " - ordinary importance sampling에서는 단순히 산술평균을 구한다.(low bias, high variance)\n",
    " $$V(s) \\doteq \\frac{\\sum_{t \\in \\Gamma(s)} \\rho_{t:T(t)-1}G_t}{\\mid\\Gamma(s)\\mid}$$\n",
    " - weighted importance sampling에서는 가중평균을 구한다.(high bias, log variance)\n",
    " $$V(s) \\doteq \\frac{\\sum_{t \\in \\Gamma(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t \\in \\Gamma(s)} \\rho_{t:T(t)-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### n-step off-policy + Importance Sampling : n-step 에피소드에서  $\\pi$와 $b$라는 두개의 정책의 차이를  보정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6장에서 다뤘던 Impotance Sampling은 MC를 대상으로 하기 때문에 'Importance Sampling Ratio'($\\rho$)를 구할 때 특정 time-step $t$부터 에피소드 끝($T-1$)까지가 나올 확률을 계산했다. ($\\prod_{k=1}^{T-1}$)\n",
    "\n",
    "\n",
    "- 반면 'n-step off-policy + Importance Sampling'에서 'Importance Sampling Ratio'($\\rho$)를 구할 때는 에피소드의 특정 time-step $t$부터 $t+n-1$까지가 나올 확률을 계산한다.($\\prod_{k=t}^{\\min(t+n-1, T-1)}$)\n",
    "\n",
    "$$\\rho_{t:t+n-1} \\doteq \\frac{\\prod_{k=t}^{\\min(t+n-1, T-1)} \\pi(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k)}{\\prod_{k=t}^{\\min(t+n-1, T-1)} b(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k)} = \\frac{\\prod_{k=t}^{\\min(t+n-1, T-1)} \\pi(A_k \\mid S_k) }{\\prod_{k=t}^{\\min(t+n-1, T-1)} b(A_k \\mid S_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 $V_{t+n}(S_t)$를 아래와 같이 업데이트 한다. (만약 $\\pi$와 $b$가 같다면 $\\rho_{t:t+n-1}$는 항상 1이므로 이 경우 'on-policy TD Prediction'이 된다.)\n",
    "$$V_{t+n}(S_t) \\doteq V_{t+n-1}(S_t) + \\alpha \\rho_{t:t+n-1} \\big[ G_{t:t+n} - V_{t+n-1}(S_t \\big], ~~ 0 \\leq t < T$$\n",
    "\n",
    "\n",
    "- 이 접근법을 n-step Sarsa(= on-policy n-step TD control)에 적용하면 $Q_{t+n}(S_t, A_t)$를 아래와 같이 업데이트 한다.(on-policy라면 $\\rho$값이 1이 되고, off-policy라면 다른 값이 될 것이다. 또한 Sarsa에서는 2 step을 지나야 다음 상태를 보게되기 때문에 $\\rho_{t:t+n-1}$가 아니라 $\\rho_{t+1:t+n-1}$가 사용된다.)\n",
    "$$Q_{t+n}(S_t, A_t) \\doteq Q_{t+n-1}(S_t, A_t) + \\alpha \\rho_{t+1:t+n-1} \\big[ G_{t:t+n} - Q_{t+n-1}(S_t, A_t \\big], ~~ 0 \\leq t < T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'n-step Expected Sarsa'의 off-policy 버전에서는 $Q(S_t, A_t)$를 아래와 같이 업데이트 한다.    \n",
    "($n$ step 중 마지막 step의 행동에 해당하는 $Q$값은 그냥 기댓값을 취하므로 $\\rho_{t+1:t+n-1}$ 대신 $\\rho_{t+1:t+n-2}$를 사용한다.\n",
    "\n",
    "$$\\begin{align}\n",
    "Q_{t+n}(S_t, A_t) &\\doteq Q_{t+n-1}(S_t, A_t) + \\alpha \\rho_{t+1:t+n-2} \\big[ G_{t:t+n} - Q_{t+n-1}(S_t, A_t) \\big], ~~ 0 \\leq t < T \\\\\n",
    "G_{t:t+n} &\\doteq R_{t+1} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\sum_a \\pi(a\\mid S_{t+n}) Q_{t+n-1}(S_{t+n}, a), ~~~ n \\geq 1, ~ 0 \\leq t \\leq T-n\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 \\*Per-decision Off-policy Methods with Control Variates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tree Backup 알고리즘 : Q-learning, 1-step Expected Sarsa의 전근법을 n-step으로 확장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-step으로 제한되기는 하지만 6장에서 다뤘던 Q-learning과 Expected Sarsa는 off-policy learning이지만 Importance Sampling을 사용하고 있지 않다. 대신 1-time-step 지날 때마다 관측된 $\\big( S_t, A_t, R_{t+1}, S_{t+1}\\big)$의 $R_{t+1}$, 그리고 $S_{t+1}$에서 가능한 모든 $Q(S_{t+1}, a)$의 최대값(Q-learning) 혹은 기댓값(Expected Sarsa)을 사용하여 $Q(S_t, A_t)$ 업데이트를 위한 타겟을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 아이디어를 n-step으로 확장한 것이 **n-step Tree Backup 알고리즘**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-step Tree Backup 알고리즘을 예로들어 time-step에 따른 반환값(return, $G$)을 살펴보자.\n",
    " 1. $\\big( S_t, A_t, R_{t+1}, S_{t+1}\\big)$ 와 $S_{t+1}$에서 가능한 모든 행동을 생각하면, 이 에피소드에서 $\\big( S_t, A_t \\big)$가 받게되는 보상 $G_{t:t+1}$은 아래와 같다. (여기까지는 Q-learning이다.)\n",
    " $$G_{t:t+1} \\doteq R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) Q_t(S_{t+1}, a), ~~ t < T-1$$\n",
    " - 다음으로 $\\big( S_{t+1}, A_{t+1} \\big)$이 받게되는 보상 $G_{t+1:t+2}$은 아래와 같다.\n",
    " $$G_{t+1:t+2} \\doteq R_{t+2} + \\gamma \\sum_a \\pi(a \\mid S_{t+2}) Q_t(S_{t+2}, a), ~~ t < T-2$$\n",
    " - 마찬가지로 $\\big( S_{t+2}, A_{t+2} \\big)$이 받게되는 보상 $G_{t+2:t+3}$은 아래와 같다.\n",
    " $$G_{t+2:t+3} \\doteq R_{t+3} + \\gamma \\sum_a \\pi(a \\mid S_{t+3}) Q_t(S_{t+3}, a), ~~ t < T-3$$\n",
    " - 1과 2를 조합하면 $t$에서 $t+2$까지의 정보 $\\big( S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, R_{t+2}, S_{t+2}\\big)$를 고려했을 때 $\\big( S_t, A_t \\big)$가 받게되는 보상 $G_{t:t+2}$를 아래와 같이 구할 수 있다. (※ $\\gamma \\sum_{a \\neq A_{t+1}} \\pi(a \\mid S_{t+1}) Q_{t+1} (S_{t+1}, a)$은 현재 에피소드에 등장하지 않은 행동들에서 기대되는 반환값이고, $\\gamma \\pi(A_{t+1} \\mid S_{t+1}) G_{t+1:t+2}$는 $A_{t+1}$을 했을 때 기대되는 반환값이다.)\n",
    " $$G_{t:t+2} \\doteq R_{t+1} + \\bigg( \\gamma \\sum_{a \\neq A_{t+1}} \\pi(a \\mid S_{t+1}) Q_{t+1} (S_{t+1}, a) \\bigg) + \\bigg(\\gamma \\pi(A_{t+1} \\mid S_{t+1}) G_{t+1:t+2} \\bigg), ~~ t < T-2$$\n",
    " - 이를 n-step으로 확장하면 n-step까지의 에피소드 정보를 이용했을 때, $\\big( S_t, A_t \\big)$가 받게되는 보상 $G_{t:t+n}$은 아래와 같다.\n",
    " $$G_{t:t+n} \\doteq R_{t+1} + \\bigg( \\gamma \\sum_{a \\neq A_{t+1}} \\pi(a \\mid S_{t+1}) Q_{t+n-1} (S_{t+1}, a) \\bigg) + \\bigg(\\gamma \\pi(A_{t+1} \\mid S_{t+1}) G_{t+1:t+n} \\bigg), ~~ t+1 < T, n > 1$$\n",
    " - 결론적으로 n-step Sarsa에 'n-step Tree Backup 알고리즘'을 적용했을 때 $Q_{t+n}(S_t, A_t)$를 아래와 같이 업데이트할 수 있다.(사실 7.2에서 등장한 n-step Sarsa의 업데이트식과 동일하며, $G_{t:t+n}$를 tree backup으로 구했다는 것만 다르다.)\n",
    "$$Q_{t+n}(S_t, A_t) \\doteq Q_{t+n-1}(S_t, A_t) + \\alpha \\bigg[ G_{t:t+n} - Q_{t+n-1}(S_t, A_t)\\bigg], ~~ 0 \\leq t < T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/sutton/p122_tree_backup1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 \\*A Unifying Algorithm: n-step $Q(\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
