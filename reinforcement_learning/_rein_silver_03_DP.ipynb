{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy Evaluation : 누군가 어떤 정책을 정해줬을때 그 정책이 얼마나 좋은지를 평가\n",
    "- Policy Iteration : Policy  Evaluation으로 정책의 가치를 판단한 후 정책 수정을 반복?\n",
    "- Value Iteration : Value function 개선을 반복?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가: 정해진 정책을 따랐을 때 각 상태에 해당하는 가치를 반복적으로 갱신하여 구하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bellman expectation equation -> Policy Evaluation\n",
    "- Bellman optimality equation -> Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그리드 밖으로 향하는 액션을 할 경우 상태가 변하지 않는다. 예를들어 $s=1$에서 $up$이란 행동을 할 경우 상태는 변함없이 $s=1$이라는 것이다.\n",
    "- 모든 액션에 대한 보상 $\\mathcal{R}_{s}^{a}$은 -1이다.\n",
    "- 모든 액션에 대한 상태 변환은 확정적이다. 예를들어 $p(s=6, r=-1 \\mid s=5, a=right)=1$, $p(s=7, r=-1 \\mid s=7, a=right)=1$, $p(s=10, r \\mid s=5, a=right)=0$이다. (7번셀에서 오른쪽으로 움직이면 그리드 밖이므로 다시 7번셀로 돌아올 확률은 1이다. 5번 셀에서 오른쪽으로 움직였을 때 바로 10번 셀로 이동할 확률은 0이다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 정책 평가 과정에서는 이터레이션($k=0, 1, \\cdots$)이 진행되더라도 정책은 4방향 모두 0.25로서 변함이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "v_{k=1}(s=1) \n",
    "&= \\pi(a=l \\mid s=1) (\\mathcal{R}_{s=1}^{a=l} + v_{k=0}(s=0)) \\\\\n",
    "&+ \\pi(a=r \\mid s=1) (\\mathcal{R}_{s=1}^{a=r} + v_{k=0}(s=2)) \\\\\n",
    "&+ \\pi(a=u \\mid s=1) (\\mathcal{R}_{s=1}^{a=u} + v_{k=0}(s=\\color{red}1)) \\\\\n",
    "&+ \\pi(a=d \\mid s=1) (\\mathcal{R}_{s=1}^{a=d} + v_{k=0}(s=5)) \\\\\n",
    "&= 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 + 0) \\\\\n",
    "&= -1\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "v_{k=1}(s=8) \n",
    "&= \\pi(a=l \\mid s=8) (\\mathcal{R}_{s=8}^{a=l} + v_{k=0}(s=7)) \\\\\n",
    "&+ \\pi(a=r \\mid s=8) (\\mathcal{R}_{s=8}^{a=r} + v_{k=0}(s=9)) \\\\\n",
    "&+ \\pi(a=u \\mid s=8) (\\mathcal{R}_{s=8}^{a=u} + v_{k=0}(s=5)) \\\\\n",
    "&+ \\pi(a=d \\mid s=8) (\\mathcal{R}_{s=8}^{a=d} + v_{k=0}(s=12)) \\\\\n",
    "&= 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 + 0) \\\\\n",
    "&= -1\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "v_{k=2}(s=1) \n",
    "&= \\pi(a=l \\mid s=1) (\\mathcal{R}_{s=1}^{a=l} + v_{k=1}(s=0)) \\\\\n",
    "&+ \\pi(a=r \\mid s=1) (\\mathcal{R}_{s=1}^{a=r} + v_{k=1}(s=2)) \\\\\n",
    "&+ \\pi(a=u \\mid s=1) (\\mathcal{R}_{s=1}^{a=u} + v_{k=1}(s=\\color{red}1)) \\\\\n",
    "&+ \\pi(a=d \\mid s=1) (\\mathcal{R}_{s=1}^{a=d} + v_{k=1}(s=5)) \\\\\n",
    "&= 0.25 \\times (-1 + 0) \\\\\n",
    "&+ 0.25 \\times (-1 -1) \\\\\n",
    "&+ 0.25 \\times (-1 -1) \\\\\n",
    "&+ 0.25 \\times (-1 -1) \\\\\n",
    "&= -1.75\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "v_{k=2}(s=8) \n",
    "&= \\pi(a=l \\mid s=8) (\\mathcal{R}_{s=8}^{a=l} + v_{k=1}(s=7)) \\\\\n",
    "&+ \\pi(a=r \\mid s=8) (\\mathcal{R}_{s=8}^{a=r} + v_{k=1}(s=9)) \\\\\n",
    "&+ \\pi(a=u \\mid s=8) (\\mathcal{R}_{s=8}^{a=u} + v_{k=1}(s=5)) \\\\\n",
    "&+ \\pi(a=d \\mid s=8) (\\mathcal{R}_{s=8}^{a=d} + v_{k=1}(s=12)) \\\\\n",
    "&= 0.25 \\times (-1 -1) \\\\\n",
    "&+ 0.25 \\times (-1 -1) \\\\\n",
    "&+ 0.25 \\times (-1 -1) \\\\\n",
    "&+ 0.25 \\times (-1 -1) \\\\\n",
    "&= -2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 쉽게 생각하면 주변 셀의 가치에 -1(=$\\mathcal{R}_s^a$)를 더한 후 평균 구하는 것이라 할 수 있다. 단 그리드 밖으로 가는 행동에 대해서는 현재 셀의 가치 값을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가 이터레이션을 통해 주어진 정책을 따랐을 때 각 상태마다 기대 가치를 구할 수 있다. 또한 이 결과를 통해 보다 나은 정책(optimal policy)를 찾을 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contraction Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/silver/03_DP-42.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
