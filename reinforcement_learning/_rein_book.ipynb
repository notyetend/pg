{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 스터디 참고 자료\n",
    "- Sutton 교수 책 : http://www.incompleteideas.net/book/the-book-2nd.html\n",
    "- Silver 교수 강의 : http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "- 버클리 강의 : http://rll.berkeley.edu/deeprlcourse/\n",
    "- 게임에 적용한 예 : https://www.slideshare.net/deview/ai-67608549\n",
    "- Coursera 강의 : https://www.coursera.org/learn/practical-rl\n",
    "- Udacity 강의 : https://www.udacity.com/course/reinforcement-learning--ud600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 강화학습이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 환경에 놓인 에이전트가 누적 보상을 최대화 하도록 연속된 행동을 하도록 하는 머신러닝 방법\n",
    "\n",
    "\n",
    "> Reinforcement learning (RL) is an area of machine learning inspired by behaviourist psychology, concerned with how software **agents** ought to take **actions** in an **environment** so as to maximize some notion of **cumulative reward**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 에이전트? (agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 환경? (environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 보상? (reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 행동? (action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 정책? (policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모든 상태에 대해 에이전트가 어떤 행동을 해야하는지 정해놓은 것, 최적 정책(optimal policy)는 정책들 중 가장 좋은 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 상태? (state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MDP? (Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP & Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 순차적 행동 결정 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 결론에 이르기까지 여러번의 선택을 해야하는 문제로서 현재의 행동이 다음 상태와 행동에 영향을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 순차적 행동 결정 문제를 수학적으로 표현한 것이 바로 MDP이다.\n",
    "\n",
    "\n",
    "> MDP는 상태(state), 행동(action), 상태 변환 확률(state trantion probability), 보상함수(reward function), 할인율(혹은 감가율, reward factor), 정책(policy), 반환값(return), 가치함수(value function), 큐함수(Q function) 등의 개념으로 설명된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\mathbf{S}$, 상태: 에이전트가 관찰 가능한 상태의 집합, 그리드월드에서는 격자상의 위치들의 집합이 상태 집합이다. 시간 $t$에서의 상태를 확률변수 $S_t$로 표현하고 그 값은 $s$로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\mathbf{A}$, 행동: 에이전트가 할 수 있는 모든 상태의 집합, 상태 $S_t$에서 할 수 있는 행동을 확률변수 $A_t$로 표현하며  그 값은 $a$로 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $P_{ss}^a$, 상태 변환 확률: 상태 $s$에서 행동 $a$를 취했을 때 실제 다음 상태 $s\\prime$에 도달할 확률로서 에이전트의 행동에 대한 환경의 상호작용을 모델링한 것이다. \n",
    "$$P_{ss\\prime}^a = P[S_{t+1}=s \\mid S_t=s, A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $R_s^a$, 보상함수: 시간 $t$에서 상태가 $S_t=s$이고 행동이 $A_t=a$일 때 에이전트가 받을 보상의 기대값은 아래와 같다. $$R_s^a=E[R_{t+1} \\mid S_t=s, A_t=a]$$\n",
    "- $\\gamma$, 감가율(할인율): 현재에 가까운 보상일수록 더 큰 가치를 부여하는데, 나중에 받을 보상을 얼마나 줄일지에 대한 비율. 현재시간($t$)으로부터 $k$시간이 지난 후 보상이 $R_{t+k}$라면 이 보상의 현재가치는 $\\gamma$만큼 $k-1$번 할인하여 $\\gamma^{k-1}R_{t+k}$가 된다. ($t$시점 행동에 대한 보상이 한 시점 다음에 발생하여 $R_{t+1}$이므로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\pi(a|s)$, 정책: 모든 상태에서 에이전트가 할 행동으로서 상태가 입력으로 들어오면 행동을 출력하는 함수로서 특정 상태에서 가능한 각 행동에 대한 확률 분포이다. $$\\pi(a|s)=P[A_t=a \\mid S_t=s]$$\n",
    "- $G_t$, 반환값(return): 에피소드의 $t$시점 이후 받은 감가율을 적용한 보상들의 합 \n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $v(s)$, 가치함수(value function): 어떤 상태에서 시작해서 앞으로 받을 보상에 대한 기댓값\n",
    "$$\\begin{align}\n",
    "v(s) &= E[G_t \\mid S_t=s] \\\\\n",
    "&= E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t=s]\\\\\n",
    "&= E[R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\cdots) \\mid S_t=s]\\\\\n",
    "&= E[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s]\\\\\n",
    "&= E[R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t=s]\\\\\n",
    "\\end{align}$$\n",
    " - 정책을 고려한 가치함수는 아래와 같으며 이를 **벨만 기대 방정식**이라 한다.(현재의 가치함수와 다음 상태의 가치함수 사이의 관계를 표현)\n",
    "$$\\begin{align}\n",
    "v_{\\pi}(s) &= E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t=s] \\\\\n",
    "&= \\sum_{a \\in A} \\pi(a \\mid s) \\left( R_{t+1} + \\gamma \\sum_{s\\prime \\in S} P_{ss\\prime}^a v_{\\pi}(s\\prime)  \\right)\n",
    "\\end{align}$$\n",
    "위 식은 현재의 상태에서 가능한 모든 행동($a \\in A$)에 대해 가치의 기댓값을 구하며 각 행동에 대해서도 상태변환확률($P_{ss\\prime}^a$)에 따라 결정되는 가능한 다음 상태들($s\\prime \\in S$)의 가치함수($v_{\\pi}(s\\prime)$)에 감가율($\\gamma$)을 적용하여 더한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $q_{\\pi}(s, a)$, 큐함수(Q Function): 가치함수는 상태에 대한 함수로서 특정 상태에서 할 행동을 결정하기 위해서는 발생 가능한 모든 다음 상태에 대한 가치함수를 따져봐야한다. 이를 약간 변형하여 특정 상태에서 어떤 행동을 했을 때 가치에 대한 함수, 즉 행동과 상태를 입력으로 하는 조금 다른 가치함수를 생각해볼 수 있으며 이를 큐함수라 한다. 특정 상태에서 가능한 행동들에 대한 큐함수가 있을 것이고, 각 행동을 할 확률이 정책으로 주어질 것이므로 이 둘을 곱하고 더하면 현재 상태에 대한 가치함수가 된다. 이 관계를 아래와 같이 표현할 수 있다.\n",
    "$$v_{\\pi}(s) = \\sum_{a\\in\\mathbf{A}} \\pi(a \\mid s) ~ q_{\\pi}(s, a)$$\n",
    ">  - **벨만 기대 방정식 형태의 큐함수**는 아래와 같다.???\n",
    "$$q_{\\pi}(s, a) = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **벨만 최적 방정식**(Bellman Optimality Equation) ???\n",
    " - 최적 정책: 여러 정책 중 가치함수 혹은 큐함수를 가장 크도록 하는 정책 \n",
    " $$\\pi\\ast(s, a) = \\begin{cases}1 & \\text{if } a = \\text{argmax}_{a\\in A} q\\ast(s, a) \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "  - 최적 가치함수: 최적 정책을 따랐을 때의 가치함수, 가치함수 중 가장 큰 것 혹은 최적 큐함수 중 가장 큰 것 (세번째 식은 '가치함수에 대한 벨만 최적 방정식')\n",
    " $$\\begin{align}\n",
    " v\\ast(s) &= \\max_{\\pi} \\left[ v_{\\pi}(s)\\right] \\\\\n",
    " &=\\max_a \\left[ q \\ast (s, a) \\mid S_t = s, A_t = a\\right] \\\\\n",
    " &= \\max_a ~ E \\left[ R_{t+1} + \\gamma v \\ast (S_{t+1}) \\mid S_t = s, A_t = a\\right]\n",
    " \\end{align}$$\n",
    " - 최적 큐함수: 최적 정책을 따랐을 때의 큐함수(두번째 식은 '큐함수에 대한 벨만 최적 방정식')\n",
    " $$\\begin{align}\n",
    " q\\ast(s, a) &= \\max_{\\pi} \\left[ q_{\\pi}(s, a)\\right] \\\\\n",
    " &= E \\left[ R_{t+1} + \\gamma \\max_{a \\ast} q \\ast (S_{t+1}, a \\ast) \\mid S_t = s, A_t = a\\right]\n",
    " \\end{align}$$\n",
    "  (최적 정책을 따라갈 때 현재 상태의 큐함수는 다음 상태에 선택 가능한 행동 중에서 가장 높은 값의 큐함수를 1번 감가하고 보상을 더한 것과 같다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3장: 강화학습 기초 2 - 그리드월드와 다이내믹 프로그래밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다이나믹 프로그래밍: 작은 문제가 큰 문제 안에 중첩돼 있는 경우에 작은 문제의 답을 다른 작은 문제에 이용함으로써 효율적으로 계산하는 방법\n",
    "- 정책 이터레이션: 다이나믹 프로그래밍으로 벨만 기대 방정식($v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t=s]$)을 푸는 것\n",
    "- 가치 이터레이션: 다이나믹 프로그래밍으로 벨만 최적 방정식($v \\ast(s) = \\max_a ~ E \\left[ R_{t+1} + \\gamma v \\ast (S_{t+1}) \\mid S_t = s, A_t = a\\right]$)을 푸는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\mathcal{P}_{ss\\prime} = \\mathbb{P}[S_{t+1}=s\\prime \\mid S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\mathcal{P}_{ss\\prime}^a = \\mathbb{P}[S_{t+1}=s\\prime \\mid S_t = s, A_t = a]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
