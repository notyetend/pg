{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Kullback-Leibler divergence\n",
    "> - Murphy, ML, 2.8.2\n",
    "> - wiki\n",
    "> - Bishop, PRaML, 1.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 두 probability distrubution $p$와 $q$의 차이를 측정하는 방법으로 Kullback-Leibler divergence(KL divergence, relative entropy)라는 것이 있다.\n",
    "\n",
    "$p$와 $q$가 discrete probability distribution일 경우 아래와 같이 정의된다.\n",
    "$$\\mathbb{KL}(p||q) \\triangleq \\sum_{k=1}^{K}p(k) \\log\\frac{p(k)}{q(k)}$$\n",
    "\n",
    "또한 $p$와 $q$가 continuous probability distribution 경우에는 아래와 같이 정의된다.\n",
    "$$\\mathbb{KL}(p||q) \\triangleq \\int p(x) \\log \\frac{p(x)}{q(x)}dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 값에 대해 아래와 같은 해석이 가능하다.\n",
    "- $\\mathbf{E}\\left[ \\log p(x) - \\log q(x)) \\right]$, 즉 $p$와 $q$의 logarithmic difference의 기댓값.\n",
    "- belief를 prior probability distribution $q$에서 posterior probability distribution $p$로 바꿨을 때 추가로 얻게된 정보\n",
    "- 실제 분포 $p$를 $q$로 근사 했을 때 잃게 되는 정보량\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mixtures of Gaussians\n",
    "> - Bishop, PRaML, 1.6.1\n",
    "> - Bishop, PRaML, 2.3.9\n",
    "> - Bishop, PRaML, 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian distribution이 중요한 해석적 특성들을 갖고 있기는 하지만, 실제 데이터를 modelling할 때는 한계가 많다. 따라서 간단한 분포들의 선형 결합(linear combination)을 통해 어떤 혼합 분포(Mixture distribution)을 만들어 보다 복잡한 형태의 분포를 modelling하게 된다. 혼합의 재료(components of the mixture)로 Gaussian distribution을 사용하는 분포를 Misture of Gaussian이라 하고 형태는 아래와 같다.($\\pi_k$를 missing coefficient라 한다.)\n",
    "\n",
    "$$p(x) = \\sum_{K}^{k=1} \\pi_k \\mathcal{N}(x|\\mu_k , \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EM Algorithm\n",
    "> - Bishop, PRaML Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 K-mean algorithm에 대해서 간단히 살펴보자. $n$개의 data point($\\mathbf{x}_i$)가 있다고 해 보자. $$\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}, \\textbf{x}_i \\in \\mathbb{R}^{D}$$\n",
    "그리고 이들을 $K$개의 그룹(cluster)로 나누려고 한다. 이때 각 data point가 어떤 cluster에 할당되는지를 표현하기 위한 binary indicator variable, $r_{nk} = \\begin{cases} 1 & \\mathbf{x}_n \\text{ is asigned to cluster } k \\\\ 0 & otherwise\\end{cases}$와       \n",
    "$K$개의 $D$-dimensional vector $\\mathbf{\\mu}_k \\in \\mathbb{R}^{D}$를 사용하도록 하자.\n",
    "\n",
    "이때 distortion measure(cost function) $J$는 아래와 같다.   \n",
    "$$J = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} ||\\mathbf{x}_n - \\mathbf{\\mu}_k||^2$$\n",
    "결국 $J$를 최소화 하는 $\\{r_{nk}\\}$와 $\\{\\mathbb{\\mu}_k\\}$를 찾는 것이 목표가 된다.\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘은 몇 단계로 나뉘는데\n",
    "1. $\\mathbf{\\mu}_k$를 어떤 값으로 초기화\n",
    "2. [1st phase] $\\mathbf{\\mu}_k$를 고정시키고, $r_{nk}$를 (재)할당하여 $J$를 최소화   \n",
    "(minimize $J$ with respect to the $r_{nk}$, keeping the $\\mathbf{\\mu}_k$ fixed.)\n",
    "3. [2nd phase] $r_{nk}$를 고정시키고 $\\mathbf{\\mu}_k$를 변화시켜  $J$를 최소화   \n",
    "(minimize $J$ with respect to the $\\mathbf{\\mu}_k$, keeping the $r_{nk}$ fixed.)\n",
    "4. converge할 때까지 [1st phase]와 [2nd phase]를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 '각 data point의 cluster를 재할당하는 [1st phase]'와       \n",
    "'각 cluster의 중심점을 갱신하는 [2nd phase]'는    \n",
    "각각 EM algorithm에서 E (expectation)와 M (maximization) 단계에 해당한다.\n",
    "\n",
    "참고로 EM algorithm을 적용하기 전에 Gaussian mixture model의 초기값을 할당할 때 K-mean algorithm을 사용하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hessian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Kalmal filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
