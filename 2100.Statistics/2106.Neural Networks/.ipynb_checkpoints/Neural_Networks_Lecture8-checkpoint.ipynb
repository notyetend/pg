{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 Neural Networks for Machine Learning 강의를 정리한 내용임.   \n",
    "2016.11.18. by Dongwan Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week8. More recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 8a, A brief overview of \"Hessian-Free\" optimization (ADVANCED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 강의에서는 Optimization method로 Gradient descent(Steepest descent) 방법을 사용 했었다. 이 방법은 error surface의 현재 지점에서 가장 하강 경사가 급한 방향(steepest descent)의 gradient 값에 비례한 어떤 값 만큼 weight vector를 이동하겠다는 아이디어 였다. \n",
    "$$w(t) = w(t-1) + \\Delta w(t), ~ \\Delta w(t) = - ~ \\alpha \\frac{\\partial E}{\\partial w}(t)$$\n",
    "\n",
    "그런데 이 방법은 error function이 좁은 협곡형태의 구간을 갖을 경우 그곳에서 zig-zag 형태로 제자리를 오가는 일이 발생하게 된다. 예를들어 아래 그림에서 (a)에서 출발할 경우 문제가 없으나, (b)에서 출발하는 경우 협곡에서 빠져나오는데 오랜 시간이 걸리게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_gradient_valley.png\"/>\n",
    "<h6 align='right'>http://ludovicarnold.altervista.org/teaching/optimization/gradient-descent/?doing_wp_cron=1480304533.1284360885620117187500</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결국 error surface의 현재 지점에서 어느 방향으로 이동하는 것이 결국 가장 많은 error를 줄이게 되는지가 중요한 문제라고 할 수 있다. 이런 문제를 풀 수 있는 여러가지 아디이어가 있겠으나 문제를 단순화 하여 curvature(error function의 2차 미분)이 상수인 경우, 특정 방향으로 이동하여 줄어드는 error의 크기는 $\\frac{gradient}{curvature}$에 비례한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_curvature_compaired.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 8b, Modeling character string with multiplicative connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Intro</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 다뤘던 Hessian-free optimization을 이용하여 wikipedia의 character string을 모델링 해 보자. 잠간! 여기서 하려는 것은 strings of words가 아니라 character string을 모델링 하는 것이다. 즉 문자 단위로 예측이 진행된다. 형태소나 단어 사전을 놓고 어떤 단어나 형태소가 다음에 나올지를 예측하는 것이 아니라 다음 나올 문자를 예측하는 것이다.\n",
    "\n",
    "본격적인 논의에 앞서 **multiplicative connection**이 왜 필요한 것인지 그리고 어떻게 효과적으로 구현할 수 있는지 알아보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Model for character string</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 RNN을 이용하여 아래와 같은 형태로 character string을 모델링 하는 것을 생각해 볼 수 있다. hidden layer에는 1500개의 unit이 있고, 입력으로 1-of-86 character가 들어온다. 여기서는 이 두 입력(current hidden layer와 1-of-86 character)은 **additive**로 next hidden layer에 입력된다. 마지막으로 hidden layer의 출력이 86개 input을 갖는 softmax에 입력되어 다음에 나올 문자를 예측하게 된다. 이 경우 parameter space의 크기는 86x1500x1500이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_obvious_rnn.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 아래와 같은 tree모형을 사용하는 아이디어도 가능할 것이다. 아래 그림은 아주 아주 큰 tree의 일부분인데, 예를들어 fix까지 문자열이 구성되었고 다음에 나올 문자의 가지수는 86개이므로 root node에서 86개의 edge가 펼쳐져 나갈 것이다.(그림에는 두개만 표현됨) 또한 펼쳐져 나간 86개의 노드에서 다시 각각 86개의 edge-node가 펼쳐져 나갈 것이다. 그런데 이런 방식으로는 첫 문자가 주어져 있을 때, 길이가 $N$인 문자열을 표현하기 위해서는 $86^{N-1}$개의 확률을 계산해야 한다.\n",
    "\n",
    "그런데 아래와 같은 모델을 RNN으로 구현한다면 각 node는 RNN의 hidden state vector라 할 수 있다. 또한 distributed representation 덕분에 각 node의 hidden state vector간에는 어떤 언어적인 특성들을 공유하게 된다. 예를들어 지금까지 나온 문자들이 fixi일 경우 마지막 나온 문자 i 전까지의 문자열(fix)가 동사라는 정보를 이용하여 i 다음에 n이 나올 가능성이 높다는 것을 알 수 있다. fix가 동사라는 특성은 distributed representation에 의해 학습되므로 RNN을 통한 학습방식이 tree방식에 비해 장점이 있다.\n",
    "\n",
    "그런데 곰곰히 생각해보면 next hidden representation은 'current hidden representation'과 'current character'의 conjunction(교집합, 확률곱)이라 할 수 있다. 앞서 예를 생각해보면 현재까지 나온 문자열이 fixi일때, 앞서의 단어(fix)가 동사가 아니라면 i다음에 굳이 n이 나올 확률이 높지 않아도 된다. 즉 앞서의 상황(current hidden representation)과 i가 나왔다는 정보는 다음 문자를 예측하기 위해 conjunction 관계를 갖는 것이 합리적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_tree.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 RNN에서 input이 next hidden layer에 additive한 입력이 아니라 multiplicative한 입력으로 동작했으면 하는 상황들이 있다. 즉 current input이 hidden-to-hidden weight matrix 전체를 크게 좌우하는 역할을 했으면 하는 것이다. \n",
    "\n",
    "(Tree 모델 전에 소개한 RNN과 같이) 물론 86x1500x1500로 parameter space를 늘려서 구현하는 방법도 있으나 parameter수가 너무 많아지는 문제가 발생하고 이는 overfit으로 이어질 수 있다. 그렇다면 좀더 적은 수의 parameter를 이용하면서 이런 효과를 낼수는 없을까? 그리고 추가적인 조건으로 86개의 문자에 각각 다른 1500x1500 weight matrix가 대응되면 좋겠는데, 각 문자마다 너무 다른 matrix는 아니었으면 좋겠다. 예를들어 문자 9과 문자 8이 서로 다른 weight matrix를 갖으면 좋겠는데 너무 다르지는 않았으면 좋겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Factors - Multiplicative connection</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서의 여러 요건들을 만족시키는 모델을 만들기 위해 input사이의 multiplicative interaction을 구현하는 factor라는 아이디어가 있다.\n",
    "\n",
    "우선 몇가지 notation을 먼저 정의해보면..\n",
    "- $\\mathbf{a}$ : current hidden layer state vector, $\\mathbf{a} \\in \\mathbb{R}^{m \\times 1}$ \n",
    "- $\\mathbf{u}_f$ : weight from current hidden layer state to a factor, $\\mathbf{u}_f \\in \\mathbb{R}^{m \\times 1}$\n",
    "- $\\mathbf{b}$ : input vector, $\\mathbf{b} \\in \\mathbb{R}^{n \\times 1}$\n",
    "- $\\mathbf{w}_f$ : weight from input layer to a factor, $\\mathbf{w}_f \\in \\mathbb{R}^{n \\times 1}$\n",
    "- $\\mathbf{c}$ : next hidden layer state vector, $\\mathbf{c} \\in \\mathbb{R}^{m \\times 1}$\n",
    "- $\\mathbf{c}_f$ : 1 of vectors composing next hidden layer state vector, $\\mathbf{c}_f \\in \\mathbb{R}^{m \\times 1}$\n",
    "- $\\mathbf{v}_f$ : weight from a factor to next hidden layer, $\\mathbf{v}_f \\in \\mathbb{R}^{m \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_factor.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 7에서 다뤘던 additive connection을 사용하는 RNN에서는 hidden layer에 $n$개의 unit이 있을 경우 current hidden layer와 next hidden layer사이에는 $n \\times n$개의 weight들이 존재한다. \n",
    "\n",
    "위 그림에서 factor $f$ 하나는 current hidden layer에서 next hidden layer의 unit들 중 하나로 가는 weighted sum과 input weighted sum의 곱을 표현한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 논의를 간단하게 하기 위해 하나의 factor 연산만을 생각해보자. 하나의 factor에서 'next hidden layer'로 들어가는 입력 $\\mathbb{c}_f$를 아래와 같이 나타낼 수 있다.\n",
    "\n",
    "$$\\mathbf{c}_f = (\\mathbf{b}^T \\mathbf{w}_f ) (\\mathbf{a}^T \\mathbf{u}_f ) \\mathbf{v}_f$$\n",
    "\n",
    "또한 위 식에서 $\\mathbf{a}$와 $\\mathbf{v}$의 순서를 바꿔서 아래와 같이 표현할 수 있다.\n",
    "\n",
    "$$\\mathbf{c}_f = (\\mathbf{b}^T \\mathbf{w}_f ) (\\mathbf{u}_f \\mathbf{v}^T_f ) \\mathbf{a}$$\n",
    "위 식에서 $\\mathbf{b}^T \\mathbf{w}_f$는 어떤 scalar 값이고, $\\mathbf{u}_f \\mathbf{v}^T_f$는 rank가 1인 transition matrix($\\in \\mathbb{R}^{m \\times m}$)가 된다. 이때 $\\mathbf{c}_f$을 구성하는데 필요한 weight의 수는 $m + m + n$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 이런 factor $f$가 $j$개 존재하고 이들을 모두 더하여 next hidden layer state $\\mathbf{c}$를 아래와 같이 나타낼 수 있다.(아래 식에서 $\\sum_f (\\mathbf{b}^T \\mathbf{w}_f ) (\\mathbf{u}_f \\mathbf\n",
    "{v}^T_f )$는 rank가 1인 $j$개의 $m \\times m$ transition matrix들을 더한 결과임)\n",
    "\n",
    "$$\\mathbf{c} = \\left( \\sum_{f=1}^{j} (\\mathbf{b}^T \\mathbf{w}_f ) (\\mathbf{u}_f \\mathbf\n",
    "{v}^T_f )\\right) \\mathbf{a}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 전체 네트워크에 필요한 weight의 수는 $(m + m + n)j$개 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Character modeling using factors<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character string의 전체 모델은 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_factor_character.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇가지 살펴볼 점은 input vector $\\mathbf{b}$는 1-of-86의 sparse vector이므로,$k$번째 문자에 대한 $\\mathbf{b}^T \\mathbf{w}_f$는 결국 $w_{kf}$이라는 것이고 이 값이 trainsition matrix에 곱해져, gain과 같은 역할을 하게 된다. 그리고 hidden state vector를 softmax(86 units)에 입력하여 다음 문자에 대한 예측값을 얻게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 8c, Learning to predict the next character using HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Hessian-Free optimizer를 사용하여 optimize한 character string model을 살펴보도록 하자.\n",
    "\n",
    "Geoffrey Hinton의 제자인 Ilya Sutskever(이사 스키바)는 wikipedia에서 500만개의 문자열을 이용하여 11번째 다음 단어를 예측하는 모델을 만들었다. 이때 HF optimizer를 사용 해쏙, GPU board를 이용해서 한달간 학습 시켰다. 이 모델은 (강의 시점에) 현존하는 가장 성능이 좋은 단일 character predictor or generator이다.\n",
    "\n",
    "이 모델을 학습시킨 후 character를 생성하는 과정은 아래와 같다.\n",
    "- 우선 default hidden state으로 시작\n",
    "- 물론 모델이 11번째 다음 단어를 예측하도록 할수 있지만, 모형을 안정화 시키기 위해서 어느정도의 버려지는(burn-in) 문자열을 입력한다.\n",
    "- 어느정도 문자열을 입력 했으면 이제 다음에 나올 문자를 예측하기 위해 다음 문자에 대한 확률 분포를 확인한다.\n",
    "- 앞서 확률 분포의 확률로 100개 문자중 랜덤하게 한 character를 선택한다.\n",
    "- 앞서 뽑은 character가 정답이라 가정하고 계속 문자들을 뽑는다.\n",
    "\n",
    "이 모델이 생성한 문장중 하나를 보면 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_ilya1.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 내용은 횡설수설하는 것 같지만, 문장을 구성하는 단어 3~4개 정도만을 보면 대단히 자연스럽다는 것을 확인 할 수 있다. 물론 emphemerable과 같이 영어가 아닌 단어를 생성하기도 하지만 얼핏 보면 정말 영어 단어라고 생각할만큼 자연스러운 형태의 단어이다. 또한 RTUS 다음의 닫는괄호와 같이 괄호 짝을 잘 못 맞추는 경우도 있다.\n",
    "\n",
    "다음으로 영어에 없는 동사처럼 생긴 단어 'thrunge'를 이용하여 몇가지 테스트를 해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_ilya2.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sheila thrunge? : Sheila가 어떤 이름이라서 thrunge라는 동사 다음에 s를 붙이는 예측을 하고 있다.\n",
    "- People thrunge? : People이 복수 이므로 thrunge다음에 (s없이) 빈칸을 예측 했다.\n",
    "- Shiela, Thrunge???? : thrunge의 첫 글자를 대문자로 썼기 때문에 어떤 이름일 것이라는 가정으로 Thrungelini del Rey라는 이름을 만들어 냈다.\n",
    "- The meaning of life is ??? : 다음에 literary recognition이라는 그럴듯해 보이는 예측을 하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 이용한 next word prediction은 여러 장점이 있는데, 우선 다른 방법론에 비해 비슷한 수준의 성능을 내기 위한 training data의 크기가 작다는 점과 데이터셋의 크기가 커질 수록 모델의 성능이 월등히 좋아진다는 점 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 8d, Echo State Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 비유로 시작해 보면, 연못이 하나 있다고 했을 때, 이 연못의 특정 위치에 특정 높이에서 돌을 하나 떨어뜨리면 구슬의 낙하점을 중심으로 물결이 퍼져 나갈 것이다. (연못이 충분히 넓어서 경계에서 반사되는 파동은 없다고 가정하자.) 시간이 지나면 물결을 잦아들 것이고, 이번에는 다른 크기의 돌을 같은 위치와 같은 높이에서 떨어뜨리면 더 다른 파고의 물결이 발생할 것이다. 이 두 상황에서 연못의 특정 위치에서 파동을 측정하면 각 물결이 다르기 때문에 물결을 일으킨 물체도 다르다는 것을 알 수 있다. 이번에는 물결이 잦아들기 전에 같은 위치에 같은 돌을 떨어뜨리면 기존의 물결이 더 커진 형태로 전달될 것이고, 이 물결을 측정하여 기존에 물결을 발생시켰던 것과 같은 원인이 다시 발생했음을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to hidden layer, hidden layer to hidden layer의 weight을 random하게 할당했을때, input value가 hidden layer에서 전파되는 것을 앞서 비유와 같이 여러 signal들이 non-linear dynamics를 주고 받는 것과 같아진다. 강의에서는 reservoir of coupled oscillator라고 표현하는데, 이는 여러 작용점을 갖는 용수철이 무작위로 연결되어 있는 형태이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 network에서는 다른 모든 weight들은 학습할 필요가 없고 단지 last hidden layer to output layer사이의 weight만을 학습시키면 된다. 나머지는 random으로 생성된 고정된 값을 사용한다. 또한 hidden layer의 unit에는 logistic function과 같은 non-linear function을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 예를 살펴보면 아래와 같은 시간에 따라 변하는 실수값을 갖는 input signal이 주어지는데, 이 값은 output으로 나올 sine wave의 frequency를 나타낸다. dynamical reservoir(loosely coupled oscillators)에서 다양한 형태로 전달 되고, 결국 sine wave를 출력으로 내 놓는다. 모델 학습은 dynamical reservoir의 last hidden layer의 출력과 target output을 이용하여 단순한 linear model로 두 layer 사이의 weight을 학습 시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_esn1.png\"/>\n",
    "<h6 align='right'>http://www.scholarpedia.org/article/Echo_state_network</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 결과를 보면 input signal 앞부분의 3가지 상태값이 sine wave의 frequency로 표현됨을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"08_esn2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echo State Networks은 마지막 hidden layer값을 linear model로 학습시키기만 하면 되기 때문에 그 학습 속도가 매우 빠른 장점이 있고, weight값을 잘 초기화 하는 것이 얼마나 중요한지 잘 보여준다고 할 수 있다. 또한 1차원 time-series 데이터를 모델링하는데 아주 효과적이다. 하지만 음성과 같은 고차원 데이터를 모델링 하기에는 한계가 있다. 또한 RNN에 비해 훨신 많은 수의 hidden unit이 필요한 단점이 있다. 최근에는 ESN에서의 초기화 기법을 이용하는 RNN이 매우 효과적일 수 있다는 것이 실증적으로 밝혀 졌다."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
