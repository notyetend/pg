{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 Neural Networks for Machine Learning 강의를 정리한 내용임.   \n",
    "2016.11.14. by Dongwan Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week14. Deep neural nets with generative pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 14a, Learning layers of features by stacking RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 장 에서는 Sigmoid belief net을 학습시키는 다른 방법에 대해서 알아보겠다. 이 방법은 사실 예상치 못하게 발견된 측면이 있는데, 사실 Hinton은 Boltzmann machine에 대한 연구 때문에 Sigmoid Belief net에 대한 연구를 별로 진행하지 않고 있었는데, 이때 Restricted Boltzmann machine을 이용해 nonlinear한 feature들을 쉽게 학습시킬 수 있다는 것을 발견하게 된다. 그렇다면 이렇게 학습된 feature들을 data로 취급하여 추가적인 Restriected Boltzmann machine을 학습시켜 feature들 간의 correlation을 모델링할 수 있지 않을까 하는 아이디어를 갖게 된다. 즉 RBM위에 또 다른 RBM을 쌓는 형식으로 많은 layer의 nonlinear feature을 학습시키겠다는 것이다. 이런 아이디어로 인해 Deep neural net에 대한 연구자들의 관심이 증가하게 된다.\n",
    "\n",
    "그런데 이 아이디어에서 많은 RMB을 쌓았을 때 이것이 여러개의 독립적인 RBM인 것인지 혹은 이것을 하나의 모델로 간주해야할지가 이슈가 된다. 얼핏 보면 이런 중첩 모델이 하나의 'multi-layer Boltzmann machine'이지 않을까 생각할 수 있는데, Hinton의 제자인 <a href=\"https://www.stats.ox.ac.uk/~teh/\">Yee Whye Teh</a>가 이런 중첩 모델이 하나의 'multi-layer Boltzmann machine'가 아니고 대신 Sigmoid belief net과 유사한 것임을 밝혀 낸다.\n",
    "\n",
    "이는 Deep sigmoid belief net을 학습시키는 문제를 (우회적으로) Boltzmann machine과 같은 undirected model을 학습시키는 방식으로 해결할 수 있었을 것이라는 것 때문에 Yee Whye Teh의 발견은 Hinton에게는 대단히 놀라운 것이었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Train a deep network by stacking RBMs</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 이미지를 입력으로 받는 RBM을 생각해보자. 이 RBM을 Learning 시킬 수 있고(Lecture 12.c 참고) 이렇게 학습된 RBM의 hidden layer activation들을 다른 RBM의 입력으로 사용할 수 있다. 이런 RBM 중첩을 여러번 반복할 수 있고, 각 RMB layer는 이전 RBM의 activation에 존재하는 correlation을 모델링한다고 볼 수 있다. \n",
    "\n",
    "우리가 RBM을 중첩할 때마다 이 중첩 모델이 해당 데이터를 generate할 log probability의 variational lower을 개선하는(높이는) 효과가 있다는 것을 증명할 수 있다. 증명은 사실 좀 복잡한데, RBM이 결국 infinitely deep belief net와 같다는 사실을 기반으로 한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./_images/12_rbm1.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Combining two RBMs to make a DBN</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 예로 두개의 RMB을 중첩해보고 이것이 결국 하나의 Deep Belief net이 되는 과정을 살펴보자.\n",
    "\n",
    "우선 아래 좌측 하단과 같이 하나의 RBM에 data가 주어졌을 때 이를 학습시킨다. 그리고 이때 hidden layer($h_1$)의 activation vector를 두번째 RBM(아래 좌측 상단)의 입력으로 사용하여 두번째 RBM을 학습시킨다.\n",
    "\n",
    "그런데 흥미로운 점은 두번째 RBM의 hidden layer $h_2$의 unit 수가 첫번째 RBM의 visible layer $v$의 unit수 만큼이면, 두번째 RBM의 weight matrix($W_2$) 자리에 첫번째 RBM의 weight matrix($W_1$)의 transpose를 사용하면 이미 두번째 RBM이 $h_1$에 대한 좋은 모델이 된다는 것이다. 이것은 두번째 RBM이 첫번째 RBM을 뒤집은 형태이기 때문이다. \n",
    "\n",
    "이제 두 RBM을 하나로 합치면 아래 우측의 모형이 되는데, 상단의 두 layer $h_2$와 $h_1$은 좌측 상단의 RBM와 같다고 할 수 있다. 즉 symmetric weight을 사용하는 undirected graph이다. 반면 하단의 두 layer는 조금 다르다. 이 경우 sigmoid belief net과 같은 directed graph이다. 즉 앞서 좌측 하단 RBM에서 $v$에서 $h_1$으로 향하는 weight을 없앤 형태가 된다. (왜 이렇게 되는지는 상당히 복잡한 설명이 필요하며, Lecture 14e에서 살펴보도록 하겠다.\n",
    "\n",
    "이렇게 만들어진 우측의 (합쳐진) 모형은 하단에 non symmetric connection을 갖고 있으므로 RBM은 분명 아니다. 이는 Deep belief net이라 부르는 graphical model인데, 하단의 두 layer는 sigmoid belief net이고 상단의 두 layer는 RBM인 형태인 hybrid model이라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./_images/14_stackingRBM.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The generative model after learning 3 layers</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBM을 3개 쌓을 경우 아래와 같은 hybrid model이 만들어진다. 상단의 두 layer는 RBM이고, 두번째 layer부터는 sigmoid belief net과 같은 directed graph이다.\n",
    "\n",
    "이 모델에서 data를 generate하기 위해서는 (즉 weight들이 정해져 있고 model에서 데이터 생성)\n",
    "- 우선 $h2$와 $h3$를 RBM으로 간주하고 이 두 layer를 번갈아 업데이트하여 thermal equilibrium에 이르도록 한다. 이 과정은 alternating gibbs sampling을 이용해 (아마도 한 layer에서 update할 unit을 여러개 선택하는 것으로 보임) $h_2$와 $h_3$의 unit들을 병렬로 update하는 것이다. 사실 이 RBM은 $h_2$에 대한 prior distribution을 만드는 과정이라 할 수 있다.\n",
    "- 다음으로 앞서 thermal equilibrium에 다다른 RBM의 $h_2$에서 activation vector를 얻고 이를 weight이 $W_2$인 generative connection을 통해 $h_1$으로 보낸다. 그리고 다시 한번 weight이 $W_1$인 generative connection을 통해 data를 생성한다. (사실 $h_2$에서 data까지의 과정은 단순히 sigmoid belief net에서의 그것과 동일하다.)\n",
    "\n",
    "아래 그림에서 붉은색 화살표는 사실 모델에 포함된 내용이 아니다. 이것들은 각각에 대응하는 녹색 weight matrix의 transpose이다. 이것들은 사실 추론(inference)에 사용되는 것들이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"./_images/14_stacking3RBM.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>An aside: Averaging factorial distributions</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Boltzmann machine을 쌓는 것(stacking up)이 왜 좋은 아이디어 인지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Why does greedy learning work?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Fine-tuning with a contrastive version of the wake-sleep algorithm</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The DBN used for modeling the joint distribution of MNIST dights and their labels</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 14b, Discriminative fine-tuning for DBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Fine-tuning for discrimination</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Why backpropagation works better with greedy pre-training: The optimization view</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Why backpropagation works better with greedy pre-training: the overfitting view</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>First, model the distribution of digit images</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Results on the permuatation-invariant MINIST task</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Unsupervised \"pre-training\" also helps for models that have more data and better priors</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Phone recognition on the TIMIT benchmark</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 14c, What happens during discriminative fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Learning Dynamics of Deep Nets</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Effect of Unsupervised Pre-training</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Effect of Depth</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Trajectories of the learning in function space</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Why unsupervised pre-training makes sense</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 14d, Modeling real-valued data with an RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Modeling real-valued data</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>A standard type of real-valued visible unit</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Gaussian-Binary RBM's</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Stepped sigmoid units: A neat way to implement integer values</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Fast approximations</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>A nice property of rectified linear units</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 14e, RBMs are Infinite Sigmoid Belief Nets (ADVANCED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Another view of why layer-by-layer learning works</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>An infinite sigmoid belief net that is equivalent to an RBM</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Inference in an infinite sigmoid belief net</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Learning a deep directed network</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>What happens when the weights in higher layers become different from the weights in the first layer?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>What is really happening in contrastive divergence learning?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Why is it OK to ignore the derivatives in higher layers?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
