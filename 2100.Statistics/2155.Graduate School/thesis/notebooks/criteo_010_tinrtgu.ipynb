{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://kaggle2.blob.core.windows.net/forum-message-attachments/53646/1539/fast_solution.py?sv=2012-02-12&se=2015-12-04T20%3A40%3A32Z&sr=b&sp=r&sig=qTDaOlHCMWaqBB9aOK6haM6Vo2FmmkfopqtwQaexnC0%3D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "                   Version 2, December 2004\n",
    "\n",
    "Copyright (C) 2004 Sam Hocevar <sam@hocevar.net>\n",
    "\n",
    "Everyone is permitted to copy and distribute verbatim or modified\n",
    "copies of this license document, and changing it is allowed as long\n",
    "as the name is changed.\n",
    "\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "  TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n",
    "\n",
    " 0. You just DO WHAT THE FUCK YOU WANT TO.\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "\n",
    "# parameters #################################################################\n",
    "#train = 'train.csv'  # path to training file\n",
    "#test = 'test.csv'  # path to testing file\n",
    "\n",
    "#train = r'C:\\Users\\NYE\\Downloads\\dac.tar\\train.txt'\n",
    "#test = r'C:\\Users\\NYE\\Downloads\\dac.tar\\test.txt'\n",
    "\n",
    "train = r'D:\\etc\\Thesis\\data\\dac_sample.tar\\train.txt'\n",
    "test = r'D:\\etc\\Thesis\\data\\dac_sample.tar\\test.txt'\n",
    "\n",
    "\n",
    "D = 2 ** 20   # number of weights use for learning\n",
    "alpha = .1    # learning rate for sgd optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function definitions #######################################################\n",
    "\n",
    "# A. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Apply hash trick of the original csv row\n",
    "# for simplicity, we treat both integer and categorical features as categorical\n",
    "# INPUT:\n",
    "#     csv_row: a csv dictionary, ex: {'Lable': '1', 'I1': '357', 'I2': '', ...}\n",
    "#     D: the max index that we can hash to\n",
    "# OUTPUT:\n",
    "#     x: a list of indices that its value is 1\n",
    "def get_x(csv_row, D):\n",
    "    x = [0]  # 0 is the index of the bias term\n",
    "    for key, value in csv_row.items():\n",
    "        index = int(value + key[1:], 16) % D  # weakest hash ever ;)\n",
    "        x.append(index)\n",
    "    return x  # x contains indices of features that have a value of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_p**\n",
    "\n",
    "아래 get_p 함수를 살펴보자.  \n",
    "여기서 인자로 받는 x는 특정 row의 bucket번호를 갖고 있는 array이다. 각 row마다 자신이 해당하지 않는 bucket의 weight들은 전혀 의미가 없고, 모든 범주들이 hashing되어 있으므로 해당하는 bucket에 대응하는 x값은 모두 1이라고 할 수 있다. 따라서 wTx를 구하기 위하여 해당하는 bucket들의 weight을 단순히 더해주면 된다. 이 linear combination값을 inverse sigmoid function에 넣어 bernulli trial의 성공확률 theta를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def get_p(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D. Update given model\n",
    "# INPUT:\n",
    "#     w: weights\n",
    "#     n: a counter that counts the number of times we encounter a feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# OUTPUT:\n",
    "#     w: updated model\n",
    "#     n: updated count\n",
    "def update_w(w, n, x, p, y):\n",
    "    for i in x:\n",
    "        # alpha / (sqrt(n) + 1) is the adaptive learning rate heuristic\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1\n",
    "        w[i] -= (p - y) * alpha / (sqrt(n[i]) + 1.)\n",
    "        n[i] += 1.\n",
    "\n",
    "    return w, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 테스트 코드를 통해서 무슨 일이 일어나고 있는지 살펴보자.   \n",
    "\n",
    "우선 우리가 다루려는 데이터는 Y값이 0/1의 binomial이고, I1부터 I13까지의 13개의 수치형으로 보이는 변수(물론 의미적으로는 범주형 일수도 있음)와 C1부터 C26까지 26개의 범주형 변수로 이루어져 있다. 그런데 각 범주형 변수의 범주의 수가 수천개 혹은 그 이상일 수도 있다. 이처럼 범주가 많고 기존에는 없던 범주가 언제든지 추가될 수 있는 상황이고 게다가 이런 데이터를 실시간 처리 해야 할 경우 모든 발생 가능한 범주는 상정해서 dummy variable을 생성할 수가 없다. 따라서 이런 경우에는 어느정도의 collision을 감수하더라도 hash tric을 사용하는 것이 합리적인 선택이다. 또한 데이터의 sample수가 많기 때문에 collision으로 인한 영향이 크지 않다고 볼 수 있다.\n",
    "\n",
    "이 예에서는 2의 20승개의 bucket을 사용하려 한다. 경우에 따라서 더 많은 수 혹은 적은 수의 bucket을 사용할 수도 있을 것이다. 변수 D는 bucket의 수를 나타내고, w은 D개의 weight array를 나타낸다. \n",
    "\n",
    "여기에서의 hashing 방법은 **get_x** 함수로 구현되어 있다. 이를 살펴보면 csv_row는 dict type으로 한 record를 의미한다. 하나의 record를 받는데, x는 이 record가 값을 갖는 bucket 번호를 갖는 array이다. 하나의 record를 담은 dict인 csv_row에서 items()라는 함수를 사용하여 (key, value)단위로 iteration을 다시 돌게 된다. 각 key, value는 str값으로서, value값과 key의 두번째 문자부터를 concat한 후 이를 16진수로 보고 10진수로 변환한다. 예를들어 0번 row의 'I6' 변수의 값은 4이므로   value + key[1:] 의 값은 46이 된다. 이를 16진수로 보고 10진수 변환하게 되면 16x4 + 6 = 70이 된다. 마지막으로 bucket수인 D로 modular하면, bucket수로 제한된 숫자가 나오게 된다. 이 경우 70은 bucket수 보다 한참 작으므로 그대로 70이 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "0.5\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "\n",
    "w = [0.] * D  # weights\n",
    "n = [0.] * D  # number of times we've encountered a feature\n",
    "loss = 0.\n",
    "\n",
    "\n",
    "sample = r'D:/etc/Thesis/data/dac_sample.tar/dac_sample.txt'\n",
    "\n",
    "f = open(sample)\n",
    "fn = ['Label'] + [ 'I' + str(i) for i in list(range(1,14))] + [ 'C' + str(i) for i in list(range(1,27))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):\n",
    "    if t == 0:\n",
    "        #print('%s | %s' % (t, row))\n",
    "        y = 1. if row['Label'] == '1' else 0.\n",
    "        del row['Label']  # can't let the model peek the answer\n",
    "        #print('%s | %s' % (t, row))\n",
    "        \n",
    "        x = get_x(row, D)\n",
    "        for key, value in row.items():\n",
    "            if key == 'I6':\n",
    "                print(int(value + key[1:], 16) % D)\n",
    "        \n",
    "        p = get_p(x, w)\n",
    "        print(p)\n",
    "        \n",
    "        loss = logloss(p, y)\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-04 03:16:15.760836\tencountered: 1000000\tcurrent logloss: 0.473722\n",
      "2015-12-04 03:17:58.278966\tencountered: 2000000\tcurrent logloss: 0.468350\n",
      "2015-12-04 03:19:41.453531\tencountered: 3000000\tcurrent logloss: 0.466356\n",
      "2015-12-04 03:21:26.204966\tencountered: 4000000\tcurrent logloss: 0.465526\n",
      "2015-12-04 03:22:57.902531\tencountered: 5000000\tcurrent logloss: 0.464644\n",
      "2015-12-04 03:24:21.518360\tencountered: 6000000\tcurrent logloss: 0.463928\n",
      "2015-12-04 03:25:45.492840\tencountered: 7000000\tcurrent logloss: 0.463666\n",
      "2015-12-04 03:27:09.426853\tencountered: 8000000\tcurrent logloss: 0.463781\n",
      "2015-12-04 03:28:33.313838\tencountered: 9000000\tcurrent logloss: 0.463295\n",
      "2015-12-04 03:29:57.276867\tencountered: 10000000\tcurrent logloss: 0.463704\n",
      "2015-12-04 03:31:21.061795\tencountered: 11000000\tcurrent logloss: 0.463851\n",
      "2015-12-04 03:32:44.879741\tencountered: 12000000\tcurrent logloss: 0.463777\n",
      "2015-12-04 03:34:08.922518\tencountered: 13000000\tcurrent logloss: 0.463684\n",
      "2015-12-04 03:35:32.851917\tencountered: 14000000\tcurrent logloss: 0.464402\n",
      "2015-12-04 03:36:56.798900\tencountered: 15000000\tcurrent logloss: 0.464294\n",
      "2015-12-04 03:38:20.636408\tencountered: 16000000\tcurrent logloss: 0.464274\n",
      "2015-12-04 03:39:44.497379\tencountered: 17000000\tcurrent logloss: 0.464271\n",
      "2015-12-04 03:41:08.383364\tencountered: 18000000\tcurrent logloss: 0.464256\n",
      "2015-12-04 03:42:32.338389\tencountered: 19000000\tcurrent logloss: 0.464048\n",
      "2015-12-04 03:43:56.186352\tencountered: 20000000\tcurrent logloss: 0.464077\n",
      "2015-12-04 03:45:19.976283\tencountered: 21000000\tcurrent logloss: 0.464015\n",
      "2015-12-04 03:46:44.014795\tencountered: 22000000\tcurrent logloss: 0.463479\n",
      "2015-12-04 03:48:08.067876\tencountered: 23000000\tcurrent logloss: 0.463600\n",
      "2015-12-04 03:49:32.003398\tencountered: 24000000\tcurrent logloss: 0.463450\n",
      "2015-12-04 03:50:56.211701\tencountered: 25000000\tcurrent logloss: 0.463238\n",
      "2015-12-04 03:52:20.271785\tencountered: 26000000\tcurrent logloss: 0.463050\n",
      "2015-12-04 03:53:43.829582\tencountered: 27000000\tcurrent logloss: 0.463105\n",
      "2015-12-04 03:55:07.459421\tencountered: 28000000\tcurrent logloss: 0.463153\n",
      "2015-12-04 03:56:31.282370\tencountered: 29000000\tcurrent logloss: 0.463142\n",
      "2015-12-04 03:57:55.050289\tencountered: 30000000\tcurrent logloss: 0.463218\n",
      "2015-12-04 03:59:18.921265\tencountered: 31000000\tcurrent logloss: 0.463161\n",
      "2015-12-04 04:00:42.837268\tencountered: 32000000\tcurrent logloss: 0.463042\n",
      "2015-12-04 04:02:06.771280\tencountered: 33000000\tcurrent logloss: 0.462936\n",
      "2015-12-04 04:03:30.468157\tencountered: 34000000\tcurrent logloss: 0.463195\n",
      "2015-12-04 04:04:54.291107\tencountered: 35000000\tcurrent logloss: 0.463283\n",
      "2015-12-04 04:06:18.273147\tencountered: 36000000\tcurrent logloss: 0.463242\n",
      "2015-12-04 04:07:42.237177\tencountered: 37000000\tcurrent logloss: 0.463310\n",
      "2015-12-04 04:09:06.260240\tencountered: 38000000\tcurrent logloss: 0.463217\n",
      "2015-12-04 04:10:30.067180\tencountered: 39000000\tcurrent logloss: 0.462850\n",
      "2015-12-04 04:11:53.906138\tencountered: 40000000\tcurrent logloss: 0.462564\n",
      "2015-12-04 04:13:17.739094\tencountered: 41000000\tcurrent logloss: 0.462859\n",
      "2015-12-04 04:14:41.790172\tencountered: 42000000\tcurrent logloss: 0.462879\n",
      "2015-12-04 04:16:05.876272\tencountered: 43000000\tcurrent logloss: 0.463001\n",
      "2015-12-04 04:17:29.817289\tencountered: 44000000\tcurrent logloss: 0.463061\n",
      "2015-12-04 04:18:53.650243\tencountered: 45000000\tcurrent logloss: 0.463056\n"
     ]
    }
   ],
   "source": [
    "# training and testing #######################################################\n",
    "\n",
    "# initialize our model\n",
    "w = [0.] * D  # weights\n",
    "n = [0.] * D  # number of times we've encountered a feature\n",
    "\n",
    "# start training a logistic regression model using on pass sgd\n",
    "loss = 0.\n",
    "\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(train)\n",
    "fn = ['Label'] + [ 'I' + str(i) for i in list(range(1,14))] + [ 'C' + str(i) for i in list(range(1,27))]\n",
    "\n",
    "\n",
    "# t : int\n",
    "# row : dict, this dict could be accessed with key. for example row['Label'] returns y value.\n",
    "for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):\n",
    "#for t, row in enumerate(DictReader(open(train))):\n",
    "    y = 1. if row['Label'] == '1' else 0. # Because data type of 'Label' is str, converting to numeric 1 or 0.\n",
    "\n",
    "    del row['Label']  # can't let the model peek the answer\n",
    "    #del row['Id']  # we don't need the Id, but there is no 'Id' column in the data that we can get after the kaggle contest.\n",
    "\n",
    "    # main training procedure\n",
    "    # step 1, get the hashed features\n",
    "    x = get_x(row, D) # x contains the bucket numbers that has the value for current row.\n",
    "\n",
    "    # step 2, get prediction\n",
    "    p = get_p(x, w)\n",
    "\n",
    "    # for progress validation, useless for learning our model\n",
    "    loss += logloss(p, y)\n",
    "    if t % 1000000 == 0 and t > 1:\n",
    "        print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "            datetime.now(), t, loss/t))\n",
    "\n",
    "    # step 3, update model with answer\n",
    "    w, n = update_w(w, n, x, p, y)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing (build kaggle's submission file)\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(test)\n",
    "with open('submission1234.csv', 'w') as submission:\n",
    "    submission.write('Predicted\\n')\n",
    "    #for t, row in enumerate(DictReader(open(test))):\n",
    "    for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):\n",
    "        #Id = row['Id']\n",
    "        #del row['Id']\n",
    "        y = 1. if row['Label'] == '1' else 0.\n",
    "        del row['Label']  # can't let the model peek the answer\n",
    "    \n",
    "        x = get_x(row, D)\n",
    "        p = get_p(x, w)\n",
    "        submission.write('%f\\n' % (p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
