{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1. K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering은 (클러스터 갯수가 정해져 있다고 할 때) 아래와 같은 과정으로 데이터를 나눈다.\n",
    "1. 우선 초기 클러스터 중심 위치($\\boldsymbol{\\mu}_k \\in \\mathbb{R}^{D}$, prototype vector)를 적당히 잡는다.\n",
    "2. $n$개 데이터 샘플($\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$)을 클러스터 중심과의 거리가 가장 짧은 클러스터에 할당한다.\n",
    "3. 각 $k$개 클러스터에 할당된 점들의 중심으로 클러스터의 중심을 다시 잡는다.\n",
    "4. $n$개점의 클러스터 할당이 바뀌지 않을때까지 혹은 정해진 반복 횟수만큼 '2 단계'부터 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 2번째 단계를 EM algorithm의 M(maximization) step이라 할 것이고,     \n",
    "3번째 단계를 E(expectation) step이라 할 것이다.    \n",
    "왜 이렇게 부르는지는 이후 설명된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1. Image segmentation and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2. Mixtures of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Mixtures of Gaussian'은 $K$개의 gaussian($\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )$) 각각에 mixing coefficient $\\pi_k$ 곱하고 더한 아래와 같은 형태이다.(2.3.9에 처음 등장)\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) \\qquad (9.7)$$\n",
    "- mixing coefficient $\\pi_k$는 0보다 크거나 같아야 하고, 더해서 1이 되어야 한다.\n",
    "\\begin{align} \n",
    "0 \\leq &\\pi_k \\leq 1 \\\\\n",
    "\\sum_{k=1}^K &\\pi_k = 1 \\qquad (9.9)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $k$번째 원소가 1일 확률이 $\\pi_k$인 1-of-K 형태의 $K$차원 이항 확률변수 $\\mathbf{z}$가 있다고 하자.      \n",
    "($K$ 차원 벡터($z_1, \\dots, z_k, \\dots, z_K$)가 있는데 하나의 값만 1이고 나머지는 0이다.)\n",
    "- 즉 mixing coefficient $\\pi_1, \\dots, \\pi_k, \\dots, \\pi_K$들은 확률변수 $\\mathbf{z}$의 확률분포라고 할 수 있다.\n",
    "\\begin{align}\n",
    "&z_k \\in \\{0, 1\\} \\\\\n",
    "\\sum_k &z_k = 1 \\\\\n",
    "p(&z_k = 1) = \\pi_k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률변수 $\\mathbf{z}$의 값은 '1-of-K' 형태이므로 $p(\\mathbf{z})$를 아래와 같이 나타낼 수 있다.\n",
    "$$p(\\mathbf{z}) = \\prod_{k=1}^K\\pi_k^{z_k} \\qquad (9.10)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p(\\mathbf{x})$가 'mixtures of gaussian'이므로 $k$번째 gaussian은 '$z_k = 1$'일때 $p(\\mathbf{x})$의 확률 분포, 즉 $p(\\mathbf{x} \\mid z_k=1)$이라 할 수 있다.\n",
    "$$p(\\mathbf{x} \\mid z_k = 1) = \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률변수 $\\mathbf{z}$의 값은 '1-of-K' 형태이므로 $p(\\mathbf{z})$를 아래와 같이 나타낼 수 있다.\n",
    "$$p(\\mathbf{x} \\mid \\mathbf{z}) = \\prod_{k=1}^K\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )^{z_k} \\qquad (9.11)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 $p(\\mathbf{z})$와 $p(\\mathbf{x} \\mid \\mathbf{z})$를 이용해 'mixtures of gaussian', $p(\\mathbf{x})$를 아래와 같이 표현할 수 있다.\n",
    "$$p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{z})p(\\mathbf{x} \\mid \\mathbf{z}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) \\qquad (9.12)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- figure 9.4와 같이 $p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{z})p(\\mathbf{x} \\mid \\mathbf{z})$를 graphical model로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 돌아 돌아 같은 표현에 도달한 것 같지만, $p(\\mathbf{x})$를 $p(\\mathbf{x}, \\mathbf{z})$을 이용해 표현할 수 있게 된 것이고, 이는 나중에 유용하게 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또한가지 나중에 중요하게 사용될 것은 '$\\mathbf{x}$가 주어졌을 때 $\\mathbf{z}$의 조건부 분포 $\\gamma(z_k)$이다.\n",
    "$$\\gamma(z_k) \\equiv p(z_k = 1 \\mid \\mathbf{x}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) }{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j ) } \\qquad (9.13)$$\n",
    " - 여기서 중요한 것은 $\\pi_k$는 $z_k = 1$일 '사전' 확률이고, $p(z_k = 1 \\mid \\mathbf{x})$는 데이터를 보고난 후 '사후' 확률이라는 점이다.\n",
    " - 이 녀석 $\\gamma(z_k)$을 **responsibility**이라 부를 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature가 $D$개인 $n$개의 샘플 데이터 $\\{ \\mathbf{x}_1, \\dots, \\mathbf{x}_N \\}$가 있다고 하자.(이것을 $N \\times D$ 행렬 $\\mathbf{X}$로 표기)\n",
    "- 각 샘플은 서로 독립이라고 가정하면, 이 데이터에 대한 분포를 $N$개의 'mixtures of gaussian'으로 표현할 수 있다.@@@ (figure 9.6의 graphical model로 표현할 수 있다. figure 9.4와는 다르다.) 또한 이때 $N$개의 샘플에 대한 likelihood는 아래와 같다.\n",
    "\n",
    " - likehood for the incomplete data set $\\{\\mathbf{X} \\}$\n",
    "\\begin{align}\n",
    "p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\prod_{n=1}^N  p(\\mathbf{x}_n \\mid \\boldsymbol{\\pi}_n, \\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n ) \\\\\n",
    "&=\\prod_{n=1}^N \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\\\\n",
    "\\end{align}\n",
    "  - log likehood for the incomplete data set $\\{\\mathbf{X} \\}$, eq 9.14\n",
    "\\begin{align}\n",
    "\\ln p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\sum_{n=1}^N \\ln p(\\mathbf{x}_n) \\\\\n",
    "&=\\sum_{n=1}^N \\ln \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\qquad (9.14)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 likelihood는 최대화하는 $\\{\\pi_1, \\dots, \\pi_N\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. EM for Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3. An Alternative View of EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. Gaussian mixtures revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. Relation to K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3. Mixtures of Bernoulli distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.4. EM for Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4. The EM Algorithm in General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian mixture distribution, eq 9.7\n",
    "\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- posterior probability once given observed $\\mathbf{x}$\n",
    "\n",
    "$$\\gamma(z_k) = p(z_k = 1 \\mid \\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- likehood for the incomplete data set $\\{\\mathbf{X} \\}$\n",
    "\\begin{align}\n",
    "p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\prod_{n=1}^N  p(\\mathbf{x}_n \\mid \\boldsymbol{\\pi}_n, \\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n ) \\\\\n",
    "&=\\prod_{n=1}^N \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\\\\n",
    "\\end{align}\n",
    " - log likehood for the incomplete data set $\\{\\mathbf{X} \\}$, eq 9.14\n",
    "\\begin{align}\n",
    "\\ln p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\sum_{n=1}^N \\ln p(\\mathbf{x}_n) \\\\\n",
    "&=\\sum_{n=1}^N \\ln \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- likelihood for the complete data set $\\{\\mathbf{X}, \\mathbf{Z}\\}$, eq 9.35\n",
    "$$p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)^{z_{nk}}$$\n",
    "\n",
    " - log likelihood for the complete data set $\\{\\mathbf{X}, \\mathbf{Z}\\}$, eq 9.36\n",
    "$$\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\sum_{n=1}^N \n",
    "\\sum_{k=1}^K \n",
    "z_{nk}\n",
    "\\big\\{\n",
    "\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\big\\}$$\n",
    "\n",
    " - expected value of the log likelihood for the complete data set $\\{\\mathbf{X}, \\mathbf{Z}\\}$\n",
    "$$\\mathbb{E}_{\\mathbf{Z}}[\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi})]\n",
    "=\\sum_{n=1}^N \n",
    "\\sum_{k=1}^K \n",
    "\\gamma(z_{nk})\n",
    "\\big\\{\n",
    "\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\big\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. The EM Algorithm in General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM 알고리즘은 '잠재변수를 포함하는 확률 모형'의 ML(최대 우도) 해를 찾는 일반적인 방법이다. 관측변수들의 데이터셋을 $\\mathbf{X}$, (각 관측 샘플에 대한) 잠재변수들을 $\\mathbf{Z}$, $p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})$의 파라미터들을 $\\boldsymbol{\\theta}$라 할 때, $\\mathbf{X}$에 대한 우도는 아래와 같고, 이를 최대화하는 $\\boldsymbol{\\theta}$를 찾는 것이 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- likehood for the incomplete data set $\\{\\mathbf{X} \\}$\n",
    "\\begin{align}\n",
    "p (\\mathbf{X} \\mid \\boldsymbol{\\theta} ) \n",
    "&=\\prod_{n=1}^N p(\\mathbf{x}_n \\mid \\boldsymbol{\\theta}_n) \\\\\n",
    "&=\\prod_{n=1}^N \\sum_{\\mathbf{z_n}} p(\\mathbf{x}_n, \\mathbf{z}_n \\mid \\boldsymbol{\\theta}_n) \\\\\n",
    "&= \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 로그 우도는 아래와 같이 두개 항으로 분해된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) \n",
    "&= \\mathcal{L}(q, \\boldsymbol{\\theta}) + \\text{KL}(q \\Vert p) \\\\\n",
    "&= \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\ln \\bigg\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\bigg\\} - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\bigg\\{ \\frac{p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}))}{q(\\mathbf{Z})} \\bigg\\} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식에서 $\\text{KL}$은 항상 0보다 크거나 같으므로 $p(\\mathbf{X} \\mid \\boldsymbol{\\theta})  \\geq \\mathcal{L}(q, \\boldsymbol{\\theta})$이다. 즉 $\\mathcal{L}(q, \\boldsymbol{\\theta})$은 $p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$의 lower bound이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
