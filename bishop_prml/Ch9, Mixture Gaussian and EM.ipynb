{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1. K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering은 (클러스터 갯수가 정해져 있다고 할 때) 아래와 같은 과정으로 데이터를 나눈다.\n",
    "1. 우선 초기 클러스터 중심 위치($\\boldsymbol{\\mu}_k \\in \\mathbb{R}^{D}$, prototype vector)를 적당히 잡는다.\n",
    "2. $n$개 데이터 샘플($\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$)을 클러스터 중심과의 거리가 가장 짧은 클러스터에 할당한다.\n",
    "3. 각 $k$개 클러스터에 할당된 점들의 중심으로 클러스터의 중심을 다시 잡는다.\n",
    "4. $n$개점의 클러스터 할당이 바뀌지 않을때까지 혹은 정해진 반복 횟수만큼 '2 단계'부터 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 2번째 단계를 EM algorithm의 M(maximization) 단계라 할 것이고,     \n",
    "3번째 단계를 E(expectation) 단계이라 할 것이다.    \n",
    "왜 이렇게 부르는지는 이후 설명된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1. Image segmentation and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2. Mixtures of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Mixtures of Gaussian'은 $K$개의 gaussian($\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )$) 각각에 mixing coefficient $\\pi_k$ 곱하고 더한 아래와 같은 형태이다.(2.3.9에 처음 등장)\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) \\qquad (9.7)$$\n",
    "- mixing coefficient $\\pi_k$는 0보다 크거나 같아야 하고, 더해서 1이 되어야 한다.\n",
    "\\begin{align} \n",
    "0 \\leq &\\pi_k \\leq 1 \\\\\n",
    "\\sum_{k=1}^K &\\pi_k = 1 \\qquad (9.9)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $k$번째 원소가 1일 확률이 $\\pi_k$인 1-of-K 형태의 $K$차원 이항 확률변수 $\\mathbf{z}$가 있다고 하자.      \n",
    "($K$ 차원 벡터($z_1, \\dots, z_k, \\dots, z_K$)가 있는데 하나의 값만 1이고 나머지는 0이다.)\n",
    "- 즉 mixing coefficient $\\pi_1, \\dots, \\pi_k, \\dots, \\pi_K$들은 확률변수 $\\mathbf{z}$의 확률분포라고 할 수 있다.\n",
    "\\begin{align}\n",
    "&z_k \\in \\{0, 1\\} \\\\\n",
    "\\sum_k &z_k = 1 \\\\\n",
    "p(&z_k = 1) = \\pi_k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률변수 $\\mathbf{z}$의 값은 '1-of-K' 형태이므로 $p(\\mathbf{z})$를 아래와 같이 나타낼 수 있다.\n",
    "$$p(\\mathbf{z}) = \\prod_{k=1}^K\\pi_k^{z_k} \\qquad (9.10)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p(\\mathbf{x})$가 'mixtures of gaussian'이므로 $k$번째 gaussian은 '$z_k = 1$'일때 $p(\\mathbf{x})$의 확률 분포, 즉 $p(\\mathbf{x} \\mid z_k=1)$이라 할 수 있다.\n",
    "$$p(\\mathbf{x} \\mid z_k = 1) = \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률변수 $\\mathbf{z}$의 값은 '1-of-K' 형태이므로 $p(\\mathbf{z})$를 아래와 같이 나타낼 수 있다.\n",
    "$$p(\\mathbf{x} \\mid \\mathbf{z}) = \\prod_{k=1}^K\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )^{z_k} \\qquad (9.11)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 $p(\\mathbf{z})$와 $p(\\mathbf{x} \\mid \\mathbf{z})$를 이용해 'mixtures of gaussian', $p(\\mathbf{x})$를 아래와 같이 표현할 수 있다.\n",
    "$$p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{z})p(\\mathbf{x} \\mid \\mathbf{z}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) \\qquad (9.12)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- figure 9.4와 같이 $p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{z})p(\\mathbf{x} \\mid \\mathbf{z})$를 graphical model로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 돌아 돌아 같은 표현에 도달한 것 같지만, $p(\\mathbf{x})$를 $p(\\mathbf{x}, \\mathbf{z})$을 이용해 표현할 수 있게 된 것이고, 이는 나중에 유용하게 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또한가지 나중에 중요하게 사용될 것은 '$\\mathbf{x}$가 주어졌을 때 $\\mathbf{z}$의 조건부 분포 $\\gamma(z_k)$이다.\n",
    "$$\\gamma(z_k) \\equiv p(z_k = 1 \\mid \\mathbf{x}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) }{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j ) } \\qquad (9.13)$$\n",
    " - 여기서 중요한 것은 $\\pi_k$는 $z_k = 1$일 '사전' 확률이고, $p(z_k = 1 \\mid \\mathbf{x})$는 데이터를 보고난 후 '사후' 확률이라는 점이다.\n",
    " - 이 녀석 $\\gamma(z_k)$을 **responsibility**이라 부를 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature가 $D$개인 $n$개의 샘플 데이터 $\\{ \\mathbf{x}_1, \\dots, \\mathbf{x}_N \\}$가 있다고 하자.(이것을 $N \\times D$ 행렬 $\\mathbf{X}$로 표기)\n",
    "- 각 샘플은 서로 독립이라고 가정하면, 이 데이터에 대한 분포를 $N$개의 'mixtures of gaussian'으로 표현할 수 있다.@@@ (figure 9.6의 graphical model로 표현할 수 있다. figure 9.4와는 다르다.) 또한 이때 $N$개의 샘플에 대한 likelihood는 아래와 같다.\n",
    "\n",
    " - likehood for the incomplete data set $\\{\\mathbf{X} \\}$\n",
    "\\begin{align}\n",
    "p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\prod_{n=1}^N  p(\\mathbf{x}_n \\mid \\boldsymbol{\\pi}_n, \\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n ) \\\\\n",
    "&=\\prod_{n=1}^N \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\\\\n",
    "\\end{align}\n",
    "  - log likehood for the incomplete data set $\\{\\mathbf{X} \\}$, eq 9.14\n",
    "\\begin{align}\n",
    "\\ln p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\sum_{n=1}^N \\ln p(\\mathbf{x}_n) \\\\\n",
    "&=\\sum_{n=1}^N \\ln \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\qquad (9.14)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주어진 데이터 $\\mathbf{X}$에 대해 위 likelihood를 최대화하는 파라미터들($\\{\\pi_1, \\dots, \\pi_N\\}$, $\\{ \\boldsymbol{\\mu}_1, \\dots, \\boldsymbol{\\mu}_k \\}$, $\\{ \\boldsymbol{\\Sigma}_1, \\dots, \\boldsymbol{\\Sigma}_K \\}$을 찾으면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 'gaussian mixture'에 maximum likelihood를 적용하면 특이점(singularity) 문제가 발생할수도 있다. (자세한 내용은 p433, p434 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\ln \\sum$ 형태의 미분은 어렵다. Gaussian mixture 조차도 쉽지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. EM for Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 식 (9.14)를 최대화하는 파라미터들을 찾기 위해, 이 식을 $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma_k}$, $\\pi_k$에 대해 각각 순서대로 미분하고 0으로 놓으면 아래 식들을 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\boldsymbol{\\mu}_k &= \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) \\mathbf{x}_n \\qquad{(9.17)}\\\\\n",
    "\\boldsymbol{\\Sigma}_k &= \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k ) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\intercal \\qquad (9.19) \\\\\n",
    "\\pi_k &= \\frac{N_k}{N} \\qquad (9.22) \\\\\n",
    "N_k &= \\sum_{n=1}^N \\gamma(z_{nk}) \\\\\n",
    "\\gamma(z_k) &\\equiv p(z_k = 1 \\mid \\mathbf{x}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) }{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j ) } \\qquad (9.13)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 위 식들은 서로 얽혀 있어서 바로 해를 구할 수 없다.        \n",
    "예를들어 $\\pi_k$를 구하기 위해서는 $\\boldsymbol{\\mu}_k$와 $\\boldsymbol{\\Sigma}_k$를 알아야 하고       \n",
    "그 반대의 경우에도 다른 파라미터를 알아야 한다.            \n",
    ".    \n",
    "이 때문에 파라미터들을 바로 구할수는 없고          \n",
    "각 파라미터를 조금식 수정하는 순차적인 방법을 사용한다.           \n",
    "이 순차적인 방법이 이후 설명할 EM 알고리즘의 특정한 경우라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순차 과정\n",
    " 1. $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma_k}$, $\\pi_k$를 적당히 초기화\n",
    " - (E 단계) $\\gamma(z_k)$를 계산, figure 9.8(b)와 같이 각 점($\\mathbf{x_n}$)에 해당하는 $\\gamma(z_nk)$를 계산하고, $\\gamma(z_1)$ 값만큼 파란 잉크를 / $\\gamma(z_2)$ 값만큼 빨간 잉크를 써서 점의 색을 결정\n",
    " - (M 단계) 앞서 구한 $\\gamma(z_nk)$값들과 식 (9.17), (9.19), (9.22)을 이용해 $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma_k}$, $\\pi_k$를 업데이트\n",
    " - 수렴할때까지 '2단계'로 돌아가 반복 (likelihood(식 9.14) 값의 변화가 임계값 이하이면 수렴)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3. An Alternative View of EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 관측 데이터 $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$에 대한 잠재변수 $\\mathbf{Z} \\in \\mathbb{R}^{N \\times K}$를 도입하면 $\\mathbf{X}$에 대한 log likelihood를 아래와 같이 표현할 수 있다.\n",
    "\n",
    "$$\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\ln \\bigg\\{ \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\bigg\\} \\qquad (9.29)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 $\\ln$안에 $\\sum$이 있는 형태는 미분이 어렵기 때문에 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$를 최대화하는 해를 찾기가 어렵다.     \n",
    "대안으로 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$를 최대화하는 대신 $\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big]$를 최대화 하는 접근법을 사용한다. ($\\mathbf{Z} \\sim p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})$)        \n",
    "$\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big]$를 최대화하는게 결국 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$를 최대화하는 것과 같은데, 왜 그런지는 9.4에 설명된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이후에는 $\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big]$를 아래와 같이 $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})$라고 표기하기로 한다.\n",
    "$$\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) = \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'E 단계'는 $\\boldsymbol{\\theta}^{\\text{old}}$가 주어졌을 때 함수 $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})$를 구하는 과정이고     \n",
    "'M 단계'는 $\\boldsymbol{\\theta}^{\\text{old}}$를 최대화하는 $\\boldsymbol{\\theta}^{\\text{new}}$를 구하는 과정이다. \n",
    "\\begin{align}\n",
    "\\boldsymbol{\\theta}^{\\text{new}} &= \\arg \\max_{\\boldsymbol{\\theta}} \\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) \\\\\n",
    "\\boldsymbol{\\theta}^{\\text{old}} &\\leftarrow \\boldsymbol{\\theta}^{\\text{new}} \\\\\n",
    "\\end{align}\n",
    "즉 EM 단계를 반복하면서 $\\boldsymbol{\\theta}$를 수정해간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EL 알고리즘을 MAP(maximum posterior) 해를 구하는데 사용할수도 있는데,           \n",
    "파라미터 $\\boldsymbol{\\theta}$의 사전분포를 $p(\\boldsymbol{\\theta})$라 할 때,          \n",
    "아래 식을 최대화하는 $\\boldsymbol{\\theta}$를 찾으면 된다.     \n",
    "(E 단계에서는 앞서와 같이 $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})$를 구하면 되고, M 단계에서는 아래 식을 최대화 한다.)\n",
    "$$\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) + \\ln p(\\boldsymbol{\\theta})$$\n",
    "사전분포 $p(\\boldsymbol{\\theta})$를 잘 선택하면 MLE에서의 특이점(singularity) 문제가 없다.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. Gaussian mixtures revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 관측 데이터 $\\mathbf{X}$가 주어진 경우 이 데이터를 'incomplete data set' $\\{ \\mathbf{X}\\}$이라 하고         \n",
    "관측 데이터 각각($\\mathbf{x}_n$)에 대응하는 잠재변수($\\mathbf{z}_n$들) $\\mathbf{Z}$까지 주어진 경우 이 데이터를 'complete data set' $\\{ \\mathbf{X}, \\mathbf{Z} \\}$라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(\\mathbf{z}) &= \\prod_{k=1}^K\\pi_k^{z_k} \\qquad (9.10) \\\\\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}) &= \\prod_{k=1}^K\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )^{z_k} \\qquad (9.11)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 등장했던 위 결과를 이용하면 complete data set $\\{ \\mathbf{X}, \\mathbf{Z} \\}$에 대한 log likelihood를 아래와 같이 표현할 수 있다.\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) &= \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}}\\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k )^{z_{nk}} \\qquad (9.35) \\\\\n",
    "\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) &= \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\big\\{ \\ln \\pi_k +  \\ln \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k ) \\big\\} \\qquad (9.36) \\\\\n",
    "\\end{align}\n",
    "위와 같이 $\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi})$는 $\\ln$과 gaussian이 바로 붙어 있어서 미분이 용이하다.    \n",
    "반면 $\\ln p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} )$ (아래 식 9.14)는 $\\ln \\sum \\mathcal{N}$ 형태이므로 미분이 용이하지 않다.\n",
    "\\begin{align}\n",
    "\\ln p (\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ) \n",
    "&=\\sum_{n=1}^N \\ln p(\\mathbf{x}_n) \\\\\n",
    "&=\\sum_{n=1}^N \\ln \n",
    "\\bigg\\{ \n",
    "\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{X}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\\bigg\\} \\qquad (9.14)\\\\\n",
    "\\end{align}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위와 같은 이유로 $\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi})$에 대한 MLE에 대한 닫힌 해를 구하는게 가능하다.           \n",
    "그런데 실제로는 $\\mathbf{Z}$를 관측하지 못하므로 이 방법은 현실적이지 않다.        \n",
    "대신 $\\mathbf{Z}$에 대한 분포 $p(\\mathbf{Z})$를 생각하고        \n",
    "$\\mathbf{X}$가 주어졌을 때 $\\mathbf{Z}$에 대한 사후 분포 $p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})$를 구하고\n",
    "$\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big]$를 최대화 하는 $\\boldsymbol{\\theta}$를 찾는다.      \n",
    "($\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big]$를 최대화하는게 결국 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$를 최대화하는 것과 같은데, 왜 그런지는 9.4에 설명된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gaussian mixture에 대한 $\\mathbf{X}$가 주어졌을 때 $\\mathbf{Z}$에 대한 사후 분포 $p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})$는 식 (9.38)과 같은 비례식이 성립하는데,          \n",
    "$n$번째 데이터 샘플에 대한 $k$번째 gaussian 성분 $z_{nk}$의 기댓값은 responsibility $\\gamma(z_{nk})$와 같다.\n",
    "$$\\mathbb{E}[z_{nk}] = \\gamma(z_{nk}) \\qquad (9.39)$$\n",
    "이 결과를 이용하면 gaussian mixture에 대한 $\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big]$를 아래와 같이 정리할 수 있다.           \n",
    "(여기서 $\\boldsymbol{\\theta}$는 $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$이고, 이 기댓값을 $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})$라 표기한다. 또한 $\\boldsymbol{\\theta}^{\\text{old}}$는 $\\gamma(z_{nk})$를 구할 때 사용됐던 파라미터 $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$를 의미한다.)\n",
    "$$\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) \\big] = \n",
    "\\sum_{n=1}^N \\sum_{k=1}^K \\gamma(z_{nk}) \\big\\{ \\ln \\pi_k + \\ln \\mathcal{N} (\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\big\\} \\qquad (9.40)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아무튼 먼 길을 돌아 결론은            \n",
    "gaussian mixture에 대한 maximum likelihood 해를 구하기 위해                        \n",
    "$\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi})$를 최대화 하는 대신 $\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) \\big]$를 최대화하는 $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$를 아래 EM 과정으로 찾는다는 것이다.          \n",
    "\n",
    " 1. $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$를 적당히 초기화\n",
    " - (E 단계) $\\gamma(z_{nk})$ 값을 구하고\n",
    " - (M 단계) 함수 $\\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) \\big]$를 최대화하는 $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$로 파라미터를 업데이트   \n",
    " - 수렴할때까지 2부터 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. Relation to K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3. Mixtures of Bernoulli distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.4. EM for Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4. The EM Algorithm in General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 EM 알고리즘의 일반적인 설명이 등장한다. 물론 앞서 설명했던 내용의 반복된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EM 알고리즘은 잠재변수를 포함하는 확률 모형의 최대 우도 해를 구하는 일반적인 방법.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\ln \\bigg\\{ \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\bigg\\} \\qquad (9.29)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 관측변수를 $\\mathbf{X}$, 잠재변수를 $\\mathbf{Z}$라 하고, $\\mathbf{X}$와 $\\mathbf{Z}$의 결합 확률분포가 파라미터 $\\boldsymbol{\\theta}$로 결정될때        \n",
    "likelihood(9.69)식  혹은 log-likelihood(식 9.29)를 최대화하는 파라미터 $\\boldsymbol{\\theta}$를 찾는 것이 목표이고, EM 알고리즘은 이 목표를 달성하는 많은 방법 중 하나이다.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) &= \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\qquad (9.29) \\\\\n",
    "\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) &= \\ln \\bigg\\{ \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\bigg\\} \\qquad (9.69)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$를 아래와 같이 두개로 분해할 수 있다.\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) &= \\mathcal{L}(q, \\boldsymbol{\\theta}) + \\text{KL}(q \\Vert p) \\qquad (9.70) \\\\\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}) &= \\sum_\\mathbf{Z} q(\\mathbf{Z}) \\ln \\bigg\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})}{q(\\mathbf{Z})} \\bigg\\} \\qquad (9.71) \\\\\n",
    "\\text{KL}(q \\Vert p) &= - \\sum_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\bigg\\{ \\frac{p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}))}{q(\\mathbf{Z})} \\bigg\\} \\qquad (9.72)\n",
    "\\end{align}\n",
    "$\\mathcal{L}(q, \\boldsymbol{\\theta})$는 분포 $q(\\mathbf{Z})$에 대한 functional이다.? (Appendix D)     \n",
    ".           \n",
    "위 식에서 $\\text{KL}$은 항상 0보다 크거나 같으므로 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})  \\geq \\mathcal{L}(q, \\boldsymbol{\\theta})$이다.      \n",
    "즉 $\\mathcal{L}(q, \\boldsymbol{\\theta})$은 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta})$의 lower bound이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EM 알고리즘의...      \n",
    "'E 단계'(figure 9.12)에서는 $p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})$의 파라미터 $\\boldsymbol{\\theta}$를 $\\boldsymbol{\\theta}^{\\text{old}}$로 고정한체로 $\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})$를 최대화하는 함수 $q(\\mathbf{Z})$를 찾아야 한다.      \n",
    "그런데 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$는 $q(\\mathbf{Z})$에 의존적이지 않으므로 $q(\\mathbf{Z})$가 바뀌더라도 $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\theta}^{\\text{old}})$의 크기가 바뀌지 않는다.(figure 9.11)           \n",
    "반면 $\\text{KL}(q \\Vert p)$는 $q(\\mathbf{Z})$에 의존적이므로 $q(\\mathbf{Z})$를 바꿔서 $\\text{KL}(q \\Vert p)$를 최소화 한다면 $\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})$가 최대화 될 것이다.       \n",
    "$\\text{KL}(q \\Vert p)$는 $q(\\mathbf{Z})$와 $p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}))$가 같을때 최소화 된다.    \n",
    "이 결과를 이용해서 $\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}})$를 아래와 같이 정리할 수 있다.\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q, \\boldsymbol{\\theta}^{\\text{old}}) &= \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) - \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}} \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}}) \\\\\n",
    "&= \\mathbb{E}_{\\mathbf{Z}}\\big[ \\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\big] + \\text{const}\\\\\n",
    "&= \\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) + \\text{const} \\qquad (9.74)\n",
    "\\end{align}\n",
    "($- \\sum_{\\mathbf{Z}} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}} \\ln p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{\\text{old}})$는 분포 $q$에 대한 엔트로피이므로 상수이다.)\n",
    ".       \n",
    "다음으로 'M 단계'(figure 9.13)에서는 $\\mathcal{Q}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})$를 최대화하는 $\\boldsymbol{\\theta}$를 찾는데, 이 함수는 $\\ln \\mathcal{N}$ 형태이므로 쉽게 최적화 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
