{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = np.where(df[4] == 'Iris-setosa', 1, 0)\n",
    "df_train = df.sample(frac=0.7, random_state=0)\n",
    "df_test = df.sample(frac=0.3, random_state=0)\n",
    "\n",
    "X_train = np.array(df_train[[0, 1]]).T\n",
    "Y_train = np.array(df_train[[4]]).T\n",
    "X_test = np.array(df_test[[0, 1]]).T\n",
    "Y_test = np.array(df_test[[4]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def relu(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return np.maximum(0, Z)\n",
    "    else:  # derivative of ReLU, {1 if x>0, 0 if x <= 0}\n",
    "        return ((np.sign(Z) + 1) // 2).astype(int)\n",
    "\n",
    "\n",
    "def sigmoid(Z, is_forward=True):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \"\"\"\n",
    "    if is_forward:\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    else:  # derivative of sigmoid, a(1-a)\n",
    "        return np.multiply(sigmoid(Z), (1 - sigmoid(Z)))\n",
    "\n",
    "\n",
    "def forward_propagation(A_l_prev, W_l, b_l, g_l):\n",
    "    Z_l = b_l + np.dot(W_l, A_l_prev)\n",
    "    A_l = g_l(Z_l)\n",
    "    return A_l, Z_l\n",
    "\n",
    "\n",
    "def get_random_b_W(n_l_prev, n_l):\n",
    "    W_l = np.random.randn(n_l, n_l_prev) * 0.01\n",
    "    b_l = np.zeros((n_l, 1))\n",
    "    return b_l, W_l\n",
    "\n",
    "\n",
    "def backward_propagation(dA_l, A_l_prev, W_l, Z_l, g_l):\n",
    "    m = dA_l.shape[1]\n",
    "    assert (m == Z_l.shape[1])\n",
    "\n",
    "    dZ_l = np.multiply(dA_l, g_l(Z_l, is_forward=False))\n",
    "    dW_l = np.dot(dZ_l, A_l_prev.T) / m\n",
    "    db_l = np.sum(dZ_l, axis=1, keepdims=True) / m\n",
    "    dA_l_prev = np.dot(W_l.T, dZ_l)\n",
    "\n",
    "    return dA_l_prev, dW_l, db_l\n",
    "\n",
    "\n",
    "class SimpleFfnn:\n",
    "    def __init__(self, layer_dims, layer_activations, random_seed=None):\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.n_L = len(layer_dims) - 1  # input layer is not counted.\n",
    "        self.cache_A = [None] * (self.n_L + 1)\n",
    "        self.cache_Z = [None] * (self.n_L + 1)  # cache_Z[0] is not used.\n",
    "        self.cache_W = [None] * (self.n_L + 1)  # cache_W[0] is not used.\n",
    "        self.cache_b = [None] * (self.n_L + 1)  # cache_b[0] is not used.\n",
    "        self.cache_dA = [None] * (self.n_L + 1)\n",
    "        self.cache_dZ = [None] * (self.n_L + 1)  # cache_dZ[0] is not used.\n",
    "        self.cache_dW = [None] * (self.n_L + 1)  # cache_dW[0] is not used.\n",
    "        self.cache_db = [None] * (self.n_L + 1)  # cache_db[0] is not used.\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        for i in range(1, len(self.layer_dims)):  # 1, 2, ..., n_L\n",
    "            n_l = self.layer_dims[i]\n",
    "            n_l_prev = self.layer_dims[i - 1]\n",
    "\n",
    "            self.cache_b[i], self.cache_W[i] = get_random_b_W(n_l_prev, n_l)\n",
    "\n",
    "    def forward_propagation_deep(self, X_train, Y_train):\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        assert (self.layer_dims[0] == X_train.shape[0])\n",
    "        assert (X_train.shape[1] == Y_train.shape[1])\n",
    "        self.cache_A[0] = self.X\n",
    "\n",
    "        for i in range(1, len(self.layer_dims)):  # 1, 2, ..., n_L\n",
    "            A_l_prev = self.cache_A[i - 1]\n",
    "            W_l = self.cache_W[i]\n",
    "            b_l = self.cache_b[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "\n",
    "            self.cache_A[i], self.cache_Z[i] = forward_propagation(A_l_prev, W_l, b_l, g_l)\n",
    "\n",
    "    def backward_propagation_deep(self):\n",
    "        Y_actual = self.Y\n",
    "        Y_prediction = self.cache_A[self.n_L]\n",
    "\n",
    "        self.cache_dA[self.n_L] = - (np.divide(Y_actual, Y_prediction) - np.divide(1 - Y_actual, 1 - Y_prediction))\n",
    "\n",
    "        for i in list(reversed(range(1, len(self.layer_dims)))):  # n_L, n_L-1, ..., 2, 1\n",
    "            dA_l = self.cache_dA[i]\n",
    "            A_l_prev = self.cache_A[i - 1]\n",
    "            W_l = self.cache_W[i]\n",
    "            Z_l = self.cache_Z[i]\n",
    "            g_l = self.layer_activations[i]\n",
    "\n",
    "            self.cache_dA[i - 1], self.cache_W[i], self.cache_b[i] = backward_propagation(dA_l, A_l_prev, W_l, Z_l, g_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train.shape[0]\n",
    "\n",
    "layer_dims = [n_x, 4, 4, 1]\n",
    "layer_activations = [None, relu, relu, sigmoid]\n",
    "\n",
    "nn = SimpleFfnn(layer_dims, layer_activations, random_seed=1)\n",
    "nn.forward_propagation_deep(X_train, Y_train)\n",
    "nn.backward_propagation_deep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> activation matrix of l's layer:\n",
      " [[ 0.          0.02917199]\n",
      " [ 0.          0.        ]\n",
      " [ 0.00994349  0.01168037]\n",
      " [ 0.01165128  0.        ]\n",
      " [ 0.          0.        ]] \n",
      "> shape(n_l, m): (5, 2)\n"
     ]
    }
   ],
   "source": [
    "#  TEST CODE\n",
    "np.random.seed(1)  # to make test's result consistent.\n",
    "\n",
    "n_l_prev = 4  # number of nodes in l-1's layer\n",
    "n_l = 5  # number nodes in l's layer\n",
    "m = 2  # sample size for this batch\n",
    "\n",
    "# setting test values\n",
    "W_l = np.random.randn(n_l, n_l_prev) * 0.01\n",
    "b_l = np.zeros((n_l, 1))\n",
    "A_l_prev = np.random.randn(n_l_prev, m)\n",
    "\n",
    "A_l, Z_l = forward_propagation(A_l_prev, W_l, b_l, relu)\n",
    "\n",
    "cache_l = (A_l_prev, W_l, Z_l)  \n",
    "\n",
    "print(\"> activation matrix of l's layer:\\n\", A_l, '\\n> shape(n_l, m):', A_l.shape)\n",
    "\n",
    "#  TEST CODE(continued from test code of forward propagation)\n",
    "dA_l = np.random.randn(n_l, m)  # test acvivation values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "x1 = np.random.randn(3, 4)\n",
    "\n",
    "np.random.seed(1)\n",
    "x2 = np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]\n",
      " [ 0.86540763 -2.3015387   1.74481176 -0.7612069 ]\n",
      " [ 0.3190391  -0.24937038  1.46210794 -2.06014071]] [[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]\n",
      " [ 0.86540763 -2.3015387   1.74481176 -0.7612069 ]\n",
      " [ 0.3190391  -0.24937038  1.46210794 -2.06014071]]\n"
     ]
    }
   ],
   "source": [
    "print(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "A_l_prev = np.random.randn(3, 2)\n",
    "W_l = np.random.randn(1, 3)\n",
    "b_l = np.random.randn(1, 1)\n",
    "g_l = sigmoid\n",
    "A_l, Z_l = forward_propagation(A_l_prev, W_l, b_l, g_l)\n",
    "np.array_equal(A_l, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968900232783661"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_l[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96890023,  0.11013289]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.array([[0.96890023, 0.11013289]]); x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.round(A_l, 3), np.round(x2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False]], dtype=bool)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
