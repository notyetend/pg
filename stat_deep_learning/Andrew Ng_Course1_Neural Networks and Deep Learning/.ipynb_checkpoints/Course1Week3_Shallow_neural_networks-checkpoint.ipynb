{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coursera  | Andrew Ng's Deep Learning Class | Course1. Neural Networks and Deep Learning | Week3. Shallow_neural_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1]. Shallow neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-1]. Neural Networks Overviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(지난주 내용을 간단히 짚고 넘어간다.) 로지스틱 회귀모형에서는 파라미터 $w$, $b$와 입력 데이터 $x$가 주어졌을 때  $z=W^Tx + b$, $a=\\sigma(z)$, $\\mathcal{L}(a, y)$를 차례로 계산하게 된다.\n",
    "\n",
    "이에 상응하는 신경망 모형은 아래 두번째 그림으로 히든 레이어의 노드가 3개 이므로 아래와 같은 3가지 버전의 $z$를 계산하게 되며($z^{[1]}_1$\n",
    ", $z^{[1]}_2$, $z^{[1]}_3$), 이때 각기 다른 파라미터($w^{[1]}_1$, $w^{[1]}_2$, $w^{[1]}_3$)가 사용된다.\n",
    "\n",
    "몇가지 표기법을 정리해보면 우선 각 i번째 레이어를 표현하기 위해 윗첨자로 [l]를 붙일 것이며, 데이터 샘플의 번호는 윗첨자 (i)를, 특정 레이어의 노드 번호는 아랫첨자 j를 사용할 것이다. 또한 행렬 $M$이 p x q 차원이라는 것을 $M \\in \\mathbb{R}^{p \\times q}$와 같이, $s$가 스칼라 값이라는 것을 $s \\in \\mathbb{R}$로 표기할 것이다. 앞으로 사용할 변수들의 차원과 의미는 아래와 같다.\n",
    "\n",
    "- $x^{(i)}_j \\in \\mathbb{R}^{}$ : i번째 샘플의 j번째 변수\n",
    "- $x_j \\in \\mathbb{R}^{1 \\times m}$ : j번째 변수들(모든 샘플에 대한)\n",
    "\n",
    "\n",
    "- $W^{[l]}_j \\in \\mathbb{R}^{1 \\times n^{[l-1]}}$ : $l-1$번째 레이어와 $l$번째 레이어의 $j$번째 노드를 연결하는 weights\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ : $l-1$번째 레이어와 $l$번째 레이어를 연결하는 weights\n",
    "\n",
    "\n",
    "- $z^{[l](i)}_j \\in \\mathbb{R}^{}$ : $i$번째 샘플에 대한 $l$번째 레이어의 $j$번째 노드의 weighted sum\n",
    "- $z^{[l]}_j \\in \\mathbb{R}^{1 \\times m}$ : $l$번째 레이어의 $j$번째 노드의 weighted sum (모든 샘플에 대한)\n",
    "- $Z^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ : $l$번째 레이어의 weighted sum (모든 샘플에 대한)\n",
    "\n",
    "\n",
    "- $a^{[l](i)}_j \\in \\mathbb{R}^{}$ : $i$번째 샘플에 대한 $l$번째 레이어의 $j$번째 노드의 activation\n",
    "- $a^{[l]}_j \\in \\mathbb{R}^{1 \\times m}$ : $l$번째 레이어의 $j$번째 노드의 activation (모든 샘플에 대한)\n",
    "- $A^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ : $l$번째 레이어의 activation (모든 샘플에 대한)\n",
    "\n",
    "\n",
    "- $dw$ : $w$에 대한 미분항으로서 $w$의 차원과 같다. (예를들어 $dW^{[l]}$의 차원은 $W^{[l]}$의 차원과 같다.)\n",
    "- $dz$ : $z$에 대한 미분항으로서 $z$의 차원과 같다.\n",
    "- $da$ : $a$에 대한 미분항으로서 $a$의 차원과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L01_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-2]. Neural Network Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림에서 첫번째 레이어를 입력 레이어(input layer), 두번째 레이어를 히든 레이어(hidden layer), 세번째 레이어를 출력 레이어(output layer)라 한다. 일반적으로 히든 레이어는 입력 레이어와 출력 레이어 사이의 레이어들을 말한다. 그리고 아래 그림의 신경망을 '2 레이어 신경망'이라 하는데, 레이어 수를 셀때는 입력 레이어를 제외하고 세는 관래가 있다.\n",
    "\n",
    "$w^{[1]}$와 $b^{[1]}$은 입력 레이어에서 (첫번째) 히든 레이어를 연결하는 파라미터를 의미하며 $w^{[1]}$는 4x3 행렬이고 $b^{[1]}$는 4x1 행렬이다.\n",
    "\n",
    "또한 $w^{[2]}$와 $b^{[2]}$은 히든 레이어에서 출력 레이어를 연결하는 파라미터를 의미하며 $w^{[2]}$는 1x4 행렬이고 $b^{[2]}$는 1x1 행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-3]. Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실 신경망에서 노드(혹은 뉴런)을 표현하는 동그라미는 두가지 연산을 수행한다. $l$번째 레이어의 $j$번째 노드는 우선 이전 레이어의 출력값들($a^{[l-1]}$ 혹은 첫번째 히든 레이어일 경우 $x$)을 입력받아 가중합(weighted sum)으로 $z_j$를 계산하는 것이고, 두번째는 $z_j$를 입력으로 activation function 값($\\sigma(z_j)$)을 구하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "측 첫번째 히든 레이어의 첫 노드는 아래 계산을 하는 것이고\n",
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1 \\\\\n",
    "& a^{[1]}_1 = \\sigma(z^{[1]}_1)\n",
    "\\end{align}\n",
    "$$\n",
    "첫번째 히든 레이어의 두번째 노드는 아래 계산을 하는 것이다.\n",
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2 \\\\\n",
    "& a^{[1]}_2 = \\sigma(z^{[1]}_2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 히든 레이어 각 노드의 가중합와 activation은 아래와 같은데 이들 계산을 for-loop로 구할수도 있으나, 벡터화 하는 것이 보다 효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1 \\\\\n",
    "& z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2 \\\\\n",
    "& z^{[1]}_3 = w^{[1]T}_3 x + b^{[1]}_3 \\\\\n",
    "& z^{[1]}_4 = w^{[1]T}_4 x + b^{[1]}_4 \\\\\n",
    "&\\\\\n",
    "& a^{[1]}_1 = \\sigma(z^{[1]}_1) \\\\\n",
    "& a^{[1]}_2 = \\sigma(z^{[1]}_2) \\\\\n",
    "& a^{[1]}_3 = \\sigma(z^{[1]}_3) \\\\\n",
    "& a^{[1]}_4 = \\sigma(z^{[1]}_4) \\\\\n",
    "&\\\\\n",
    "&\\left( z^{[1]}_j \\in \\mathbb{R}^{}, ~ w^{[1]}_j \\in \\mathbb{R}^{3 \\times 1}, ~ x \\in \\mathbb{R}^{3 \\times 1}, ~ b^{[1]}_j \\in \\mathbb{R}^{}, ~ a^{[1]}_j \\in \\mathbb{R}^{1 \\times 1} \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 계산을 벡터화 하여 아래와 같이 표현할 수 있다.\n",
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]} = w^{[1]}x + b^{[1]}\\\\\n",
    "& \\\\\n",
    "& a^{[1]} = \\sigma(z^{[1]}) \\\\\n",
    "& \\\\\n",
    "& \\left( z^{[1]} \\in \\mathbb{R}^{4 \\times 1}, ~ w^{[1]} \\in \\mathbb{R}^{4 \\times 3}, ~ x \\in \\mathbb{R}^{3 \\times 1}, ~ b^{[1]} \\in \\mathbb{R}^{4 \\times 1}, ~ a^{[1]} \\in \\mathbb{R}^{4 \\times 1}\\right) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 각 변수의 차원을 적어볼 수 있는데 주의할 점은 데이터 샘플 하나에 대한 것이라는 점이다. (전체 데이터 $m$건에 대한 계산을 벡터화 하는 것은 다음 비디오에서 다루게 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_05.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-4]. Vectorizing across multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-5]. Explanation for Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-6]. Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-7]. Why do you need non-linear activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-8]. Derivatives of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-9]. Gradient descent for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-10]. Backpropagation intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "&(1) \\qquad z^{[1]}=W^{[1]}x + b^{[1]}       & \\text{dz1} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} = \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\left( \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) =  \\frac{\\partial g^{[1]}}{\\partial z^{[1]}} * w^{[2]T} \\cdot \\text{dz2} \\qquad (9) \\\\\n",
    "&(2) \\qquad a^{[1]} = g^{[1]}(z^{[1]})        & \\text{da1} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} = \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\left( \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) = w^{[2]T} \\cdot \\text{dz2} \\qquad (8) \\\\\n",
    "&(3) \\qquad z^{[2]}=W^{[2]}a^{[1]} + b^{[2]} & \\text{dz2} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} = \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} = a^{[2]}(1-a^{[2]}) \\cdot \\left(-\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}\\right) = a^{[2]}-y \\qquad (7) \\\\\n",
    "&(4) \\qquad a^{[2]} = \\sigma(z^{[2]})        & \\text{da2} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} = \\left(-\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}\\right) \\qquad (6) \\\\\n",
    "&(5) \\qquad \\mathcal{L}(a^{[2]}, y)          & \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-11]. Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법(gradient descent)과 같이 점근/반복적 방법으로 파라미터들(weights)를 구하는 방법을 사용할 때 파라미터에 대한 초기값 설정이 필요하다. 로지스틱 회귀모형의 경우 모든 파라미터 값을 0으로 초기화 해도 문제가 없지만, 신경망의 경우 파라미터 값을 0으로 초기화 할 경우 경사하강법이 잘 동작하지 않는다. \n",
    "\n",
    "아래 그림과 같이 2개의 입력 변수와 2개의 레이어(hidden & output)로 구성된 네트워크에서 파라미터 $W$와 바이어스 $b$를 모두 0으로 설정하면 $a^{[1]}_1$과 $a^{[1]}_2$가 $0 + x_1 \\cdot 0 + x_2 \\cdot 0$로 같아진다. 또한 $dz^{[1]}_1$과 $dz^{[1]}_2$ 값도 같아진다.  역전파(backpropagation)가 진행되면 파라미터 $W$의 값이 0에서는 벗어나게 되지만 $w^{[1]}_1$과 $w^{[1]}_2$가 같은 값으로 변하게 되므로($W$행렬의 각 행이 같은 값을 갖는다.), 다시 $a^{[1]}_1$과 $a^{[1]}_2$가 같아진다. 결국 각 레이어에 뉴닛은 많을 수 있지만 모두 같은 함수계산이 된다. 이런 대칭(symmetry)문제가 있게 되면 최적화가 잘 되지 않는다. (사실 바이어스의 초기값을 0으로 하는 것은 문제를 일으키지 않는다.)\n",
    "\n",
    "이런 문제를 해결하기 위해 파라미터 $W$를 어떤 임의의 값으로 설정하게 된다. 임의초기화(random initialization)을 위한 파이썬 코드는 아래와 같다.\n",
    "\n",
    "```Python\n",
    "W1 = np.random.randn((2,2)) * 0.01     \n",
    "b1 = np.zero((2,1))     \n",
    "     \n",
    "```\n",
    "\n",
    "위 초기화 코드에서 왜 랜덤값에 0.01을 곱하는 것은 $w$ 초기값이 지나치케 커지면 $z$값도 커지게되고 (sigmoid나 tanh 의 경우 큰 입력값에 대한 gradient가 작기 때문에) 학습 속도가 느려지게 된다. 신경망의 깊이등 상황에 따라 0.01이 아니라 다른 상수값을 사용할 수도 있는데, 적절한 상수를 선택하는 방법에 대해서 추후 다루게 될 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
