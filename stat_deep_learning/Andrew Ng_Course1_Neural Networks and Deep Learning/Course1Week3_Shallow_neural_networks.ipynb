{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coursera  | Andrew Ng's Deep Learning Class | Course1. Neural Networks and Deep Learning | Week3. Shallow_neural_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1]. Shallow neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-1]. Neural Networks Overviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(지난주 내용을 간단히 짚고 넘어간다.) 로지스틱 회귀모형에서는 파라미터 $w$, $b$와 입력 데이터 $x$가 주어졌을 때  $z=W^Tx + b$, $a=\\sigma(z)$, $\\mathcal{L}(a, y)$를 차례로 계산하게 된다.\n",
    "\n",
    "이에 상응하는 신경망 모형은 아래 두번째 그림으로 히든 레이어의 노드가 3개 이므로 아래와 같은 3가지 버전의 $z$를 계산하게 되며($z^{[1]}_1$\n",
    ", $z^{[1]}_2$, $z^{[1]}_3$), 이때 각기 다른 파라미터($w^{[1]}_1$, $w^{[1]}_2$, $w^{[1]}_3$)가 사용된다.\n",
    "\n",
    "앞으로 사용할 몇가지 표기법을 정리해보면 우선 각 i번째 레이어를 표현하기 위해 윗첨자로 [l]를 붙일 것이며, 데이터 샘플의 번호는 윗첨자 (i)를, 특정 레이어의 노드 번호는 아랫첨자 j를 사용할 것이다. 또한 행렬 $M$이 p x q 차원이라는 것을 $M \\in \\mathbb{R}^{p \\times q}$와 같이, $s$가 스칼라 값이라는 것을 $s \\in \\mathbb{R}$로 표기할 것이다. 앞으로 사용할 변수들의 차원과 의미는 아래와 같다.\n",
    "\n",
    "- $x^{(i)}_j \\in \\mathbb{R}^{}$ : i번째 샘플의 j번째 변수\n",
    "- $x_j \\in \\mathbb{R}^{1 \\times m}$ : j번째 변수들(모든 샘플에 대한)\n",
    "\n",
    "\n",
    "- $W^{[l]}_j \\in \\mathbb{R}^{1 \\times n^{[l-1]}}$ : $l-1$번째 레이어와 $l$번째 레이어의 $j$번째 노드를 연결하는 weights\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ : $l-1$번째 레이어와 $l$번째 레이어를 연결하는 weights\n",
    "\n",
    "\n",
    "- $z^{[l](i)}_j \\in \\mathbb{R}^{}$ : $i$번째 샘플에 대한 $l$번째 레이어의 $j$번째 노드의 weighted sum\n",
    "- $z^{[l]}_j \\in \\mathbb{R}^{1 \\times m}$ : $l$번째 레이어의 $j$번째 노드의 weighted sum (모든 샘플에 대한)\n",
    "- $Z^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ : $l$번째 레이어의 weighted sum (모든 샘플에 대한)\n",
    "\n",
    "\n",
    "- $a^{[l](i)}_j \\in \\mathbb{R}^{}$ : $i$번째 샘플에 대한 $l$번째 레이어의 $j$번째 노드의 activation\n",
    "- $a^{[l]}_j \\in \\mathbb{R}^{1 \\times m}$ : $l$번째 레이어의 $j$번째 노드의 activation (모든 샘플에 대한)\n",
    "- $A^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ : $l$번째 레이어의 activation (모든 샘플에 대한)\n",
    "\n",
    "\n",
    "- $dw$ : $w$에 대한 미분항으로서 $w$의 차원과 같다. (예를들어 $dW^{[l]}$의 차원은 $W^{[l]}$의 차원과 같다.)\n",
    "- $dz$ : $z$에 대한 미분항으로서 $z$의 차원과 같다.\n",
    "- $da$ : $a$에 대한 미분항으로서 $a$의 차원과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L01_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-2]. Neural Network Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림에서 첫번째 레이어를 입력 레이어(input layer), 두번째 레이어를 히든 레이어(hidden layer), 세번째 레이어를 출력 레이어(output layer)라 한다. 일반적으로 히든 레이어는 입력 레이어와 출력 레이어 사이의 레이어들을 말한다. 그리고 아래 그림의 신경망을 '2 레이어 신경망'이라 하는데, 레이어 수를 셀때는 입력 레이어를 제외하고 세는 관래가 있다.\n",
    "\n",
    "$w^{[1]}$와 $b^{[1]}$은 입력 레이어에서 (첫번째) 히든 레이어를 연결하는 파라미터를 의미하며 $w^{[1]}$는 4x3 행렬이고 $b^{[1]}$는 4x1 행렬이다.\n",
    "\n",
    "또한 $w^{[2]}$와 $b^{[2]}$은 히든 레이어에서 출력 레이어를 연결하는 파라미터를 의미하며 $w^{[2]}$는 1x4 행렬이고 $b^{[2]}$는 1x1 행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-3]. Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실 신경망에서 노드(혹은 뉴런)을 표현하는 동그라미는 두가지 연산을 수행한다. $l$번째 레이어의 $j$번째 노드는 우선 이전 레이어의 출력값들($a^{[l-1]}$ 혹은 첫번째 히든 레이어일 경우 $x$)을 입력받아 가중합(weighted sum)으로 $z_j$를 계산하는 것이고, 두번째는 $z_j$를 입력으로 activation function 값($\\sigma(z_j)$)을 구하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "측 첫번째 히든 레이어의 첫 노드는 아래 계산을 하는 것이고\n",
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1 \\\\\n",
    "& a^{[1]}_1 = \\sigma(z^{[1]}_1)\n",
    "\\end{align}\n",
    "$$\n",
    "첫번째 히든 레이어의 두번째 노드는 아래 계산을 하는 것이다.\n",
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2 \\\\\n",
    "& a^{[1]}_2 = \\sigma(z^{[1]}_2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 히든 레이어 각 노드의 가중합와 activation은 아래와 같은데 이들 계산을 for-loop로 구할수도 있으나, 벡터화 하는 것이 보다 효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1 \\\\\n",
    "& z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2 \\\\\n",
    "& z^{[1]}_3 = w^{[1]T}_3 x + b^{[1]}_3 \\\\\n",
    "& z^{[1]}_4 = w^{[1]T}_4 x + b^{[1]}_4 \\\\\n",
    "&\\\\\n",
    "& a^{[1]}_1 = \\sigma(z^{[1]}_1) \\\\\n",
    "& a^{[1]}_2 = \\sigma(z^{[1]}_2) \\\\\n",
    "& a^{[1]}_3 = \\sigma(z^{[1]}_3) \\\\\n",
    "& a^{[1]}_4 = \\sigma(z^{[1]}_4) \\\\\n",
    "&\\\\\n",
    "&\\left( z^{[1]}_j \\in \\mathbb{R}^{}, ~ w^{[1]}_j \\in \\mathbb{R}^{3 \\times 1}, ~ x \\in \\mathbb{R}^{3 \\times 1}, ~ b^{[1]}_j \\in \\mathbb{R}^{}, ~ a^{[1]}_j \\in \\mathbb{R}^{1 \\times 1} \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 계산을 벡터화 하여 아래와 같이 표현할 수 있다.\n",
    "$$\n",
    "\\begin{align}\n",
    "& z^{[1]} = w^{[1]}x + b^{[1]}\\\\\n",
    "& \\\\\n",
    "& a^{[1]} = \\sigma(z^{[1]}) \\\\\n",
    "& \\\\\n",
    "& \\left( z^{[1]} \\in \\mathbb{R}^{4 \\times 1}, ~ w^{[1]} \\in \\mathbb{R}^{4 \\times 3}, ~ x \\in \\mathbb{R}^{3 \\times 1}, ~ b^{[1]} \\in \\mathbb{R}^{4 \\times 1}, ~ a^{[1]} \\in \\mathbb{R}^{4 \\times 1}\\right) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 각 변수의 차원을 적어볼 수 있는데 주의할 점은 데이터 샘플 하나에 대한 것이라는 점이다. (전체 데이터 $m$건에 대한 계산을 벡터화 하는 것은 다음 비디오에서 다루게 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L02_05.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-4]. Vectorizing across multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 비디오에서는 '한건의 샘플 데이터가 주어졌을때 for-loop를 이용한 각 레이어의 노드값($z$, $a$) 계산하는 것'을 벡터화하 방법에 대해 알아봤었다. (이 내용은 아래 'Step 1 코드'가 'Step 2 코드'로 바뀌는 것이다.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L03_01.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for the preparation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# etc\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def get_log_losses(y_true, y_pred):\n",
    "    return np.multiply(-y_true, np.log(y_pred)) - np.multiply(1-y_true, np.log(1-y_pred))\n",
    "\n",
    "\n",
    "# data\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = np.where(df[4] == 'Iris-setosa', 1, 0)\n",
    "df_train = df.sample(frac=0.7, random_state=0)\n",
    "df_test = df.sample(frac=0.3, random_state=0)\n",
    "\n",
    "X_test = np.array(df_test[[0, 1]]).T\n",
    "Y_test = np.array(df_test[[4]]).T\n",
    "X_train = np.array(df_train[[0, 1]]).T\n",
    "Y_train = np.array(df_train[[4]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the vectorized version  :  9.018182754516602ms\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "# model parameters\n",
    "n_x = X_train.shape[0]  # number of features\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y_train.shape[0]  # size of output layer (= 1)\n",
    "m = X_train.shape[1]  # number of samples\n",
    "alpha = 0.01  # learning rate\n",
    "num_epoch = 1\n",
    "\n",
    "# weights\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# Training - looping for epoch and for all traning data\n",
    "for _ in range(num_epoch):  # for each epoch\n",
    "\n",
    "    # initializing gradients and cost\n",
    "    J = 0;\n",
    "\n",
    "    for i in range(m):  # for each sample\n",
    "        x_i = X_train[:, i].reshape(-1, 1)  # column vector\n",
    "        y_i = Y_train[0, i].reshape(-1, 1)\n",
    "\n",
    "        z1_i = np.empty((n_h, 1))  # weighted sum for 1st hidden layer (n_h x 1 column vector)\n",
    "        a1_i = np.empty((n_h, 1))  # activation of 1st hidden layer (n_h x 1 column vector)\n",
    "        for j in range(n_h):  # for each node in the hidden layer\n",
    "            w1_ij = W1[j, :].reshape(-1, 1)  # weights[1] for j'th node (column vector)\n",
    "            z1_i[j] = np.dot(w1_ij.T, x_i) + b1[j]\n",
    "            a1_i[j] = np.tanh(z1_i[j])\n",
    "\n",
    "        z2_i = np.empty((n_y, 1))  # weighted sum for 2nd hidden layer (n_y x 1 column vector)\n",
    "        a2_i = np.empty((n_y, 1))  # activation of 2nd hidden layer (n_y x 1 column vector)\n",
    "        for j in range(n_y):  # for each node in the output layer\n",
    "            w2_ij = W2[j, :].reshape(-1, 1)  # weights[2] for j'th node (column vector)\n",
    "            z2_i[j] = np.dot(w2_ij.T, a1_i) + b2[j]\n",
    "            a2_i[j] = sigmoid(z2_i[j])\n",
    "\n",
    "toc = time.time()\n",
    "print('Execution time for the vectorized version  : ', str(1000 * (toc - tic)) + 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the vectorized version  :  5.002737045288086ms\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "# vectorized computation for each node\n",
    "# but still using for-loop for each sample\n",
    "\n",
    "# model parameters\n",
    "n_x = X_train.shape[0]  # number of features\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y_train.shape[0]  # size of output layer (= 1)\n",
    "m = X_train.shape[1]  # number of samples\n",
    "alpha = 0.01  # learning rate\n",
    "num_epoch = 1\n",
    "\n",
    "# weights\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# Training - looping for epoch and for all traning data\n",
    "for _ in range(num_epoch):  # for each epoch\n",
    "\n",
    "    # initializing gradients and cost\n",
    "    J = 0\n",
    "\n",
    "    for i in range(m):  # for each sample\n",
    "        x_i = X_train[:, i].reshape(-1, 1)  # column vector\n",
    "        y_i = Y_train[0, i].reshape(-1, 1)\n",
    "\n",
    "        z1_i = np.dot(W1, x_i) + b1   # weighted sum for 1st hidden layer (n_h x 1 column vector)\n",
    "        a1_i = np.tanh(z1_i)  # activation of 1st hidden layer (n_h x 1 column vector)\n",
    "\n",
    "        z2_i = np.dot(W2, z1_i) + b2\n",
    "        a2_i = sigmoid(z2_i)\n",
    "\n",
    "        loss = -1 * (np.log(a2_i) * y_i + (1 - y_i) * np.log(1 - a2_i))\n",
    "\n",
    "\n",
    "toc = time.time()\n",
    "print('Execution time for the vectorized version  : ', str(1000 * (toc - tic)) + 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 'Step 2' 구현을 더 발전시켜 $m$개 샘플에 대한 for-loop를 벡터화 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for Step 1\n",
    "$$\n",
    "\\begin{align}\n",
    "&m = \\text{number of samples} \\\\\n",
    "&n_h = \\text{number of nodes in the hidden layer} \\\\\n",
    "&n_y = \\text{number of nodes in the output layer} \\\\\n",
    "&\\textbf{for} ~ i=1 ~ \\text{to} ~ m ~ \\textbf{do} \\\\\n",
    "&\\qquad \\textbf{for} ~ j=1 ~ \\text{to} ~ n_h ~ \\textbf{do} \\\\\n",
    "&\\qquad \\qquad z^{[1](i)}_j = W^{[1]T}_j \\cdot x^{(i)} + b^{[1]}_j \\\\\n",
    "&\\qquad \\qquad a^{[1](i)}_j = \\sigma(z^{[1](i)}_j)\n",
    "&\\\\\n",
    "&\\qquad \\textbf{for} ~ j=1 ~ \\text{to} ~ n_y ~ \\textbf{do} \\\\\n",
    "&\\qquad \\qquad z^{[2](i)}_j = W^{[2]T}_j \\cdot z^{[1](i)} + b^{[2]}_j \\\\\n",
    "&\\qquad \\qquad a^{[2](i)}_j = \\sigma(z^{[2](i)}_j)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for Step 2\n",
    "$$\n",
    "\\begin{align}\n",
    "&m = \\text{number of samples} \\\\\n",
    "&n_h = \\text{number of nodes in the hidden layer} \\\\\n",
    "&n_y = \\text{number of nodes in the output layer} \\\\\n",
    "&\\textbf{for} ~ i=1 ~ \\text{to} ~ m ~ \\textbf{do} \\\\\n",
    "&\\qquad z^{[1](i)} = W^{[1]} \\cdot x^{(i)} + b^{[1]} \\\\\n",
    "&\\qquad a^{[1](i)} = \\sigma(z^{[1](i)})\n",
    "&\\\\\n",
    "&\\qquad z^{[2](i)} = W^{[2]T}_j \\cdot z^{[1](i)} + b^{[2]} \\\\\n",
    "&\\qquad a^{[2](i)} = \\sigma(z^{[2](i)})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm for Step 3\n",
    "$$\n",
    "\\begin{align}\n",
    "&m = \\text{number of samples} \\\\\n",
    "&n_h = \\text{number of nodes in the hidden layer} \\\\\n",
    "&n_y = \\text{number of nodes in the output layer} \\\\\n",
    "& Z^{[1]} = W^{[1]} \\cdot X + b^{[1]} \\\\\n",
    "&A^{[1]} = \\sigma(Z^{[1]})\n",
    "&\\\\\n",
    "&Z^{[2]} = W^{[2]} \\cdot A^{[1]} + b^{[2]} \\\\\n",
    "&A^{[2]} = \\sigma(Z^{[2]})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L03_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L03_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L03_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-5]. Explanation for Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 설명한 내용이므로 설명 생략."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-6]. Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 종류의 활성함수(activation function)가 있다.\n",
    "\n",
    "- **sigmoid** : 0과 1 사이의 값을 출력하므로 이를 확률 값으로 사용하는 이항분류 문제의 출력 레이어에 사용된다. \n",
    "$$g(z) = 1/(1+e^{-z})$$\n",
    "\n",
    "- **tanh** : sigmoid를 아래로 이동하고 위아래로 늘린 버전으로서 거의 항상 sigmiod보다 성능이 좋다. tanh는 데이터값을 0을 중심으로 모이게 하는 효과(centering)가 있다. 이 때문에 sigmoid보다 학습이 용이하다.\n",
    "$$g(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "\n",
    "각 레이어마다 다른 활성함수(activation function)를 사용할수있으므로, 각 레이어의 활성를 표현하기위해, $l$번째 레이어의 활성함수를 $g^{[l]}$로 표기할 것이다.\n",
    "\n",
    "sigmoid, tanh 모두가 갖는 문제점은 z가 매우커지거나 매우작아지면 gradient값이 너무작아지는 점이다.(0에가까워짐) 이는 gradient descent의 수렴속도를 매우 느리게 만든다. Relu(rectified linear unit)를 사용하면 이런 문제를 극복할 수 있다.\n",
    "\n",
    "- **ReLU** : Rectified Linear Unit, 함수 자체는 z가0인 지점에서 미분이 안되는데 z가 0 이면 gradient를 1혹은 0으로 정하고 사용한다. ReLU도 단점으로 z가 0보다 작을 경우 derivative가 0이라는 특정이 있는데 물론 이점이 학습에 큰 영향을 주지는 않는다. 0이하에서0이 아닌 미분값을 원한다면 Leaky ReLU를 사용할수도 있다. \n",
    "$$g(z) = \\max(0, z)$$\n",
    "\n",
    "- **Leaky ReLU**\n",
    "$$g(z) = \\max(0.01z, z)$$\n",
    "\n",
    "\n",
    "신경망을 구성할 때 어떤 activation func 를 사용할지, 히든 레이어는 몇개로 할지 유닛은 몇개로할지등 여러가지 선택 사항이 있다. 사실 그 어느것에도 장답은 없고, 여러기지를 시도해보고 내 데이터에 잘 맞는것을 사용하면 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L04_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L04_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-7]. Why do you need non-linear activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 왜 activation function으로 sigmoid, tanh, relu와 같은 비선형(non-linear)함수를 사용하는 걸까? 그냥 $f(x) = x$와 같은 선형함수를 사용하면 안되나?\n",
    "\n",
    "예를들어 아래 그림과 같은 네트워크가 있고\n",
    "$g^{[1]}()$을 선형함수인 항등 함수(identity function) $g(z) = z$로 바꾼다고 해 보자. \n",
    "이때 $a^{[2]}$를 계산해보면 $a^{[2]} = (W^{[2]} W^{[1]}) x + (W^{[2]}b^{[1]} + b^{[2]})$ 이고, 이는 결국 $W'x + b'$라 할 수 있다. 이는 결국 입력$x$에 대한 선형함수 이므로 데이터가 갖는 (복잡한) 패턴을 찾아낼 수 없게 된다. \n",
    "\n",
    "물론 선형함수가 사용되는 경우가 가끔 있는데 타겟값이 실수인 회귀 문제의 경우 출력레이어에 선형함수를 사용하기도 한다. (예를들어 주택값을 예측하는 문제의 경우 타겟값의 범위가 0에서부터 상당히 큰 수 까지이므로). 하지만 이런 경우에도 히든 레이어의 엑티베이션 함수는 비선형함수를 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L05_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-8]. Derivatives of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(물론 TensorFlow나 Keras 등을 사용하면 직접 계산할 일은 없겠으나...) 역전파를 계산할 때 활성함수(activation function)의 1차 도함수(derivative, gradient, slop, 기울기, 1차 미분 등으로 표현됨)를 계산해야만 한다. 물론 sigmoid, tanh, ReLU 등과 같이 1차 미분이 잘 알려져 있고 그 형태가 간단한 활성함수들을 주로 사용하게 된다. 이번 비디오에서는 주로 사용하게될 활성함수의 1차 미분이 어떤 형태인지 알아본다.\n",
    "\n",
    "우선 이항 분류 문제의 출력 레이어에 주로 사용되는 활성함수인 시그모이드 함수(sigmoid function)의 1차 미분은 아래와 같다.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dz}g(z) &= g(z)(1-g(z))\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L06_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh 함수는 아래와 같으며 \n",
    "$$g(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "$z$에 대한 1차 도함수는 아래와 같다.\n",
    "$$g'(z) = \\frac{d}{dz}g(z) = 1 - g(z)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L06_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 함수는 그 생김새 처럼 미분항 또한 단순하다. 사실 $z$가 0일 때 미분이 안되지만 그냥 $g'(0)$을 1 혹은 0이라 하고 정하고 사용하며 이렇게 하더라도 모형 학습에 아무런 문제가 되지 않는다.\n",
    "$$\\begin{align}\n",
    "&g(z) = \\max(0, z) \\\\\n",
    "&g'(z) = \n",
    "\\begin{cases}\n",
    "0 &\\text{if}~ z < 0\\\\\n",
    "1 &\\text{if}~ z \\geq 0\\\\\n",
    "\\end{cases}\n",
    "\\end{align}$$\n",
    "\n",
    "Leaky ReLU 함수와 1차 도함수는 아래와 같다. ReLU와 마찬가지로 $z$가 0일 때 미분값을 1이나 0.01로 정하고 사용한다.\n",
    "$$\\begin{align}\n",
    "&g(z) = \\max(0.01z, z) \\\\\n",
    "&g'(z) = \n",
    "\\begin{cases}\n",
    "0.01 & \\text{if}~ z < 0\\\\\n",
    "1    & \\text{if}~ z \\geq 0\\\\\n",
    "\\end{cases}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L06_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-9]. Gradient descent for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "히든레이어가 하나 있는 신경망을 예로들어 gradient descent를 어떻게 구현하는지 알아자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-10]. Backpropagation intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀모형에서는 아래의 계산 그래프(computation graph)와 같이 입력 데이터 $x$가 주어졌을 때             \n",
    "어떤 미지의 파라미터 $w$와 $b$를 상정하고 아래 계산 과정을 통해 예측값과 실제값 사이의 오차(log-loss)를 구하고\n",
    "- $x$들의 가중합 $z=w^T x + b$를 계산\n",
    "- (앞서 계산한 $z$를 이용하여) 예측값 $\\hat{y} = a = \\sigma(z) = \\frac{1}{1 + e^{-z}}$ 를 계산\n",
    "- 앞서 구한 예측값 $a$와 실제값 $y$와의 오차를 계량하는 log-loss 값 $\\mathcal{L}(a, y)$를 계산       \n",
    "\n",
    "아래 과정과 같이 각 파라미터 $w \\in \\{w_1, w_2, \\cdots\\}$, $b$가 오차에 주는 영향을 계산하고\n",
    "- $a$가 $\\mathcal{L}(a, y)$에 주는 영향 $\\text{da} = \\frac{\\partial \\mathcal{L}}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a}$을 계산\n",
    "- $z$가 $\\mathcal{L}(a, y)$에 주는 영향 $\\text{dz} = \\frac{\\partial \\mathcal{L}}{\\partial z} = \\frac{\\partial a}{\\partial z} \\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial a}{\\partial z} \\text{da} = a-y$을 계산\n",
    "- $w$가 $\\mathcal{L}(a, y)$에 주는 영향 $\\text{dw} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial z}{\\partial w} \\frac{\\partial a}{\\partial z} \\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial z}{\\partial w} \\text{dz} = w\\cdot da$을 계산\n",
    "- $b$가 $\\mathcal{L}(a, y)$에 주는 영향 $\\text{db} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial z}{\\partial b} \\frac{\\partial a}{\\partial z} \\frac{\\partial \\mathcal{L}}{\\partial a} = 1 \\cdot \\text{dz}$\n",
    "\n",
    "아래와 같이 영향도에 따라 각 파라미터를 수정하게 된다.($\\alpha$는 learning rate)\n",
    "- $w^{new} = w^{old} - \\alpha \\cdot dw$\n",
    "- $b^{new} = b^{old} - \\alpha \\cdot db$\n",
    "\n",
    "사실 이 과정은 아래 슬라이드의 우측 상단의 그림과 같이, 히든 레이어 없이 노드 하나짜리 출력 레이어(output layer)만 존재하는 신경망에서의 순전파와 역전파 계산과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L08_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 논의를 조금 확장하여 히든 레이어가 하나 있는 신경망에서의 순전파와 역전파를 생각해보자. 이 경우 앞서 입력값 $x$의 가중합이 바로 출력 레이어인 sigmoid 함수의 입력값으로 들어가 출력값이 나오는 것과 달리, 입력값이 히든 레이어의 활성함수(activation function)을 지나 그 출력이 sigmoid 함수의 입력값으로 사용된다.\n",
    "\n",
    "각 과정을 따라가보면 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 F1부터 F5번까지 식은 순전파(forward-propagation) 과정이며, B1번부터 B4-2까지 식은 역전파(backward-propagation) 과정이다. (아래 식에서 연산자 $\\cdot$은 행렬곱(matrix multiplication)이며, 연산자 $*$는 shape이 같은 두 행렬의 같은 위치의 원소들을 각각 곱하는 것(element wise multiplication)을 의미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "& & \\text{db1} = \\frac{\\partial \\mathcal{L}}{\\partial db^{[1]}} = \\frac{\\partial z^{[1]}}{b^{[1]}} \\left( \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) = \\text{dz1} \\cdot 1 \\qquad (B4-2) \\\\\n",
    "& & \\text{dW1} = \\frac{\\partial \\mathcal{L}}{\\partial dW^{[1]}} = \\frac{\\partial z^{[1]}}{W^{[1]}} \\left( \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) = \\text{dz1} \\cdot x^T \\qquad (B4-1) \\\\\n",
    "&(F1) \\qquad z^{[1]}=W^{[1]}x + b^{[1]}       & \\text{dz1} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[1]}} = \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\left( \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) =  \\frac{\\partial g^{[1]}}{\\partial z^{[1]}} * \\text{da1} \\qquad (B4) \\\\\n",
    "&(F2) \\qquad a^{[1]} = g^{[1]}(z^{[1]})        & \\text{da1} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[1]}} = \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\left( \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) = w^{[2]T} \\cdot \\text{dz2} \\qquad (B3) \\\\\n",
    "& & \\text{db2} = \\frac{\\partial \\mathcal{L}}{\\partial db^{[2]}} = \\frac{\\partial z^{[1]}}{b^{[2]}} \\left( \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) = \\text{dz2} \\cdot 1 \\qquad (B2-2) \\\\\n",
    "& & \\text{dW2} = \\frac{\\partial \\mathcal{L}}{\\partial dW^{[2]}} = \\frac{\\partial z^{[1]}}{W^{[2]}} \\left( \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} \\right) = \\text{dz2} \\cdot a^{[1]^T} \\qquad (B2-1) \\\\\n",
    "&(F3) \\qquad z^{[2]}=W^{[2]}a^{[1]} + b^{[2]} & \\text{dz2} = \\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}} = \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} = a^{[2]} * (1-a^{[2]}) * \\text{da2} = a^{[2]}-y \\qquad (B2) \\\\\n",
    "&(F4) \\qquad a^{[2]} = \\sigma(z^{[2]})        & \\text{da2} = \\frac{\\partial \\mathcal{L}}{\\partial a^{[2]}} = \\left(-\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}\\right) \\qquad (B1) \\\\\n",
    "&(F5) \\qquad \\mathcal{L}(a^{[2]}, y)          & \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(F1). z1.shape: (4, 1)\n",
      "(F2). a1.shape: (4, 1)\n",
      "(F3). z2.shape: (1, 1)\n",
      "(F4). a2.shape: (1, 1)\n",
      "(F5). loss.shape: (1, 1)\n",
      "(B1). da2.shape: (1, 1)\n",
      "(B2). dz2.shape: (1, 1)\n",
      "(B2-1). dW2.shape: (1, 4)\n",
      "(B2-2). db2.shape: (1, 1)\n",
      "(B3). da1.shape: (4, 1)\n",
      "(B4). dz1.shape: (4, 1)\n",
      "(B4-1). dW1.shape: (4, 2)\n",
      "(B4-2). db1.shape: (4, 1)\n",
      "(4, 2) (4, 1) (1, 4) (1, 1)\n",
      "Average loss for the test set:0.6710470655304303\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "# vectorized computation for each node\n",
    "# but still using for-loop for each sample\n",
    "\n",
    "# model parameters\n",
    "n_x = X_train.shape[0]  # number of features\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y_train.shape[0]  # size of output layer (= 1)\n",
    "m = X_train.shape[1]  # number of samples\n",
    "alpha = 0.01  # learning rate\n",
    "num_epoch = 100\n",
    "\n",
    "# weights\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# Training - looping for epoch and for all traning data\n",
    "for e in range(num_epoch):  # for each epoch\n",
    "    \n",
    "    dW1 = np.zeros((n_h, n_x))\n",
    "    db1 = np.zeros((n_h, 1))\n",
    "    dW2 = np.zeros((n_y, n_h))\n",
    "    db2 = np.zeros((1, 1))\n",
    "    \n",
    "    for i in range(m):  # for each sample\n",
    "        x_i = X_train[:, i].reshape(-1, 1)  # xi.shape = (n_x, 1) = (2, 1)\n",
    "        y_i = Y_train[0, i].reshape(-1, 1)  # y.shape = (n_y, 1) = (1, 1), actual target value\n",
    "\n",
    "        # forward-propagation\n",
    "        z1_i = np.dot(W1, x_i) + b1  # (F1) z1.shape = (n_h, 1) = (4, 1)\n",
    "        a1_i = np.tanh(z1_i)  # (F2) a1.shape = (n_h, 1) = (4, 1)\n",
    "        z2_i = np.dot(W2, a1_i) + b2  # (F3) shape=(n_y, 1) = (1, 1)\n",
    "        a2_i = sigmoid(z2_i)  # (F4) shape = (n_y, 1) = (1, 1)\n",
    "        loss = -y_i * np.log(a2_i) - (1 - y_i) * np.log(1 - a2_i)  # (F5) loss.shape = (n_y, 1) = (1, 1)\n",
    "\n",
    "        # backward-propagation\n",
    "        da2_i = (-y_i / a2_i + (1 - y_i) / (1 - a2_i))  # (B1) da2.shape = (n_y by 1) = (1, 1)\n",
    "        # dz2_i = np.multiply(a2_i, 1 - a2_i, da2_i)  # (B2) dz2.shape = (n_y, 1) = (1, 1), same calculation as (a2_i - y_i)\n",
    "        dz2_i = a2_i - y_i\n",
    "        \n",
    "        dW2_i = np.dot(dz2_i, a1_i.T)  # (B2-1) dW2.shape = (n_y, n_h) = (1, 4), same shape as W2\n",
    "        db2_i = np.dot(dz2_i, 1)  # (B2-2) db2.shape = (n_y, 1) = (1, 1), same shape as b2\n",
    "        dW2 += dW2_i\n",
    "        db2 += db2_i\n",
    "        \n",
    "        da1_i = np.dot(W2.T, dz2_i)  # (B3) da1.shape=(n_h, 1) = (4, 1), W2.shape = (n_y, n_h) = (1, 4) \n",
    "        dz1_i = np.multiply(1 - a1_i ** 2, da1_i)  # (B4) dz1.shape = (n_h, 1) = (4, 1)\n",
    "\n",
    "        dW1_i = np.dot(dz1_i, x_i.T)  # (B4-1) dW1.shape = (n_h, n_x) = (4, 2), same shape as W1\n",
    "        db1_i = np.dot(dz1_i, 1)  # (B4-2) db1.shape = (n_h, 1) = (4, 1), same shape as b1\n",
    "        dW1 += dW1_i\n",
    "        db1 += db1_i\n",
    "        \n",
    "        if i == 0 and e == 0:\n",
    "            print('(F1). z1.shape:', z1_i.shape)\n",
    "            print('(F2). a1.shape:', a1_i.shape)\n",
    "            print('(F3). z2.shape:', z2_i.shape)\n",
    "            print('(F4). a2.shape:', a2_i.shape)\n",
    "            print('(F5). loss.shape:', loss.shape)\n",
    "\n",
    "            print('(B1). da2.shape:', da2_i.shape)\n",
    "            print('(B2). dz2.shape:', dz2_i.shape)\n",
    "            print('(B2-1). dW2.shape:', dW2_i.shape)\n",
    "            print('(B2-2). db2.shape:', db2_i.shape)\n",
    "            print('(B3). da1.shape:', da1_i.shape)\n",
    "            print('(B4). dz1.shape:', dz1_i.shape)\n",
    "            print('(B4-1). dW1.shape:', dW1_i.shape)\n",
    "            print('(B4-2). db1.shape:', db1_i.shape)\n",
    "            \n",
    "    # updating weights after each epoch\n",
    "    W1 -= alpha * dW1/m\n",
    "    b1 -= alpha * db1/m\n",
    "    W2 -= alpha * dW2/m\n",
    "    b2 -= alpha * db2/m\n",
    "    \n",
    "print(W1.shape, b1.shape, W2.shape, b2.shape)\n",
    "\n",
    "# prediction\n",
    "y_pred = sigmoid(np.dot(W2, np.tanh(np.dot(W1, X_test) + b1)) + b2)\n",
    "losses = get_log_losses(Y_test, y_pred)\n",
    "print('Average loss for the test set:{}'.format(np.average(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L08_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L08_03.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for the test set:0.6710470655304303\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "# fully vectorized\n",
    "\n",
    "# model parameters\n",
    "n_x = X_train.shape[0]  # number of features\n",
    "n_h = 4  # number of units in the hidden layer\n",
    "n_y = Y_train.shape[0]  # size of output layer (= 1)\n",
    "m = X_train.shape[1]  # number of samples\n",
    "alpha = 0.01  # learning rate\n",
    "num_epoch = 100\n",
    "\n",
    "# weights\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# Training - looping for epoch and for all traning data\n",
    "for e in range(num_epoch):  # for each epoch\n",
    "    \n",
    "    dW1 = np.zeros((n_h, n_x))\n",
    "    db1 = np.zeros((n_h, 1))\n",
    "    dW2 = np.zeros((n_y, n_h))\n",
    "    db2 = np.zeros((1, 1))\n",
    "    \n",
    "    # forward-propagation\n",
    "    Z1 = np.dot(W1, X_train) + b1  # (n_h, n_x) dot (n_x, m) + (n_h, 1)\n",
    "    A1 = np.tanh(Z1)  # (n_h, m)\n",
    "    Z2 = np.dot(W2, A1) + b2  # (n_y, n_h) dot (n_h, m) + (n_y, 1)\n",
    "    A2 = sigmoid(Z2)  # (n_y, m)\n",
    "    \n",
    "    # backward-propagation\n",
    "    dZ2 = A2 - Y_train  # (n_y, m)\n",
    "    dW2 = np.dot(dZ2, A1.T)  # (n_y, m) dot (m, n_h) and averaging -> (n_y, n_h)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)  # sum rows of (n_y, m) and averaging -> (n_y, 1)\n",
    "    dZ1 = np.multiply(1 - A1 **2, np.dot(W2.T, dZ2))  # (n_h, m)\n",
    "    dW1 = np.dot(dZ1, X_train.T)  # (n_h, m) dot (m, n_x) and averaging -> (n_h, n_x)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)  # sum rows of (n_h, m) and averaging -> (n_h, 1)\n",
    "            \n",
    "    # updating weights after each epoch\n",
    "    W1 -= alpha * dW1/m\n",
    "    b1 -= alpha * db1/m\n",
    "    W2 -= alpha * dW2/m\n",
    "    b2 -= alpha * db2/m\n",
    "    \n",
    "# prediction\n",
    "y_pred = sigmoid(np.dot(W2, np.tanh(np.dot(W1, X_test) + b1)) + b2)\n",
    "losses = get_log_losses(Y_test, y_pred)\n",
    "print('Average loss for the test set:{}'.format(np.average(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L08_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-11]. Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법(gradient descent)과 같이 점근/반복적 방법으로 파라미터들(weights)를 구하는 방법을 사용할 때 파라미터에 대한 초기값 설정이 필요하다. 로지스틱 회귀모형의 경우 모든 파라미터 값을 0으로 초기화 해도 문제가 없지만, 신경망의 경우 파라미터 값을 0으로 초기화 할 경우 경사하강법이 잘 동작하지 않는다. \n",
    "\n",
    "아래 그림과 같이 2개의 입력 변수와 2개의 레이어(hidden & output)로 구성된 네트워크에서 파라미터 $W$와 바이어스 $b$를 모두 0으로 설정하면 $a^{[1]}_1$과 $a^{[1]}_2$가 $0 + x_1 \\cdot 0 + x_2 \\cdot 0$로 같아진다. 또한 $dz^{[1]}_1$과 $dz^{[1]}_2$ 값도 같아진다.  역전파(backpropagation)가 진행되면 파라미터 $W$의 값이 0에서는 벗어나게 되지만 $w^{[1]}_1$과 $w^{[1]}_2$가 같은 값으로 변하게 되므로($W$행렬의 각 행이 같은 값을 갖는다.), 다시 $a^{[1]}_1$과 $a^{[1]}_2$가 같아진다. 결국 각 레이어에 뉴닛은 많을 수 있지만 모두 같은 함수계산이 된다. 이런 대칭(symmetry)문제가 있게 되면 최적화가 잘 되지 않는다. (사실 바이어스의 초기값을 0으로 하는 것은 문제를 일으키지 않는다.)\n",
    "\n",
    "이런 문제를 해결하기 위해 파라미터 $W$를 어떤 임의의 값으로 설정하게 된다. 임의초기화(random initialization)을 위한 파이썬 코드는 아래와 같다.\n",
    "\n",
    "```Python\n",
    "W1 = np.random.randn((2,2)) * 0.01     \n",
    "b1 = np.zero((2,1))     \n",
    "     \n",
    "```\n",
    "\n",
    "위 초기화 코드에서 왜 랜덤값에 0.01을 곱하는 것은 $w$ 초기값이 지나치케 커지면 $z$값도 커지게되고 (sigmoid나 tanh 의 경우 큰 입력값에 대한 gradient가 작기 때문에) 학습 속도가 느려지게 된다. 신경망의 깊이등 상황에 따라 0.01이 아니라 다른 상수값을 사용할 수도 있는데, 적절한 상수를 선택하는 방법에 대해서 추후 다루게 될 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L09_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./images/C1W3L09_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
