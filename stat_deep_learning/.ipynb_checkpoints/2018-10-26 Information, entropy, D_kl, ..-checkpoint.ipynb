{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\mathbb{I}(X=x_k) & \\triangleq - \\log p(X=x_k) \\\\\n",
    "& = - \\log p_k\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\mathbb{H}(X) &\\triangleq \\mathbb{E} \\big[ \\mathbb{I}(X=x_k) \\big] \\\\\n",
    "& = -\\sum_{k=1}^K p(X=x_k) \\log p(X=x_k) \\\\\n",
    "& = -\\sum_{k=1}^K p_k \\log p_k \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\mathbb{H}(q, p) &\\triangleq \\mathbb{E} \\big[ \\mathbb{I}(X=x_k) \\big] \\\\\n",
    "& = -\\sum_{k=1}^K q(X=x_k) \\log p(X=x_k) \\\\\n",
    "& = -\\sum_{k=1} q_k \\log p_k \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{1}{N}\\text{NLL}$ = cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  - $X$: random variable for categorical events, $\\{ c_1, \\dots, c_i, \\dots, c_n \\}$.\n",
    " - $p$: hypothetical distribution for $X$, $p(X=c_i) = y_i$\n",
    " - $\\text{data}$: total $N$ times of trial, which is $\\{ k_1 \\text{ times of } c_1,  \\dots, k_n \\text{ times of } c_n\\}$\n",
    " - $q$: empirical distribution for $X$, $q(X=c_i) = \\frac{k_i}{N} = y'_i$\n",
    "\n",
    "> - likelihood for $data$ under a $model$ : \n",
    "$$P[data \\mid model] = y_1^{k_1} \\cdots y_i^{k_i} \\cdots y_n^{k_n}$$     \n",
    "> - negative log likelihood(divided by $N$): \n",
    "$$\\begin{align}\n",
    "-\\frac{1}{N} \\log P[data \\mid model] &= -\\frac{1}{N} \\bigg( k_1 \\log y_1 + k_2 \\log y_2 + \\cdots + k_n \\log y_n \\bigg) \\\\\n",
    "& = -\\frac{1}{N} \\sum_i k_i \\log y_i \\\\\n",
    "& = -\\sum_i y'_i \\log y_i \\\\\n",
    "& = \\mathbb{H}(y', y)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 예시(범주형)\n",
    "- 각 면의 나올 확률이 동일하지 않은 동전을 5번 던진다고 해 보자. 동전을 던지기 전 예상으로는 앞면이 나올 확률이 $3/7$, 뒷면이 나올 확률이 $4/7$이다. 동전을 5번 던진 후 데이터와 비교하여 예상했던 확률의 cross entropy 혹은 negative log likelihood를 생각해보자. (loss라고 봐도 무방하다. 예상이 실제 확률에 가까울수록 이 값이 작아질 것이다.) 5번 던진 결과 앞면이 1번 뒷면이 4번 나왔다면, 실험으로 얻어진 확률은 $(1/5, 4/5)$이다.\n",
    "- 이때의 cross entropy는 아래와 같다.\n",
    "$$\\begin{align}\n",
    "\\text{NLL} &= -\\frac{1}{5} \\bigg( 1 \\cdot \\log \\frac{3}{7} + 4 \\cdot \\log \\frac{4}{7} \\bigg) \\\\\n",
    "&\\approx 0.617\n",
    "\\end{align}$$\n",
    "- 만약 최초 예상이 (실험 결과에 조금 더 가깝게) 앞면이 나올 확률은 $2/7$, 뒷면이 나올 확률은 $5/7$이라면 $\\text{NLL}$은 아래와 같다. 앞서 결과와 비교해보면 $\\text{NLL}$ 값이 조금 더 작다. 즉 $\\text{NLL}$을 최소화하면 실제에 가까운 파라미터를 찾아가게 된다.\n",
    "$$\\begin{align}\n",
    "\\text{NLL} &= -\\frac{1}{5} \\bigg( 1 \\cdot \\log \\frac{2}{7} + 4 \\cdot \\log \\frac{5}{7} \\bigg) \\\\\n",
    "&\\approx 0.520\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 예시(연속형)\n",
    "- 낚시 지렁이들을 키우고 있는데, 어느정도 자란 녀석들의 길이를 측정한다고 해보자. 지렁이들의 길이를 측정하기 전에 지렁이 길이($G$)의 분포는 $G \\sim \\mathcal{N}(12, 3^2)$일것이라 예상한다. 실제로 지렁이 5마리의 키를 측정했더니 $(g_1=13, g_2=12, g_3=15, g_4=14, g_5=15)$이었다. 이때의 cross-entropy(=NLL)은?\n",
    "\n",
    ">-  $$\\begin{align} \n",
    "\\text{NLL} &= -\\frac{1}{5} \\bigg( \\log p(g_1=13 \\mid G \\sim \\mathcal{N}(12, 3^2)) + \\cdots + \\log p(g_5=15 \\mid G \\sim \\mathcal{N}(12, 3^2)) \\bigg) \\\\\n",
    "&= -\\frac{1}{5} \\bigg( \\log(1/\\sqrt{2 \\cdot \\pi \\cdot 3^2}) - \\frac{(13-12)^2}{2 \\cdot 3^2} + \\cdots + \\log(1/\\sqrt{2 \\cdot \\pi \\cdot 3^2}) - \\frac{(15-12)^2}{2 \\cdot 3^2} \\bigg) \\\\\n",
    "&= -\\frac{1}{5} \\bigg(  5\\log(1/\\sqrt{2 \\cdot \\pi \\cdot 3^2}) -\\sum_{i=1}^5 \\frac{(g_i - 12)^2}{2 \\cdot 3^2}  \\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "> - 추정하려는 분포가 정규분포라면, NLL와 MSE는 단지 상수 만큼 차이가 있으므로 구현시에는 그냥 MSE를 사용하는 이유를 위에서 확인할 수 있다. 또한 NLL, cross-entropy, MSE를 최소화 하는 $G$의 파라미터를 찾는게 결국 Maximum likelihood estimation이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Reference\n",
    " - https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks?newreg=68e217d2659e4391914bf7ee19217138\n",
    " - https://taeoh-kim.github.io/blog/cross-entropy의-정확한-확률적-의미/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL divergence = relative entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\begin{align}\n",
    "\\mathbb{KL}(p \\Vert q) &\\triangleq \\sum_{k=1}^K p_k \\log \\frac{p_k}{q_k} \\\\\n",
    "&= \\sum_k p_k \\log p_k - \\sum_k p_k \\log q_k \\\\\n",
    "&= -\\mathbb{H}(p) + \\mathbb{H}(p, q)\n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
