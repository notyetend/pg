{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v012 : 테스트 데이터 생성 & 로지스틱 회귀\n",
    "\n",
    "v013 : applied tinrtgu's idea to this test dataset\n",
    "\n",
    "v014 : implementing ADF, 구현 완료 했고, 테스트 데이터에 적용해 봄.\n",
    "\n",
    "v015 : v010 -> v015, v014에서 구현한 내용을 criteo 데이터에 적용해 보자.\n",
    "\n",
    "v016 : 성능이 안 나오는 이유를 찾아야 함. 데이터 규모를 줄이고, Tinrtgu의 idea와 성능을 비교해 보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://kaggle2.blob.core.windows.net/forum-message-attachments/53646/1539/fast_solution.py?sv=2012-02-12&se=2015-12-04T20%3A40%3A32Z&sr=b&sp=r&sig=qTDaOlHCMWaqBB9aOK6haM6Vo2FmmkfopqtwQaexnC0%3D]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinrtgu's original idea below(little modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "                   Version 2, December 2004\n",
    "\n",
    "Copyright (C) 2004 Sam Hocevar <sam@hocevar.net>\n",
    "\n",
    "Everyone is permitted to copy and distribute verbatim or modified\n",
    "copies of this license document, and changing it is allowed as long\n",
    "as the name is changed.\n",
    "\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "  TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n",
    "\n",
    " 0. You just DO WHAT THE FUCK YOU WANT TO.\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "\n",
    "# parameters #################################################################\n",
    "#train = 'train.csv'  # path to training file\n",
    "#test = 'test.csv'  # path to testing file\n",
    "\n",
    "#train = r'C:\\Users\\Bar\\Downloads\\dac.tar\\train.txt'\n",
    "#test = r'C:\\Users\\Bar\\Downloads\\dac.tar\\test.txt'\n",
    "\n",
    "train = r'D:\\9000_etc\\Thesis\\data\\dac_sample.tar\\dac_sample.txt'\n",
    "test = r'D:\\9000_etc\\Thesis\\data\\dac_sample.tar\\dac_sample.txt'\n",
    "\n",
    "submission_dir = r'D:\\9000_etc\\Thesis\\data\\submission'\n",
    "\n",
    "D = 2 ** 20   # number of weights use for learning\n",
    "alpha = .1    # learning rate for sgd optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function definitions #######################################################\n",
    "\n",
    "# A. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Apply hash trick of the original csv row\n",
    "# for simplicity, we treat both integer and categorical features as categorical\n",
    "# INPUT:\n",
    "#     csv_row: a csv dictionary, ex: {'Lable': '1', 'I1': '357', 'I2': '', ...}\n",
    "#     D: the max index that we can hash to\n",
    "# OUTPUT:\n",
    "#     x: a list of indices that its value is 1\n",
    "def get_x(csv_row, D):\n",
    "    x = [0]  # 0 is the index of the bias term\n",
    "    for key, value in csv_row.items():\n",
    "        #index = int(value + key[1:], 16) % D  # weakest hash ever ;)\n",
    "        if value is None:\n",
    "            index = int(key[1:], 16) % D  # weakest hash ever ;)\n",
    "        else:\n",
    "            index = int(value + key[1:], 16) % D  # weakest hash ever ;)\n",
    "        x.append(index)\n",
    "    return x  # x contains indices of features that have a value of 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def get_p(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D. Update given model\n",
    "# INPUT:\n",
    "#     w: weights\n",
    "#     n: a counter that counts the number of times we encounter a feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# OUTPUT:\n",
    "#     w: updated model\n",
    "#     n: updated count\n",
    "def update_w(w, n, x, p, y):\n",
    "    for i in x:\n",
    "        # alpha / (sqrt(n) + 1) is the adaptive learning rate heuristic\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1\n",
    "        w[i] -= (p - y) * alpha / (sqrt(n[i]) + 1.)\n",
    "        n[i] += 1.\n",
    "\n",
    "    return w, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-17 20:13:57.373000\tencountered: 10000\t, y=0:p=0.181009, current logloss: 0.473324\n",
      "2016-03-17 20:13:57.852000\tencountered: 20000\t, y=0:p=0.340927, current logloss: 0.472425\n",
      "2016-03-17 20:13:58.338000\tencountered: 30000\t, y=0:p=0.187765, current logloss: 0.466744\n",
      "2016-03-17 20:13:58.830000\tencountered: 40000\t, y=0:p=0.417896, current logloss: 0.462163\n",
      "2016-03-17 20:13:59.320000\tencountered: 50000\t, y=0:p=0.020880, current logloss: 0.460264\n",
      "2016-03-17 20:13:59.810000\tencountered: 60000\t, y=0:p=0.033342, current logloss: 0.459394\n",
      "2016-03-17 20:14:00.304000\tencountered: 70000\t, y=0:p=0.243366, current logloss: 0.458124\n",
      "2016-03-17 20:14:00.853000\tencountered: 80000\t, y=0:p=0.215531, current logloss: 0.456547\n",
      "2016-03-17 20:14:01.373000\tencountered: 90000\t, y=0:p=0.217478, current logloss: 0.455392\n"
     ]
    }
   ],
   "source": [
    "# training and testing #######################################################\n",
    "\n",
    "# initialize our model\n",
    "w = [0.] * D  # weights\n",
    "n = [0.] * D  # number of times we've encountered a feature\n",
    "\n",
    "# start training a logistic regression model using on pass sgd\n",
    "loss = 0.\n",
    "\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(train)\n",
    "fn = ['Label'] + [ 'I' + str(i) for i in list(range(1,14))] + [ 'C' + str(i) for i in list(range(1,27))]\n",
    "for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):   \n",
    "#for t, row in enumerate(DictReader(open(train))):\n",
    "    y = 1. if row['Label'] == '1' else 0.\n",
    "\n",
    "    del row['Label']  # can't let the model peek the answer\n",
    "    #del row['Id']  # we don't need the Id\n",
    "\n",
    "    # main training procedure\n",
    "    # step 1, get the hashed features\n",
    "    x = get_x(row, D)\n",
    "\n",
    "    # step 2, get prediction\n",
    "    p = get_p(x, w)\n",
    "\n",
    "    # for progress validation, useless for learning our model\n",
    "    loss += logloss(p, y)\n",
    "    if t % 10000 == 0 and t > 1:\n",
    "        print('%s\\tencountered: %d\\t, y=%d:p=%f, current logloss: %f' % (\n",
    "            datetime.now(), t, y, p, loss/t))\n",
    "            \n",
    "    if t == 100000:\n",
    "        print(t)\n",
    "        break;\n",
    "    \n",
    "    # step 3, update model with answer\n",
    "    w, n = update_w(w, n, x, p, y)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-17 20:18:11.986000\tencountered: 10000\t, y=0:p=0.144410, current logloss: 0.365669\n",
      "2016-03-17 20:18:12.330000\tencountered: 20000\t, y=0:p=0.341850, current logloss: 0.380957\n",
      "2016-03-17 20:18:12.683000\tencountered: 30000\t, y=0:p=0.180576, current logloss: 0.385345\n",
      "2016-03-17 20:18:13.040000\tencountered: 40000\t, y=0:p=0.369224, current logloss: 0.387397\n",
      "2016-03-17 20:18:13.395000\tencountered: 50000\t, y=0:p=0.017098, current logloss: 0.390429\n",
      "2016-03-17 20:18:13.761000\tencountered: 60000\t, y=0:p=0.034963, current logloss: 0.393168\n",
      "2016-03-17 20:18:14.123000\tencountered: 70000\t, y=0:p=0.183090, current logloss: 0.395017\n",
      "2016-03-17 20:18:14.482000\tencountered: 80000\t, y=0:p=0.189008, current logloss: 0.396195\n",
      "2016-03-17 20:18:14.833000\tencountered: 90000\t, y=0:p=0.231500, current logloss: 0.397400\n"
     ]
    }
   ],
   "source": [
    "# testing (build kaggle's submission file)\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(test)\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "\n",
    "with open(submission_dir + '\\submission1234.csv', 'w') as submission:\n",
    "    submission.write('Id,Predicted\\n')\n",
    "    #for t, row in enumerate(DictReader(open(test))):\n",
    "    for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):\n",
    "        #Id = row['Id']\n",
    "        #del row['Id']\n",
    "        y = 1. if row['Label'] == '1' else 0.\n",
    "        del row['Label']  # can't let the model peek the answer\n",
    "    \n",
    "        x = get_x(row, D)\n",
    "        p = get_p(x, w)\n",
    "\n",
    "        # for progress validation, useless for learning our model\n",
    "        loss += logloss(p, y)\n",
    "        if t % 10000 == 0 and t > 1:\n",
    "            print('%s\\tencountered: %d\\t, y=%d:p=%f, current logloss: %f' % (\n",
    "                datetime.now(), t, y, p, loss/t))\n",
    "\n",
    "        if t == 1000000:\n",
    "            break;\n",
    "\n",
    "        submission.write('%d,%f\\n' % (60000000+int(t), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's apply ADF implimented in v014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "                   Version 2, December 2004\n",
    "\n",
    "Copyright (C) 2004 Sam Hocevar <sam@hocevar.net>\n",
    "\n",
    "Everyone is permitted to copy and distribute verbatim or modified\n",
    "copies of this license document, and changing it is allowed as long\n",
    "as the name is changed.\n",
    "\n",
    "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n",
    "  TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n",
    "\n",
    " 0. You just DO WHAT THE FUCK YOU WANT TO.\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "\n",
    "# parameters #################################################################\n",
    "#train = 'train.csv'  # path to training file\n",
    "#test = 'test.csv'  # path to testing file\n",
    "#train = r'C:\\Users\\Bar\\Downloads\\dac.tar\\train.txt'\n",
    "#test = r'C:\\Users\\Bar\\Downloads\\dac.tar\\test.txt'\n",
    "\n",
    "train = r'D:\\9000_etc\\Thesis\\data\\dac_sample.tar\\dac_sample.txt'\n",
    "test = r'D:\\9000_etc\\Thesis\\data\\dac_sample.tar\\dac_sample.txt'\n",
    "\n",
    "submission_dir = r'D:\\9000_etc\\Thesis\\data\\submission'\n",
    "\n",
    "D = 2 ** 20   # number of weights use for learning\n",
    "alpha = .1    # learning rate for sgd optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def get_p(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function definitions #######################################################\n",
    "\n",
    "# A. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Apply hash trick of the original csv row\n",
    "# for simplicity, we treat both integer and categorical features as categorical\n",
    "# INPUT:\n",
    "#     csv_row: a csv dictionary, ex: {'Lable': '1', 'I1': '357', 'I2': '', ...}\n",
    "#     D: the max index that we can hash to\n",
    "# OUTPUT:\n",
    "#     x: a list of indices that its value is 1\n",
    "def get_x(csv_row, D):\n",
    "    x = [0]  # 0 is the index of the bias term\n",
    "    for key, value in csv_row.items():\n",
    "        if value is None:\n",
    "            index = int(key[1:], 16) % D  # weakest hash ever ;)\n",
    "        else:\n",
    "            index = int(value + key[1:], 16) % D  # weakest hash ever ;)\n",
    "        x.append(index)\n",
    "    return x  # x contains indices of features that have a value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function getting updated distribution of s_t\n",
    "# Input: \n",
    "# Output: \n",
    "def get_s_t_new(y, s_t_m_old, s_t_v_old):\n",
    "\n",
    "    wi = wwi / np.sqrt(np.pi)\n",
    "    xi = xxi * np.sqrt(2) * np.sqrt(s_t_v_old) + s_t_m_old\n",
    "    \n",
    "    fw = 0\n",
    "    if(y==1):\n",
    "        fw = (1 / (1 + np.exp(-xi))) * wi\n",
    "    elif(y==0):\n",
    "        fw = ((np.exp(-xi)) / (1 + np.exp(-xi))) * wi\n",
    "    else:\n",
    "        print(\"error, y has wroing value\")\n",
    "\n",
    "    z_t = sum(fw)\n",
    "    s_t_m_new = 1/z_t * sum(xi * fw)\n",
    "    s_t_v_new = 1/z_t * sum((xi**2) * fw) - s_t_m_new**2\n",
    "        \n",
    "    return (s_t_m_new, s_t_v_new)\n",
    "\n",
    "\n",
    "def get_a_i(x, theta_t_v):\n",
    "    #return theta_t_v[x] / sum(theta_t_v[x]**2)\n",
    "    return theta_t_v[x] / sum(theta_t_v[x])\n",
    "\n",
    "\n",
    "\n",
    "def update_theta(x, theta_t_m, theta_t_v, delta_m, delta_v, n_iter):\n",
    "    a_i = get_a_i(x, theta_t_v)\n",
    "    theta_t_m[x] += (a_i * delta_m)\n",
    "    theta_t_v[x] += ((a_i**2) * delta_v) + abs(theta_t_m[x])/min((n_iter+1), 3000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-11 23:42:18.553666\tencountered: 10000\t y=0: 0.034393, loss:0.106340\n",
      "2016-01-11 23:42:20.514057\tencountered: 20000\t y=0: 0.051208, loss:0.126842\n",
      "2016-01-11 23:42:22.488458\tencountered: 30000\t y=0: 0.045364, loss:0.136674\n",
      "2016-01-11 23:42:24.396814\tencountered: 40000\t y=0: 0.167238, loss:0.144148\n",
      "2016-01-11 23:42:26.285154\tencountered: 50000\t y=0: 0.001901, loss:0.150849\n",
      "2016-01-11 23:42:28.143455\tencountered: 60000\t y=0: 0.000222, loss:0.156406\n",
      "2016-01-11 23:42:30.033815\tencountered: 70000\t y=0: 0.061015, loss:0.161269\n",
      "2016-01-11 23:42:31.937166\tencountered: 80000\t y=0: 0.011866, loss:0.165534\n",
      "2016-01-11 23:42:33.788461\tencountered: 90000\t y=0: 0.045132, loss:0.169543\n",
      "2016-01-11 23:42:35.673799\tencountered: 100000\t y=0: 0.358235, loss:0.173871\n",
      "2016-01-11 23:42:37.548131\tencountered: 110000\t y=0: 0.000924, loss:0.177308\n",
      "2016-01-11 23:42:39.422479\tencountered: 120000\t y=0: 0.003005, loss:0.180809\n",
      "2016-01-11 23:42:41.328833\tencountered: 130000\t y=0: 0.087187, loss:0.184057\n",
      "2016-01-11 23:42:43.176144\tencountered: 140000\t y=0: 0.003118, loss:0.187317\n",
      "2016-01-11 23:42:45.073490\tencountered: 150000\t y=0: 0.147219, loss:0.191060\n",
      "2016-01-11 23:42:46.977842\tencountered: 160000\t y=0: 0.066095, loss:0.194363\n",
      "2016-01-11 23:42:48.916201\tencountered: 170000\t y=0: 0.018712, loss:0.197854\n",
      "2016-01-11 23:42:50.896625\tencountered: 180000\t y=1: 0.847828, loss:0.201563\n",
      "2016-01-11 23:42:52.794972\tencountered: 190000\t y=0: 0.238234, loss:0.205152\n",
      "2016-01-11 23:42:54.654272\tencountered: 200000\t y=1: 0.826525, loss:0.208278\n",
      "2016-01-11 23:42:56.532624\tencountered: 210000\t y=1: 0.836806, loss:0.211535\n",
      "2016-01-11 23:42:58.406955\tencountered: 220000\t y=1: 0.942361, loss:0.214907\n",
      "2016-01-11 23:43:00.284813\tencountered: 230000\t y=0: 0.476623, loss:0.218149\n",
      "2016-01-11 23:43:02.215183\tencountered: 240000\t y=0: 0.601201, loss:0.221201\n",
      "2016-01-11 23:43:04.073502\tencountered: 250000\t y=0: 0.009502, loss:0.224029\n",
      "2016-01-11 23:43:05.948834\tencountered: 260000\t y=1: 0.083261, loss:0.227039\n",
      "2016-01-11 23:43:07.822163\tencountered: 270000\t y=0: 0.057251, loss:0.229787\n",
      "2016-01-11 23:43:09.730518\tencountered: 280000\t y=0: 0.034237, loss:0.232170\n",
      "2016-01-11 23:43:11.650881\tencountered: 290000\t y=0: 0.056056, loss:0.234489\n",
      "2016-01-11 23:43:13.612254\tencountered: 300000\t y=1: 0.612862, loss:0.236569\n",
      "2016-01-11 23:43:15.532636\tencountered: 310000\t y=0: 0.044829, loss:0.238709\n",
      "2016-01-11 23:43:17.589280\tencountered: 320000\t y=0: 0.100862, loss:0.241042\n",
      "2016-01-11 23:43:19.614735\tencountered: 330000\t y=1: 0.974857, loss:0.243183\n",
      "2016-01-11 23:43:22.314520\tencountered: 340000\t y=0: 0.207571, loss:0.245338\n",
      "2016-01-11 23:43:25.758954\tencountered: 350000\t y=0: 0.005126, loss:0.247303\n",
      "2016-01-11 23:43:29.339914\tencountered: 360000\t y=0: 0.007370, loss:0.249300\n",
      "2016-01-11 23:43:31.713599\tencountered: 370000\t y=0: 0.073279, loss:0.251177\n",
      "2016-01-11 23:43:34.255384\tencountered: 380000\t y=1: 0.654813, loss:0.252993\n",
      "2016-01-11 23:43:36.762183\tencountered: 390000\t y=0: 0.164997, loss:0.254663\n",
      "2016-01-11 23:43:39.317978\tencountered: 400000\t y=1: 0.809972, loss:0.256025\n",
      "2016-01-11 23:43:41.683675\tencountered: 410000\t y=0: 0.056431, loss:0.257528\n",
      "2016-01-11 23:43:44.247477\tencountered: 420000\t y=0: 0.008851, loss:0.259165\n",
      "2016-01-11 23:43:46.742966\tencountered: 430000\t y=0: 0.018139, loss:0.260593\n",
      "2016-01-11 23:43:49.351814\tencountered: 440000\t y=1: 0.617667, loss:0.262059\n",
      "2016-01-11 23:43:51.692479\tencountered: 450000\t y=0: 0.155714, loss:0.263357\n",
      "2016-01-11 23:43:54.738652\tencountered: 460000\t y=1: 0.470169, loss:0.264827\n",
      "2016-01-11 23:43:57.528651\tencountered: 470000\t y=0: 0.073505, loss:0.266274\n",
      "2016-01-11 23:44:00.750601\tencountered: 480000\t y=0: 0.141987, loss:0.267582\n",
      "2016-01-11 23:44:05.283900\tencountered: 490000\t y=0: 0.424297, loss:0.268698\n",
      "2016-01-11 23:44:08.214330\tencountered: 500000\t y=0: 0.281493, loss:0.269952\n",
      "2016-01-11 23:44:10.696965\tencountered: 510000\t y=0: 0.028503, loss:0.271049\n",
      "2016-01-11 23:44:13.037626\tencountered: 520000\t y=1: 0.085440, loss:0.272121\n",
      "2016-01-11 23:44:15.502376\tencountered: 530000\t y=1: 0.247014, loss:0.273304\n",
      "2016-01-11 23:44:18.772647\tencountered: 540000\t y=0: 0.046791, loss:0.274388\n",
      "2016-01-11 23:44:21.801660\tencountered: 550000\t y=1: 0.703948, loss:0.275361\n",
      "2016-01-11 23:44:24.053259\tencountered: 560000\t y=0: 0.013326, loss:0.276346\n",
      "2016-01-11 23:44:26.567062\tencountered: 570000\t y=0: 0.007375, loss:0.277288\n",
      "2016-01-11 23:44:28.938727\tencountered: 580000\t y=1: 0.496587, loss:0.278285\n",
      "2016-01-11 23:44:32.073952\tencountered: 590000\t y=0: 0.182513, loss:0.279162\n",
      "2016-01-11 23:44:35.102102\tencountered: 600000\t y=0: 0.595350, loss:0.279961\n",
      "2016-01-11 23:44:38.292077\tencountered: 610000\t y=0: 0.524803, loss:0.280814\n",
      "2016-01-11 23:44:41.733595\tencountered: 620000\t y=0: 0.024835, loss:0.281598\n",
      "2016-01-11 23:44:44.153330\tencountered: 630000\t y=0: 0.164831, loss:0.282577\n",
      "2016-01-11 23:44:46.314847\tencountered: 640000\t y=0: 0.010481, loss:0.283502\n",
      "2016-01-11 23:44:48.617499\tencountered: 650000\t y=0: 0.263801, loss:0.284389\n",
      "2016-01-11 23:44:50.894536\tencountered: 660000\t y=0: 0.083498, loss:0.285214\n",
      "2016-01-11 23:44:53.081083\tencountered: 670000\t y=0: 0.029526, loss:0.285994\n",
      "2016-01-11 23:44:56.446072\tencountered: 680000\t y=0: 0.063255, loss:0.286658\n",
      "2016-01-11 23:44:58.904836\tencountered: 690000\t y=1: 0.591343, loss:0.287290\n",
      "2016-01-11 23:45:00.998348\tencountered: 700000\t y=0: 0.262894, loss:0.288008\n",
      "2016-01-11 23:45:04.223456\tencountered: 710000\t y=1: 0.735010, loss:0.288567\n",
      "2016-01-11 23:45:07.347325\tencountered: 720000\t y=0: 0.011245, loss:0.289317\n",
      "2016-01-11 23:45:09.450817\tencountered: 730000\t y=1: 0.361290, loss:0.289928\n",
      "2016-01-11 23:45:11.518285\tencountered: 740000\t y=1: 0.550446, loss:0.290586\n",
      "2016-01-11 23:45:13.609766\tencountered: 750000\t y=0: 0.026703, loss:0.291384\n",
      "2016-01-11 23:45:15.683409\tencountered: 760000\t y=0: 0.363334, loss:0.291996\n",
      "2016-01-11 23:45:17.724860\tencountered: 770000\t y=0: 0.057684, loss:0.292602\n",
      "2016-01-11 23:45:19.732283\tencountered: 780000\t y=1: 0.468320, loss:0.293188\n",
      "2016-01-11 23:45:21.814764\tencountered: 790000\t y=0: 0.027712, loss:0.293777\n",
      "2016-01-11 23:45:23.891235\tencountered: 800000\t y=0: 0.033944, loss:0.294408\n",
      "2016-01-11 23:45:25.950717\tencountered: 810000\t y=0: 0.030544, loss:0.295091\n",
      "2016-01-11 23:45:28.048205\tencountered: 820000\t y=1: 0.697802, loss:0.295617\n",
      "2016-01-11 23:45:30.058613\tencountered: 830000\t y=0: 0.071509, loss:0.296218\n",
      "2016-01-11 23:45:32.101063\tencountered: 840000\t y=0: 0.036822, loss:0.296786\n",
      "2016-01-11 23:45:34.154539\tencountered: 850000\t y=0: 0.091659, loss:0.297280\n",
      "2016-01-11 23:45:36.211981\tencountered: 860000\t y=0: 0.044132, loss:0.297742\n",
      "2016-01-11 23:45:38.245443\tencountered: 870000\t y=1: 0.634826, loss:0.298191\n",
      "2016-01-11 23:45:40.278867\tencountered: 880000\t y=1: 0.500001, loss:0.298731\n",
      "2016-01-11 23:45:42.304306\tencountered: 890000\t y=0: 0.011428, loss:0.299195\n",
      "2016-01-11 23:45:44.360799\tencountered: 900000\t y=1: 0.193006, loss:0.299675\n",
      "2016-01-11 23:45:46.433254\tencountered: 910000\t y=1: 0.806553, loss:0.300169\n",
      "2016-01-11 23:45:48.459693\tencountered: 920000\t y=1: 0.913689, loss:0.300609\n",
      "2016-01-11 23:45:50.498123\tencountered: 930000\t y=0: 0.085607, loss:0.301073\n",
      "2016-01-11 23:45:52.519576\tencountered: 940000\t y=0: 0.182508, loss:0.301516\n",
      "2016-01-11 23:45:54.550998\tencountered: 950000\t y=0: 0.125932, loss:0.301999\n",
      "2016-01-11 23:45:56.645504\tencountered: 960000\t y=0: 0.044899, loss:0.302377\n",
      "2016-01-11 23:45:58.641903\tencountered: 970000\t y=0: 0.738080, loss:0.302770\n",
      "2016-01-11 23:46:00.684924\tencountered: 980000\t y=1: 0.157784, loss:0.303246\n",
      "2016-01-11 23:46:02.724372\tencountered: 990000\t y=0: 0.190365, loss:0.303631\n",
      "2016-01-11 23:46:05.091034\tencountered: 1000000\t y=0: 0.324860, loss:0.304087\n"
     ]
    }
   ],
   "source": [
    "# code for ADF\n",
    "\n",
    "n = 10\n",
    "xxi, wwi = np.polynomial.hermite.hermgauss(n)\n",
    "\n",
    "theta_t_m = np.array([0.] * D) # mean of thetas at t\n",
    "theta_t_v = np.array([.5] * D) # variance of thetas at t\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "f = open(train)\n",
    "fn = ['Label'] + [ 'I' + str(i) for i in list(range(1,14))] + [ 'C' + str(i) for i in list(range(1,27))]\n",
    "for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):   \n",
    "#for t, row in enumerate(DictReader(open(train))):\n",
    "\n",
    "    y = 1. if row['Label'] == '1' else 0.\n",
    "\n",
    "    del row['Label']  # can't let the model peek the answer\n",
    "    #del row['Id']  # we don't need the Id\n",
    "    \n",
    "    x = get_x(row, D)\n",
    "\n",
    "    s_t_m_old = sum(theta_t_m[x])\n",
    "    s_t_v_old = sum(theta_t_v[x])\n",
    "    s_t_m, s_t_v = get_s_t_new(y, s_t_m_old, s_t_v_old)\n",
    "    delta_m = s_t_m - s_t_m_old\n",
    "    delta_v = s_t_v - s_t_v_old\n",
    "    update_theta(x, theta_t_m, theta_t_v, delta_m, delta_v, t)\n",
    "    \n",
    "    p = get_p(x, theta_t_m)\n",
    "    loss += logloss(p, y)\n",
    "    \n",
    "    if t % 10000 == 0 and t > 1:\n",
    "        print('%s\\tencountered: %d\\t y=%d: %f, loss:%f' % (\n",
    "            datetime.now(), t, y, p, loss/t))\n",
    "    \n",
    "    if t == 1000000:\n",
    "        break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-11 23:38:00.285573\tencountered: 10000\t, y=0:p=0.548765, current logloss: 0.890701\n",
      "2016-01-11 23:38:01.009085\tencountered: 20000\t, y=0:p=0.106494, current logloss: 0.896731\n",
      "2016-01-11 23:38:01.816677\tencountered: 30000\t, y=0:p=0.088974, current logloss: 0.891147\n",
      "2016-01-11 23:38:02.590208\tencountered: 40000\t, y=0:p=0.237172, current logloss: 0.889546\n",
      "2016-01-11 23:38:03.325729\tencountered: 50000\t, y=0:p=0.547115, current logloss: 0.888044\n",
      "2016-01-11 23:38:04.039254\tencountered: 60000\t, y=0:p=0.017295, current logloss: 0.888149\n",
      "2016-01-11 23:38:04.843809\tencountered: 70000\t, y=0:p=0.149270, current logloss: 0.890467\n",
      "2016-01-11 23:38:05.720429\tencountered: 80000\t, y=0:p=0.940053, current logloss: 0.894560\n",
      "2016-01-11 23:38:06.588047\tencountered: 90000\t, y=1:p=0.111278, current logloss: 0.894890\n",
      "2016-01-11 23:38:07.516704\tencountered: 100000\t, y=0:p=0.034312, current logloss: 0.897533\n",
      "2016-01-11 23:38:08.512412\tencountered: 110000\t, y=0:p=0.395873, current logloss: 0.901250\n",
      "2016-01-11 23:38:09.482101\tencountered: 120000\t, y=0:p=0.651399, current logloss: 0.905195\n",
      "2016-01-11 23:38:10.575876\tencountered: 130000\t, y=0:p=0.088615, current logloss: 0.912059\n",
      "2016-01-11 23:38:11.633627\tencountered: 140000\t, y=0:p=0.130226, current logloss: 0.916545\n",
      "2016-01-11 23:38:12.630334\tencountered: 150000\t, y=1:p=0.037903, current logloss: 0.919776\n",
      "2016-01-11 23:38:13.672074\tencountered: 160000\t, y=0:p=0.027925, current logloss: 0.920963\n",
      "2016-01-11 23:38:14.646766\tencountered: 170000\t, y=0:p=0.445543, current logloss: 0.923004\n",
      "2016-01-11 23:38:15.647476\tencountered: 180000\t, y=0:p=0.580597, current logloss: 0.923343\n",
      "2016-01-11 23:38:16.638179\tencountered: 190000\t, y=0:p=0.535335, current logloss: 0.923296\n",
      "2016-01-11 23:38:17.604865\tencountered: 200000\t, y=0:p=0.566244, current logloss: 0.923415\n",
      "2016-01-11 23:38:18.633596\tencountered: 210000\t, y=0:p=0.296934, current logloss: 0.923411\n",
      "2016-01-11 23:38:19.670342\tencountered: 220000\t, y=0:p=0.602458, current logloss: 0.923047\n",
      "2016-01-11 23:38:20.840213\tencountered: 230000\t, y=0:p=0.171643, current logloss: 0.922485\n",
      "2016-01-11 23:38:21.928985\tencountered: 240000\t, y=1:p=0.953779, current logloss: 0.921632\n",
      "2016-01-11 23:38:22.951713\tencountered: 250000\t, y=1:p=0.099858, current logloss: 0.921075\n",
      "2016-01-11 23:38:23.954423\tencountered: 260000\t, y=0:p=0.672452, current logloss: 0.920580\n",
      "2016-01-11 23:38:25.013175\tencountered: 270000\t, y=0:p=0.936026, current logloss: 0.919863\n",
      "2016-01-11 23:38:26.028898\tencountered: 280000\t, y=0:p=0.082059, current logloss: 0.919240\n",
      "2016-01-11 23:38:27.081662\tencountered: 290000\t, y=0:p=0.823008, current logloss: 0.918313\n",
      "2016-01-11 23:38:28.113375\tencountered: 300000\t, y=0:p=0.015940, current logloss: 0.917412\n",
      "2016-01-11 23:38:29.265194\tencountered: 310000\t, y=0:p=0.300276, current logloss: 0.916908\n",
      "2016-01-11 23:38:30.418011\tencountered: 320000\t, y=0:p=0.780463, current logloss: 0.915968\n",
      "2016-01-11 23:38:31.449743\tencountered: 330000\t, y=0:p=0.336526, current logloss: 0.915395\n",
      "2016-01-11 23:38:32.556531\tencountered: 340000\t, y=1:p=0.497086, current logloss: 0.915173\n",
      "2016-01-11 23:38:33.577253\tencountered: 350000\t, y=0:p=0.058958, current logloss: 0.914277\n",
      "2016-01-11 23:38:34.601983\tencountered: 360000\t, y=0:p=0.438828, current logloss: 0.913720\n",
      "2016-01-11 23:38:35.684750\tencountered: 370000\t, y=0:p=0.304110, current logloss: 0.913181\n",
      "2016-01-11 23:38:36.698469\tencountered: 380000\t, y=0:p=0.046386, current logloss: 0.912666\n",
      "2016-01-11 23:38:37.683168\tencountered: 390000\t, y=0:p=0.890216, current logloss: 0.911653\n",
      "2016-01-11 23:38:38.802962\tencountered: 400000\t, y=0:p=0.470257, current logloss: 0.910758\n",
      "2016-01-11 23:38:40.043844\tencountered: 410000\t, y=0:p=0.209143, current logloss: 0.910276\n",
      "2016-01-11 23:38:41.286726\tencountered: 420000\t, y=0:p=0.237747, current logloss: 0.909663\n",
      "2016-01-11 23:38:42.385525\tencountered: 430000\t, y=1:p=0.063794, current logloss: 0.908900\n",
      "2016-01-11 23:38:43.722455\tencountered: 440000\t, y=0:p=0.850802, current logloss: 0.908713\n",
      "2016-01-11 23:38:44.896288\tencountered: 450000\t, y=0:p=0.541100, current logloss: 0.908142\n",
      "2016-01-11 23:38:45.975054\tencountered: 460000\t, y=0:p=0.023612, current logloss: 0.907416\n",
      "2016-01-11 23:38:47.044814\tencountered: 470000\t, y=0:p=0.949174, current logloss: 0.907283\n",
      "2016-01-11 23:38:48.205639\tencountered: 480000\t, y=0:p=0.683449, current logloss: 0.906302\n",
      "2016-01-11 23:38:49.363459\tencountered: 490000\t, y=0:p=0.615073, current logloss: 0.905851\n",
      "2016-01-11 23:38:51.019635\tencountered: 500000\t, y=0:p=0.960030, current logloss: 0.905354\n",
      "2016-01-11 23:38:52.157443\tencountered: 510000\t, y=1:p=0.851022, current logloss: 0.904565\n",
      "2016-01-11 23:38:53.229203\tencountered: 520000\t, y=0:p=0.786105, current logloss: 0.904343\n",
      "2016-01-11 23:38:54.272946\tencountered: 530000\t, y=0:p=0.337536, current logloss: 0.904165\n",
      "2016-01-11 23:38:55.352710\tencountered: 540000\t, y=0:p=0.218885, current logloss: 0.903853\n",
      "2016-01-11 23:38:56.404457\tencountered: 550000\t, y=0:p=0.325938, current logloss: 0.903645\n",
      "2016-01-11 23:38:57.444195\tencountered: 560000\t, y=1:p=0.721124, current logloss: 0.903488\n",
      "2016-01-11 23:38:58.478929\tencountered: 570000\t, y=0:p=0.715020, current logloss: 0.903099\n",
      "2016-01-11 23:38:59.622741\tencountered: 580000\t, y=0:p=0.028624, current logloss: 0.902526\n",
      "2016-01-11 23:39:00.696510\tencountered: 590000\t, y=0:p=0.511535, current logloss: 0.902032\n",
      "2016-01-11 23:39:01.740249\tencountered: 600000\t, y=0:p=0.389730, current logloss: 0.901729\n",
      "2016-01-11 23:39:02.835026\tencountered: 610000\t, y=0:p=0.441914, current logloss: 0.901524\n",
      "2016-01-11 23:39:03.946815\tencountered: 620000\t, y=0:p=0.664033, current logloss: 0.901181\n",
      "2016-01-11 23:39:05.014573\tencountered: 630000\t, y=0:p=0.345842, current logloss: 0.901066\n",
      "2016-01-11 23:39:06.337512\tencountered: 640000\t, y=1:p=0.367521, current logloss: 0.900889\n",
      "2016-01-11 23:39:07.393261\tencountered: 650000\t, y=0:p=0.215024, current logloss: 0.900473\n",
      "2016-01-11 23:39:08.415989\tencountered: 660000\t, y=0:p=0.913423, current logloss: 0.900332\n",
      "2016-01-11 23:39:09.468734\tencountered: 670000\t, y=0:p=0.302397, current logloss: 0.900038\n",
      "2016-01-11 23:39:10.556507\tencountered: 680000\t, y=0:p=0.102527, current logloss: 0.899747\n",
      "2016-01-11 23:39:11.588239\tencountered: 690000\t, y=0:p=0.103011, current logloss: 0.899383\n",
      "2016-01-11 23:39:12.649108\tencountered: 700000\t, y=0:p=0.727230, current logloss: 0.899057\n",
      "2016-01-11 23:39:13.799910\tencountered: 710000\t, y=0:p=0.492512, current logloss: 0.898955\n",
      "2016-01-11 23:39:15.070813\tencountered: 720000\t, y=0:p=0.502471, current logloss: 0.898563\n",
      "2016-01-11 23:39:16.102545\tencountered: 730000\t, y=0:p=0.053819, current logloss: 0.898308\n",
      "2016-01-11 23:39:17.127273\tencountered: 740000\t, y=0:p=0.200687, current logloss: 0.898128\n",
      "2016-01-11 23:39:18.164009\tencountered: 750000\t, y=0:p=0.951176, current logloss: 0.897833\n",
      "2016-01-11 23:39:19.223762\tencountered: 760000\t, y=0:p=0.612964, current logloss: 0.897498\n",
      "2016-01-11 23:39:20.297744\tencountered: 770000\t, y=0:p=0.568754, current logloss: 0.897095\n",
      "2016-01-11 23:39:21.388518\tencountered: 780000\t, y=0:p=0.584517, current logloss: 0.896961\n",
      "2016-01-11 23:39:22.469285\tencountered: 790000\t, y=0:p=0.072878, current logloss: 0.896633\n",
      "2016-01-11 23:39:23.807235\tencountered: 800000\t, y=0:p=0.259906, current logloss: 0.896461\n",
      "2016-01-11 23:39:25.026101\tencountered: 810000\t, y=0:p=0.092712, current logloss: 0.896163\n",
      "2016-01-11 23:39:26.043823\tencountered: 820000\t, y=0:p=0.056267, current logloss: 0.895869\n",
      "2016-01-11 23:39:27.094569\tencountered: 830000\t, y=0:p=0.713581, current logloss: 0.895677\n",
      "2016-01-11 23:39:28.126301\tencountered: 840000\t, y=0:p=0.146991, current logloss: 0.895400\n",
      "2016-01-11 23:39:29.136018\tencountered: 850000\t, y=0:p=0.087018, current logloss: 0.895338\n",
      "2016-01-11 23:39:30.216785\tencountered: 860000\t, y=0:p=0.591570, current logloss: 0.895179\n",
      "2016-01-11 23:39:31.248518\tencountered: 870000\t, y=0:p=0.630868, current logloss: 0.894915\n",
      "2016-01-11 23:39:32.334288\tencountered: 880000\t, y=0:p=0.032048, current logloss: 0.894638\n",
      "2016-01-11 23:39:33.463091\tencountered: 890000\t, y=0:p=0.005298, current logloss: 0.894440\n",
      "2016-01-11 23:39:34.542857\tencountered: 900000\t, y=0:p=0.381270, current logloss: 0.894098\n",
      "2016-01-11 23:39:35.697675\tencountered: 910000\t, y=0:p=0.042579, current logloss: 0.893994\n",
      "2016-01-11 23:39:36.783446\tencountered: 920000\t, y=0:p=0.575255, current logloss: 0.893686\n",
      "2016-01-11 23:39:37.841197\tencountered: 930000\t, y=0:p=0.893066, current logloss: 0.893433\n",
      "2016-01-11 23:39:38.922470\tencountered: 940000\t, y=0:p=0.573205, current logloss: 0.893340\n",
      "2016-01-11 23:39:39.972215\tencountered: 950000\t, y=0:p=0.594870, current logloss: 0.893107\n",
      "2016-01-11 23:39:40.999944\tencountered: 960000\t, y=1:p=0.845691, current logloss: 0.893003\n",
      "2016-01-11 23:39:42.017668\tencountered: 970000\t, y=0:p=0.031372, current logloss: 0.892960\n",
      "2016-01-11 23:39:43.078438\tencountered: 980000\t, y=0:p=0.043447, current logloss: 0.892822\n",
      "2016-01-11 23:39:44.102147\tencountered: 990000\t, y=1:p=0.179127, current logloss: 0.892685\n",
      "2016-01-11 23:39:45.241956\tencountered: 1000000\t, y=0:p=0.928386, current logloss: 0.892592\n"
     ]
    }
   ],
   "source": [
    "# testing (build kaggle's submission file)\n",
    "#f = open('../thesis/data/dac_sample.txt')\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "\n",
    "f = open(test)\n",
    "with open(submission_dir + '\\submission_adf_20151221_test01.csv', 'w') as submission:\n",
    "    #submission.write('Predicted\\n')\n",
    "    submission.write('Id,Predicted\\n')\n",
    "    #for t, row in enumerate(DictReader(open(test))):\n",
    "    for t, row in enumerate(DictReader(f, fieldnames=fn, delimiter='\\t')):\n",
    "        #Id = row['Id']\n",
    "        #del row['Id']\n",
    "        y = 1. if row['Label'] == '1' else 0.\n",
    "        del row['Label']  # can't let the model peek the answer\n",
    "        x = get_x(row, D)\n",
    "        p = get_p(x, theta_t_m)\n",
    "        #submission.write('%f\\n' % (p))\n",
    "        \n",
    "        loss += logloss(p, y)\n",
    "            \n",
    "        if t % 10000 == 0 and t > 1:\n",
    "            print('%s\\tencountered: %d\\t, y=%d:p=%f, current logloss: %f' % (\n",
    "                datetime.now(), t, y, p, loss/t))\n",
    "\n",
    "        if t == 1000000:\n",
    "            break;\n",
    "\n",
    "            \n",
    "        submission.write('%d,%f\\n' % (60000000+int(t), p))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
