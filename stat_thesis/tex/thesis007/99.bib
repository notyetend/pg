@book{Agresti1996,
author = {Agresti, Alan},
edition = {2nd},
publisher = {Wiley},
title = {{An introduction to categorical data analysis}},
year = {1996}
}
@article{Bottou1998,
abstract = {An abstract is not available.},
author = {Bottou, L{\'{e}}on},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Bottou - 1998 - Online learning and stochastic approximations.pdf:pdf},
isbn = {978-0521117913},
journal = {On-line learning in neural networks},
pages = {1--34},
title = {{Online learning and stochastic approximations}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=iu2v6C5nx4oC{\&}oi=fnd{\&}pg=PA9{\&}dq=Online+Learning+and+Stochastic+Approximations{\&}ots=oyFbNGmOOc{\&}sig=dMFIVHPeRai645DxGXT{\_}WIyydoA},
year = {1998}
}
@article{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, L{\'{e}}on},
doi = {10.1007/978-3-642-35289-8{\_}25},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {2045-2322},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {421--436},
pmid = {25382349},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
volume = {1},
year = {2012}
}
@article{Bottou2010,
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
author = {Bottou, L{\'{e}}on},
doi = {10.1007/978-3-7908-2604-3{\_}16},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Bottou - 2010 - Large-Scale Machine Learning with Stochastic Gradient Descent.pdf:pdf},
isbn = {0269-2155},
issn = {0269-2155},
journal = {Proceedings of COMPSTAT'2010},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
pmid = {20876631},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
year = {2010}
}
@article{Box2003,
author = {Box, P O and Hut, Fin- and Honkela, Antti and Valpola, Harri},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Box et al. - 2003 - ON-LINE VARIATIONAL BAYESIAN LEARNING Antti Honkela and Harri Valpola Helsinki University of Technology , Neural Net.pdf:pdf},
number = {April},
pages = {803--808},
title = {{ON-LINE VARIATIONAL BAYESIAN LEARNING Antti Honkela and Harri Valpola Helsinki University of Technology , Neural Networks Research Centre}},
year = {2003}
}
@article{Chan2013,
author = {Chan, Hyo and Lee, Hangsuck},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Chan, Lee - 2013 - Estimation of the Expected Loss per Exposure of Export Insurance using GLM.pdf:pdf},
keywords = {export insurance,generalized linear models,glm,loss frequency,loss severity},
pages = {857--871},
title = {{Estimation of the Expected Loss per Exposure of Export Insurance using GLM}},
volume = {26},
year = {2013}
}
@techreport{Chapelle2013,
abstract = {Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this paper a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: it is easy to implement and deploy; it is highly scalable (we have trained it on terabytes of data); and it provides models with state-of-the-art accuracy.},
archivePrefix = {arXiv},
arxivId = {1005.3014},
author = {Chapelle, Olivier and Manavoglu, Eren and Rosales, Romer},
booktitle = {people.csail.mit.edu},
doi = {10.1145/0000000.0000000},
eprint = {1005.3014},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley//Chapelle, Manavoglu, Rosales - 2013 - Simple and scalable response prediction for display advertising.pdf:pdf},
isbn = {9781627480031},
issn = {15564681},
number = {212},
pages = {1--34},
title = {{Simple and scalable response prediction for display advertising}},
url = {http://people.csail.mit.edu/romer/papers/TISTRespPredAds.pdf},
volume = {V},
year = {2013}
}
@article{Crammer2006,
abstract = {We present a unified view for {\{}$\backslash$em online{\}} classification, regression, and uniclass problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss.},
author = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
doi = {10.1.1.9.3429},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Crammer et al. - 2006 - Online Passive-Aggressive Algorithms(2).pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {551--585},
title = {{Online Passive-Aggressive Algorithms}},
url = {http://eprints.pascal-network.org/archive/00000052/},
volume = {7},
year = {2006}
}
@article{Ghahramani2000,
author = {Ghahramani, Zoubin and Computational, Gatsby and Unit, Neuroscience},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Ghahramani, Computational, Unit - 2000 - Online Variational Bayesian Learning.pdf:pdf},
journal = {Neuroscience},
number = {December},
title = {{Online Variational Bayesian Learning}},
year = {2000}
}
@article{He2014,
abstract = {Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central tomost on- line advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3{\%}, an improvement with sig- nificant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surpris- ingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic re- gression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with. 1.},
author = {He, Xinran and Bowers, Stuart and Candela, Joaquin Qui{\~{n}}onero and Pan, Junfeng and Jin, Ou and Xu, Tianbing and Liu, Bo and Xu, Tao and Shi, Yanxin and Atallah, Antoine and Herbrich, Ralf},
doi = {10.1145/2648584.2648589},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/He et al. - 2014 - Practical Lessons from Predicting Clicks on Ads at Facebook.pdf:pdf},
isbn = {9781450329996},
journal = {Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining - ADKDD'14},
pages = {1--9},
title = {{Practical Lessons from Predicting Clicks on Ads at Facebook}},
url = {http://dl.acm.org/citation.cfm?doid=2648584.2648589},
year = {2014}
}
@article{Herbrich2006,
abstract = {We present a new Bayesian skill rating system which can be viewed as a generalisation of the Elo system used in Chess. The new system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. Inference is performed by approximate message passing on a factor graph representation of the model. We present experimental evidence on the increased accuracy and convergence speed of the system compared to Elo and report on our experience with the new rating system running in a large-scale commercial online gaming service under the name of TrueSkill.},
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
doi = {10.2134/jeq2007.0177},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.pdf:pdf},
isbn = {1049-5258},
issn = {00472425},
journal = {Advances in Neural Information Processing Systems},
keywords = {bayesian learning,dynamic difficulty adjustment,match-making},
pages = {569--576},
pmid = {18268290},
title = {{TrueSkill: A Bayesian Skill Rating System}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=67956},
year = {2006}
}
@article{Hoffman2010,
abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time. 1},
author = {Hoffman, MD and Blei, DM and Bach, Francis},
doi = {10.1.1.187.1883},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Hoffman, Blei, Bach - 2010 - Online learning for latent dirichlet allocation.pdf:pdf},
isbn = {9781450300551},
issn = {08912017},
journal = {Nips},
pages = {1--9},
title = {{Online learning for latent dirichlet allocation}},
url = {http://videolectures.net/site/normal{\_}dl/tag=83534/nips2010{\_}1291.pdf},
year = {2010}
}
@article{Julier1997,
abstract = {The Kalman Filter(KF) is one of the most widely used methods for tracking$\backslash$nand estimation due to its simplicity, optimality, tractability and$\backslash$nrobustness. However, the application of the KF to nonlinear systems$\backslash$ncan be diffcult. The most common approach is to use the Extended$\backslash$nKalman Filter (EKF) which simply linearises all nonlinear models$\backslash$nso that the traditional linear Kalman Filter can be applied. Although$\backslash$nthe EKF (in its many forms) is a widely used filtering strategy,$\backslash$nover thirty years of experience with it has led to a general consensus$\backslash$nwithin the tracking and control community that it is diffcult to$\backslash$nimplement, diffcult to tune, and only reliable for systems which$\backslash$nare almost linear on the time scale of the update intervals. In this$\backslash$npaper a new linear estimator is developed and demonstrated. Using$\backslash$nthe principle that a set of discretely sampled points can be used$\backslash$nto parameterise mean and covariance, the estimator yields performance$\backslash$nequivalent to the KF for linear systems yet generalises elegantly$\backslash$nto nonlinear systems without the linearisation steps required by$\backslash$nthe EKF. We show analytically that the expected performance of the$\backslash$nnew approach is superior to that of the EKF and, in fact, is directly$\backslash$ncomparable to that of the second order Gauss Filter. The method is$\backslash$nnot restricted to assuming that the distributions of noise sources$\backslash$nare Gaussian. We argue that the ease of implementation and more accurate$\backslash$nestimation features of the new lter recommend its use over the EKF$\backslash$nin virtually all applications.},
author = {Julier, Simon J and Uhlmann, Jeffrey K},
doi = {10.1117/12.280797},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Julier, Uhlmann - 1997 - A new extension of the Kalman filter to nonlinear systems.pdf:pdf},
issn = {0277786X},
journal = {Int Symp AerospaceDefense Sensing Simul and Controls},
keywords = {Kalman filter,estimation,kalman filtering,navigation,non-linear systems,sampling},
mendeley-tags = {Kalman filter},
pages = {182--193},
title = {{A new extension of the Kalman filter to nonlinear systems}},
url = {http://link.aip.org/link/?PSI/3068/182/1{\&}Agg=doi},
volume = {3},
year = {1997}
}
@article{Kalman1960,
abstract = {The classical filtering and prediction problem is re-examined using the Bode- Shannon representation of random processes and the state transition method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modifica- tion to stationary and nonstationary statistics and to growing-memory and infinite- memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co- efficients of the difference (or differential) equation of the optimal linear filter are ob- tained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
author = {Kalman, R.E.},
doi = {10.1115/1.3662552},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Kalman - 1960 - A new approach to linear filtering and prediction problems.pdf:pdf},
isbn = {9783540769897},
issn = {00219223},
journal = {Journal of basic Engineering},
keywords = {kalman filter,lineal filter,prediction,predictor},
number = {1},
pages = {35--45},
pmid = {5311910},
title = {{A new approach to linear filtering and prediction problems}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+New+Approach+to+Linear+Filtering+and+Prediction+Problems{\#}0},
volume = {82},
year = {1960}
}
@article{Langford2009,
abstract = {We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular {\$}L{\_}1{\$}-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.},
archivePrefix = {arXiv},
arxivId = {0806.4686},
author = {Langford, John and Li, Lihong and Zhang, Tong},
eprint = {0806.4686},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Langford, Li, Zhang - 2009 - Sparse Online Learning via Truncated Gradient.pdf:pdf},
isbn = {9781605609492},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {online learning,regulariza-,sparsity,stochastic gradient descent,truncated gradient},
pages = {777--801},
title = {{Sparse Online Learning via Truncated Gradient}},
url = {http://arxiv.org/abs/0806.4686},
volume = {10},
year = {2009}
}
@misc{Lauritzen1992,
abstract = {A scheme is presented for modeling and local computation of exact probabilities, means, and variances for mixed qualitative and quantitative variables. The models assume that the conditional distribution of the quantitative variables, given the qualitative, is multivariate Gaussian. The computational architecture is set up by forming a tree of belief universes, and the calculations are then performed by local message passing between universes. The asymmetry between the quantitative and qualitative variables sets some additional limitations for the specification and propagation structure. Approximate methods when these are not appropriately fulfilled are sketched. It has earlier been shown how to exploit the local structure in the specification of a discrete probability model for fast and efficient computation, thereby paving the way for exploiting probability-based models as parts of realistic systems for planning and decision support. The purpose of this article is to extend this computational scheme to networks, where some vertices represent entities that are measured on a quantitative and some on a qualitative scale. An extension has the advantage of unifying several known techniques, but allows more flexible and faithful modeling and speeds computation as well. To handle this more general case, the properties of (CG) conditional Gaussian distributions are exploited. A fictitious but simple example is used for illustration throughout the paper, concerned with monitoring emissions from a waste incinerator. From optical measurements of the darkness of the smoke, the concentration of CO2—which are both on a continuous scale—and possible knowledge about qualitative characteristics such as the type of waste burned, one wants to infer about the state of the incinerator and the current emission of heavy metals.},
author = {Lauritzen, Steffen L. (University of Aalborg)},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1992.10476265},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Lauritzen - 1992 - Propagation of Probabilities, Means and Variances in Mixed Graphical Association Models.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
number = {420},
pages = {1098--1108},
title = {{Propagation of Probabilities, Means and Variances in Mixed Graphical Association Models}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1992.10476265},
volume = {87},
year = {1992}
}
@article{Lin2013,
abstract = {Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm – random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency – orders of magnitude speed-up compared to the state-of-the-art.},
author = {Lin, Dahua},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Lin - 2013 - Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation.pdf:pdf},
issn = {10495258},
journal = {Advances in Nueral Information Processing Systems 26 (Proceedings of NIPS)},
pages = {1--9},
title = {{Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation}},
year = {2013}
}
@article{Luttinen2014,
abstract = {BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.},
archivePrefix = {arXiv},
arxivId = {1410.0870},
author = {Luttinen, Jaakko},
eprint = {1410.0870},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Luttinen - 2014 - BayesPy Variational Bayesian Inference in Python.pdf:pdf},
journal = {arXiv preprint arXiv:1410.0870},
keywords = {probabilistic pro-,python,variational bayes,variational message passing},
month = {oct},
title = {{BayesPy: Variational Bayesian Inference in Python}},
url = {http://arxiv.org/abs/1410.0870},
year = {2014}
}
@article{McMahan2013,
abstract = {Predicting ad click--through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v2},
author = {McMahan, H Brendan and Holt, Gary and Sculley, D and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
doi = {10.1145/2487575.2488200},
eprint = {arXiv:1301.3781v2},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/McMahan et al. - 2013 - Ad click prediction a view from the trenches.pdf:pdf},
isbn = {9781450321747},
issn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {data mining,large-scale learning,online advertising},
pages = {1222--1230},
title = {{Ad click prediction: a view from the trenches}},
year = {2013}
}
@article{Minka2001,
abstract = {One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, Expectation Propagation, unites and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unication shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction|propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation,to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classiers that is faster and more accurate than any previously known. The resulting classiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classication, via Bayesian model selection.},
author = {Minka, Thomas P},
doi = {10.1016/j.conb.2011.12.004},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Minka - 2001 - A family of algorithms for approximate Bayesian inference.pdf:pdf},
issn = {09594388},
journal = {PhD Thesis},
pmid = {2688543},
title = {{A family of algorithms for approximate Bayesian inference}},
url = {papers2://publication/uuid/37D3C7DD-C308-4279-86AC-52057DE5CB29},
year = {2001}
}
@article{Minka2013,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P},
eprint = {1301.2294},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Minka - 2013 - Expectation Propagation for approximate Bayesian inference.pdf:pdf},
isbn = {1-55860-800-1},
journal = {Statistics},
month = {jan},
number = {2},
pages = {362--369},
title = {{Expectation Propagation for approximate Bayesian inference}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1319{\&}amp;rep=rep1{\&}amp;type=pdf http://arxiv.org/abs/1301.2294},
volume = {17},
year = {2013}
}
@book{Murphy2012,
author = {Murphy, Kevin P.},
publisher = {The MIT Press},
title = {{Machine learning}},
year = {2012}
}
@article{Opper1996,
author = {Opper, Manfred},
doi = {10.1103/PhysRevLett.77.4671},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper - 1996 - On-line versus Off-line Learning from Random Examples General Results.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
pages = {4671--4674},
pmid = {10062597},
title = {{On-line versus Off-line Learning from Random Examples: General Results}},
volume = {77},
year = {1996}
}
@article{Opper1999,
author = {Opper, Manfred},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper - 1999 - A Bayesian approach to on-line learning.pdf:pdf},
journal = {On-line learning in neural networks},
pages = {363----378},
title = {{A Bayesian approach to on-line learning}},
url = {http://books.google.com/books?hl=en{\&}amp;lr={\&}amp;id={\_}w-LFSwUaFIC{\&}amp;oi=fnd{\&}amp;pg=PA363{\&}amp;dq=A+Bayesian+Approach+to+Online+Learning{\&}amp;ots=eSeMhpUua6{\&}amp;sig=Ecc-dA5tLR6qdpwlMdhRs1YfQ00},
year = {1999}
}
@article{Opper1997,
author = {Opper, Manfred and Winther, Ole},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper, Winther - 1997 - A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 9},
pages = {225--231},
title = {{A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks}},
url = {http://papers.nips.cc/paper/1268-a-mean-field-algorithm-for-bayes-learning-in-large-feed-forward-neural-networks.pdf$\backslash$nfiles/998/Opper ? Winther - 1997 - A Mean Field Algorithm for Bayes Learning in Large.pdf$\backslash$nfiles/999/1268-a-mean-field-algorithm-for-baye},
year = {1997}
}
@article{Opper2005,
abstract = {We propose a novel framework for approximations to intractable probabilisticmodels which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode different features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Significant improvements are obtained when a spanning tree is used instead.},
author = {Opper, Manfred and Winther, Ole},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Opper, Winther - 2005 - Expectation Consistent Approximate Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {2177--2204},
title = {{Expectation Consistent Approximate Inference}},
volume = {6},
year = {2005}
}
@article{Ramadhian2013,
author = {Ramadhian, Fauzan Hilmi},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Ramadhian - 2013 - Performance Analysis of Bloom Filter with Various Hash Functions on Spell Checker.pdf:pdf},
title = {{Performance Analysis of Bloom Filter with Various Hash Functions on Spell Checker}},
year = {2013}
}
@article{Runge2014,
abstract = {Predicting when players will leave a game creates a unique opportunity to increase players' lifetime and revenue contribution. Players can be incentivized to stay, strategically cross-linked to other games in the company's portfolio or, as a last resort, be passed on to other companies through in-game advertisement. This paper focuses on predicting churn for highvalue players of casual social games and attempts to assess the business impact that can be derived from a predictive churn model. We compare the prediction performance of four common classification algorithms over two casual social games, each with millions of players. Furthermore, we implement a hidden Markov model to explicitly address temporal dynamics. We find that a neural network achieves the best prediction performance in terms of area under curve (AUC). In addition, to assess the business value of churn prediction, we design and implement an A/B test on one of the games, using free in-game currency as an incentive to retain players. Test results indicate that contacting players shortly before the predicted churn event substantially improves the effectiveness of communication with players. They further show that giving out free in-game currency does not significantly impact the churn rate or monetization of players. This suggests that players can only be retained by remarkably changing their gameplay experience ahead of the churn event and that cross-linking may be the more effective measure to deal with churning players.},
author = {Runge, Julian and Gao, Peng and Garcin, Florent and Faltings, Boi},
doi = {10.1109/CIG.2014.6932875},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Runge, Gao - 2014 - Churn Prediction for High-Value Players in Casual Social Games.pdf:pdf},
isbn = {9781479935468},
issn = {23254289},
journal = {IEEE Conference on Computatonal Intelligence and Games, CIG},
keywords = {A/B evaluation,churn prediction,freemium,hidden Markov model,neural networks,social casual games},
title = {{Churn prediction for high-value players in casual social games}},
year = {2014}
}
@article{Salas2015,
abstract = {Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.},
archivePrefix = {arXiv},
arxivId = {1509.02438},
author = {Salas, Arnold and Roberts, Stephen J. and Osborne, Michael A.},
eprint = {1509.02438},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Salas, Roberts, Osborne - 2015 - A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression.pdf:pdf},
month = {sep},
pages = {1--9},
title = {{A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression}},
url = {http://arxiv.org/abs/1509.02438},
year = {2015}
}
@article{Shi2009,
abstract = {We propose hashing to facilitate efficient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs.},
author = {Shi, Q. and Petterson, J. and Dror, G. and Langford, J. and Smola, A. and Vishwanathan, S. V. N.},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Shi et al. - 2009 - Hash kernels for structured data.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {graphlet kernel,hashing,multiclass classification,stream,string kernel},
pages = {2615--2637},
title = {{Hash kernels for structured data}},
volume = {10},
year = {2009}
}
@article{Shi2013,
author = {Shi, Tianlin and Zhu, Jun},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Shi, Zhu - 2013 - Online {\{}Bayes{\}}ian Passive-Aggressive Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1312.3388},
title = {{Online {\{}Bayes{\}}ian Passive-Aggressive Learning}},
url = {http://arxiv.org/abs/1312.3388},
volume = {32},
year = {2013}
}
@article{Smith1962,
abstract = {Statistical filter theory application in optimal position and velocity estimation on board circumlunar vehicles},
author = {Smith, G. L. and McGee, L. A. and Schmidt, S. F.},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Smith, McGee, Schmidt - 1962 - Application of Statistical Filter Theory to the Optimal Estimation of Position and Velocity on Board a Ci.pdf:pdf},
title = {{Application of Statistical Filter Theory to the Optimal Estimation of Position and Velocity on Board a Circumlunar Vehicle}},
url = {http://hdl.handle.net/2060/19620006857},
year = {1962}
}
@article{Wan2000,
abstract = {This paper points out the flaws in using the extended Kalman filter (EKE) and introduces an improvement, the unscented Kalman filter (UKF), proposed by Julier and Uhlman (1997). A central and vital operation performed in the Kalman filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF the state distribution is approximated by a GRV, which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the filter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF in contrast, only achieves first-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF. Julier and Uhlman demonstrated the substantial performance gains of the UKF in the context of state-estimation for nonlinear control. Machine learning problems were not considered. We extend the use of the UKF to a broader class of nonlinear estimation problems, including nonlinear system identification, training of neural networks, and dual estimation problems. In this paper, the algorithms are further developed and illustrated with a number of additional examples},
author = {Wan, E.a. a and {Van Der Merwe}, R.},
doi = {10.1109/ASSPCC.2000.882463},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Wan, Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimation.pdf:pdf},
isbn = {0780358007},
issn = {15270297},
journal = {Technology},
pages = {153--158},
pmid = {20934485},
title = {{The unscented Kalman filter for nonlinear estimation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=882463},
volume = {v},
year = {2000}
}
@article{Wang2011,
abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a poten- tially infinite number of components. It has been applied widely in probabilistic topic modeling, where the data are documents and the ...},
author = {Wang, Chong and Blei, David M},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Wang, Blei - 2011 - Online Variational Inference for the Hierarchical Dirichlet Process.pdf:pdf},
isbn = {1609258177},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {752--760},
title = {{Online Variational Inference for the Hierarchical Dirichlet Process}},
url = {http://www.cs.cmu.edu/{~}chongw/papers/WangPaisleyBlei2011.pdf},
volume = {15},
year = {2011}
}
@article{Weinberger2009,
abstract = {Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case -- multitask learning with hundreds of thousands of tasks.},
archivePrefix = {arXiv},
arxivId = {0902.2206},
author = {Weinberger, Kilian and Dasgupta, Anirban and Attenberg, Josh and Langford, John and Smola, Alex},
doi = {10.1145/1553374.1553516},
eprint = {0902.2206},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Weinberger et al. - 2009 - Feature Hashing for Large Scale Multitask Learning.pdf:pdf},
isbn = {9781605585161},
issn = {1605585165},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {(pp. 1113--1120).},
title = {{Feature Hashing for Large Scale Multitask Learning}},
url = {http://arxiv.org/abs/0902.2206$\backslash$nhttp://alex.smola.org/papers/2009/Weinbergeretal09.pdf},
year = {2009}
}
@article{Wenta2012,
abstract = {Online algorithms allow data instances to be processed in a sequential way, which is important for large-scale and real-time applications. In this paper, we propose a novel online clustering approach based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions, which can be considered as an extension of the finite GD mixture model to the infinite case. Our approach is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. Moreover, an unsupervised localized feature selection scheme is integrated with the proposed nonparametric framework to improve the clustering performance. By learning the proposed model in an online manner using a variational approach, all the involved parameters and features saliencies are estimated simultaneously and effectively in closed forms. The proposed online infinite mixture model is validated through both synthetic data sets and two challenging real-world applications namely text document clustering and online human face detection. © 2012 W. Fan {\&} N. Bouguila.},
author = {Wenta, Wentao Fan and Bouguila, Nizar},
doi = {10.1016/j.patcog.2013.03.026},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Wenta, Bouguila - 2012 - Online learning of a dirichlet process mixture of generalized dirichlet distributions for simultaneous clusteri.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Clustering,Dirichlet process,Generalized dirichlet mixtures,Localized feature selection,Nonparametric bayesian,Online learning,Variational bayes},
pages = {113--128},
title = {{Online learning of a dirichlet process mixture of generalized dirichlet distributions for simultaneous clustering and localized feature selection}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84876887338{\&}partnerID=tZOtx3y1},
volume = {25},
year = {2012}
}
@article{Winther1998,
abstract = {. In a Bayesian approach to online learning a simple parametric approximate posterior over rules is updated in each online learning step. Predictions on new data are derived from averages over this posterior. This should be compared to the Bayes optimal batch (or offline) approach for which the posterior is calculated from the prior and the likelihood of the whole training set. We suggest that minimizing the difference between the batch and the approximate posterior will optimize the performance of the Bayes online algorithm. This general principle is demonstrated for three scenarios: learning a linear perceptron rule and a binary classification rule in the simple perceptron with binary /continuous weight prior. 1},
author = {Winther, Ole and Solla, SA},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Winther, Solla - 1998 - Optimal Bayesian online learning.pdf:pdf},
journal = {Theoretical Aspects of Neural Computation (TANC- {\ldots}},
number = {section 4},
title = {{Optimal Bayesian online learning}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Optimal+Bayesian+online+learning{\#}0},
year = {1998}
}
@article{Zhang2010,
abstract = {Many real world applications employ multi-variate performance measures and each example can belong to multiple classes. The currently most popular approaches train an SVM for each class, followed by ad hoc thresholding. Probabilistic models using Bayesian decision theory are also commonly adopted. In this paper, we propose a Bayesian online multi-label classification framework (BOMC) which learns a probabilistic linear classifier. The likelihood is modeled by a graphical model similar to TrueSkillTM, and inference is based on Gaussian density fil- tering with expectation propagation. Us- ing samples from the posterior, we label the testing data by maximizing the expected F1-score. Our experiments on Reuters1-v2 dataset show BOMC compares favorably to the state-of-the-art online learners in macro- averaged F1-score and training time.},
author = {Zhang, Xinhua and Graepel, Thore and Herbich, Ralf},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Zhang, Graepel, Herbich - 2010 - Bayesian Online Learning for Multi-label and Multi-variate Performance Measures.pdf:pdf},
issn = {15324435},
journal = {Thirteenth Conference on Artificial Intelligence and Statistics AISTATS 2010},
pages = {956--963},
title = {{Bayesian Online Learning for Multi-label and Multi-variate Performance Measures}},
volume = {9},
year = {2010}
}
@article{Zoeter2007,
author = {Zoeter, Onno},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Zoeter - 2007 - Bayesian Generalized Linear Models in a Terabyte World.pdf:pdf},
title = {{Bayesian Generalized Linear Models in a Terabyte World.}},
year = {2007}
}
@article{Zoeter2005,
abstract = {Later},
author = {Zoeter, Onno and Heskes, Tom},
file = {:C$\backslash$:/My/{\_}GoogleSync/Documents/Mendeley/Zoeter, Heskes - 2005 - Gaussian quadrature based expectation propagation.pdf:pdf},
isbn = {097273581X},
journal = {Workshop on Artificial Intelligence and Statistics},
keywords = {theory {\&} algorithms},
pages = {445--452},
title = {{Gaussian quadrature based expectation propagation.}},
url = {http://eprints.pascal-network.org/archive/00000562/},
volume = {10},
year = {2005}
}
